Resumo,Chave da item,ID da item,Tipo de item,Status,Chave do projeto,Nome do projeto,Tipo de projeto,Líder do projeto,ID do líder do projeto,Descrição do projeto,Prioridade,Resolução,Responsável,ID do responsável,Relator,ID do relator,Criador,ID do criador,Criado,Atualizado(a),Última visualização,Resolvido,Data limite,Votos,Descrição,Ambiente,Observadores,Observadores,Observadores,Observadores,Observadores,ID dos observadores,ID dos observadores,ID dos observadores,ID dos observadores,ID dos observadores,Estimativa original,Estimativa de trabalho restante,Tempo gasto,Razão Trabalhada,Σ da Estimativa Original,Σ da estimativa de trabalho restante,Σ de Tempo Gasto,Nível de Segurança,Anexo,Anexo,Anexo,Anexo,Anexo,Anexo,Anexo,Anexo,Anexo,Anexo,Anexo,Anexo,Anexo,Campo personalizado (Atlas project key),Campo personalizado (Atlas project status),Campo personalizado (Categoria),Campo personalizado (Change completion date),Campo personalizado (Change reason),Campo personalizado (Change risk),Campo personalizado (Change start date),Campo personalizado (Change type),Campo personalizado (Compass),Campo personalizado (Epic Color),Campo personalizado (Epic Name),Campo personalizado (Epic Status),Campo personalizado (Impact),Campo personalizado (Issue color),Campo personalizado (Locked forms),Campo personalizado (Open forms),Campo personalizado (Organizations),Campo personalizado (Rank),Campo personalizado (Request Type),Campo personalizado (Request language),Campo personalizado (Request participants),Campo personalizado (Request participants),Campo personalizado (Request participants),Campo personalizado (Request participants),Campo personalizado (Request participants),Campo personalizado (Request participants),Classificação de satisfação,Campo personalizado (Start date),Campo personalizado (Story Points),Campo personalizado (Story point estimate),Campo personalizado (Submitted forms),Campo personalizado (Target end),Campo personalizado (Target start),Campo personalizado (Team),Campo personalizado (Tempo até a conclusão),Campo personalizado (Tempo até a primeira resposta),Campo personalizado (Time to done),Campo personalizado (Time to done),Campo personalizado (Time to first response),Campo personalizado (Time to triage normal change),Campo personalizado (Total forms),Campo personalizado (Work category),Campo personalizado ([CHART] Date of First Response),Campo personalizado (development),Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Comentar,Categoria do status
limit de fields no logstash X es,DATAPREV-25,10949,Suporte técnico remoto: esclarecimento de dúvidas,Concluído(a),DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Baixa,,Paulo Henrique Morato Góes,61e09d6968926d0068d83362,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,05/set/23 9:50 AM,08/set/23 1:48 PM,19/set/23 4:13 PM,,,0,"reparem esse erro no logstash : 

{noformat}CouldnotindexeventtoElasticsearch.status: 400,
action: [
  ""create"",
  {
    : _id=>nil,
    : _index=>""logs-infra-dtp-syslog-8.8.1"",
    : routing=>nil
  },
  {
    ""event""=>{
      ""timezone""=>""-03:00"",
      ""severity""=>5
    },
    ""log""=>{
      ""syslog""=>{
        ""severity""=>{
          ""name""=>""notice"",
          ""code""=>5
        },
        ""facility""=>{
          ""name""=>""user-level"",
          ""code""=>1
        }
      },
      ""source""=>{
        ""address""=>""10.140.224.97:38996"",
        ""ip""=>""10.140.224.97"",
        ""port""=>""38996""
      }
    },
    ""dtp.redis_key""=>""dtp-syslog"",
    ""ecs""=>{
      ""version""=>""8.0.0""
    },
    ""hostname""=>""v271p002026.prevnet"",
    ""type""=>""syslog"",
    ""agent""=>{
      ""id""=>""58bbf018-5d91-4522-868e-360f23c32a89"",
      ""name""=>""n221p036888"",
      ""version""=>""8.8.1"",
      ""ephemeral_id""=>""fcac8999-038f-4a37-91a4-ead8f6b6ab77"",
      ""type""=>""filebeat""
    },
    ""message""=>"""",
    ""dtp.redis_server""=>""n241p036886.fast.prevnet"",
    ""@timestamp""=>2023-09-05T12: 40: 46.103Z,
    ""dtp.logstash_indexer""=>""n321p004939.fast.prevnet"",
    ""dtp""=>{
      ""type""=>""syslog"",
      ""sistema""=>""syslogpadrao"",
      ""ambiente""=>""prod"",
      ""categoria""=>""syslogpadrao"",
      ""cp""=>""cpdf"",
      ""cliente""=>""dtp""
    },
    ""tags""=>[
      ""FALHO_grok_syslogpadrao""
    ],
    ""@version""=>""1"",
    ""syslog""=>{
      ""priority""=>13,
      ""facility_label""=>""user-level"",
      ""event""=>{
        ""original""=>""""
      },
      ""version""=>1,
      ""severity_label""=>""Notice"",
      ""facility""=>1,
      ""data""=>{
        ""05/Sep/2023:12:40:45""=>{
          
        }
      },
      ""msgid""=>""-""
    },
    ""input""=>{
      ""type""=>""syslog""
    },
    ""host""=>{
      ""name""=>""n221p036888""
    },
    ""data_stream""=>{
      ""dataset""=>""infra-dtp"",
      ""namespace""=>""syslog-8.8.1"",
      ""type""=>""logs""
    },
    ""process""=>{
      ""name""=>""10.140.224.10"",
      ""entity_id""=>""-""
    }
  }
],
response: {
  ""create""=>{
    ""_index""=>"".ds-logs-infra-dtp-syslog-8.8.1-2023.09.05-000081"",
    ""_id""=>""AopcZYoBEeAqUW-VDxSp"",
    ""status""=>400,
    ""error""=>{
      ""type""=>""document_parsing_exception"",
      ""reason""=>""[1:973] failed to parse: Limit of total fields [10000] has been exceeded while adding new fields [1]"",
      ""caused_by""=>{
        ""type""=>""illegal_argument_exception"",
        ""reason""=>""Limit of total fields [10000] has been exceeded while adding new fields [1]""
      }
    }
  }
}{noformat}





mas nao tem 10k fields ae … sabem pq esse erro ocorre ? unica coisa q vejo ae é q o campo message ta vazio",,Ana Karolina Moreira Viana Catta Preta,Paulo Henrique Morato Góes,Rodrigo Tornis,,,62a7957ba80881006f6130ed,61e09d6968926d0068d83362,5f9829d0dbf337006c7dc32d,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i005s3:,,português,,,,,,,,,,,,,,,63:00,,,,,,,,2023-09-05 13:17:13.234,,"05/set/23 10:17 AM;61e09d6968926d0068d83362;Prezado Elias Mussi, bom dia!



Podemos agendar uma call para avaliarmos melhor o ocorrido? Qual o horário hoje que você tem disponibilidade?



Atenciosamente,

Paulo Henrique",05/set/23 10:34 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;14hs ?,"05/set/23 10:47 AM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Marcada então a reunião para hoje as 14:00.

Segue o link da reunião: [https://us06web.zoom.us/j/81944654404?pwd=WVBYRXpQZSsyOGlibmtwclRLeVVVUT09|https://us06web.zoom.us/j/81944654404?pwd=WVBYRXpQZSsyOGlibmtwclRLeVVVUT09|smart-link]  



Atenciosamente,

Paulo Henrique",05/set/23 10:48 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;blz,"05/set/23 2:33 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Conforme conversado em reunião, ficou definido que para termos um melhor entendimento do que pode estar causando este erro, precisamos avaliar o log para debugar o problema. Sendo assim necessário que nos envie o arquivo de log. Aguardamos o arquivo para continuarmos a avaliação.



Atenciosamente,

Paulo Henrique","06/set/23 9:59 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;apanhei mas consegui pegar uns output para o ES 

reparem q nao tem nada demais

{noformat}{""syslog"":{""msgid"":""-"",""facility_label"":""user-level"",""facility"":1,""severity_label"":""Notice"",""data"":{""06/Sep/2023:12:38:55"":{}},""version"":1,""event"":{""original"":""""},""priority"":13},""hostname"":""v171p002015.prevnet"",""dtp.logstash_indexer"":""n321p004948.fast.prevnet"",""process"":{""name"":""10.140.224.10"",""entity_id"":""-""},""@timestamp"":""2023-09-06T12:38:56.907Z"",""event"":{""severity"":5,""timezone"":""-03:00""},""message"":"""",""tags"":[""FALHO_grok_syslogpadrao""],""ecs"":{""version"":""8.0.0""},""input"":{""type"":""syslog""},""dtp"":{""cliente"":""dtp"",""sistema"":""syslogpadrao"",""cp"":""cprj"",""type"":""syslog"",""ambiente"":""prod"",""categoria"":""syslogpadrao""},""@version"":""1"",""data_stream"":{""type"":""logs"",""dataset"":""infra-dtp"",""namespace"":""syslog-8.8.1""},""dtp.redis_server"":""n141p000132.fast.prevnet"",""agent"":{""version"":""8.8.1"",""type"":""filebeat"",""id"":""329ca6c2-cbc8-4148-b87b-110d1ac684ec"",""name"":""n121p000134"",""ephemeral_id"":""01216f68-2ff4-42d6-905b-462ae02491f1""},""host"":{""name"":""n121p000134""},""type"":""syslog"",""log"":{""syslog"":{""facility"":{""code"":1,""name"":""user-level""},""severity"":{""code"":5,""name"":""notice""}},""source"":{""ip"":""10.132.224.97"",""address"":""10.132.224.97:40600"",""port"":""40600""}},""dtp.redis_key"":""dtp-syslog""}
{""hostname"":""v171p002015.prevnet"",""syslog"":{""data"":{""06/Sep/2023:12:38:56"":{}},""priority"":13,""facility"":1,""severity_label"":""Notice"",""facility_label"":""user-level"",""version"":1,""event"":{""original"":""""},""msgid"":""-""},""dtp.logstash_indexer"":""n321p004948.fast.prevnet"",""process"":{""name"":""10.140.224.10"",""entity_id"":""-""},""@timestamp"":""2023-09-06T12:38:56.907Z"",""message"":"""",""event"":{""severity"":5,""timezone"":""-03:00""},""tags"":[""FALHO_grok_syslogpadrao""],""dtp"":{""sistema"":""syslogpadrao"",""cliente"":""dtp"",""cp"":""cprj"",""type"":""syslog"",""ambiente"":""prod"",""categoria"":""syslogpadrao""},""ecs"":{""version"":""8.0.0""},""input"":{""type"":""syslog""},""@version"":""1"",""data_stream"":{""type"":""logs"",""dataset"":""infra-dtp"",""namespace"":""syslog-8.8.1""},""dtp.redis_server"":""n141p000132.fast.prevnet"",""agent"":{""version"":""8.8.1"",""id"":""329ca6c2-cbc8-4148-b87b-110d1ac684ec"",""name"":""n121p000134"",""type"":""filebeat"",""ephemeral_id"":""01216f68-2ff4-42d6-905b-462ae02491f1""},""host"":{""name"":""n121p000134""},""type"":""syslog"",""log"":{""syslog"":{""facility"":{""code"":1,""name"":""user-level""},""severity"":{""code"":5,""name"":""notice""}},""source"":{""ip"":""10.132.224.97"",""address"":""10.132.224.97:40600"",""port"":""40600""}},""dtp.redis_key"":""dtp-syslog""}
{""hostname"":""v371p002019.prevnet"",""syslog"":{""data"":{""06/Sep/2023:12:38:59"":{}},""priority"":13,""facility"":1,""severity_label"":""Notice"",""facility_label"":""user-level"",""version"":1,""event"":{""original"":""""},""msgid"":""-""},""dtp.logstash_indexer"":""n321p004948.fast.prevnet"",""process"":{""name"":""10.140.224.10"",""entity_id"":""-""},""@timestamp"":""2023-09-06T12:38:59.732Z"",""message"":"""",""event"":{""severity"":5,""timezone"":""-03:00""},""tags"":[""FALHO_grok_syslogpadrao""],""input"":{""type"":""syslog""},""dtp"":{""cliente"":""dtp"",""sistema"":""syslogpadrao"",""cp"":""cpsp"",""type"":""syslog"",""ambiente"":""prod"",""categoria"":""syslogpadrao""},""ecs"":{""version"":""8.0.0""},""@version"":""1"",""data_stream"":{""type"":""logs"",""dataset"":""infra-dtp"",""namespace"":""syslog-8.8.1""},""dtp.redis_server"":""n341p004960.fast.prevnet"",""agent"":{""version"":""8.8.1"",""type"":""filebeat"",""id"":""8f59890e-9596-466c-9974-ad9cbdee7ade"",""name"":""n321p004957"",""ephemeral_id"":""2909f05b-eeab-4687-926f-c6c1a57b73ab""},""host"":{""name"":""n321p004957""},""type"":""syslog"",""log"":{""syslog"":{""facility"":{""code"":1,""name"":""user-level""},""severity"":{""code"":5,""name"":""notice""}},""source"":{""ip"":""10.148.224.95"",""address"":""10.148.224.95:36681"",""port"":""36681""}},""dtp.redis_key"":""dtp-syslog""}
{""hostname"":""v371p002019.prevnet"",""syslog"":{""data"":{""06/Sep/2023:12:39:08"":{}},""facility_label"":""user-level"",""facility"":1,""severity_label"":""Notice"",""priority"":13,""version"":1,""event"":{""original"":""""},""msgid"":""-""},""dtp.logstash_indexer"":""n321p004948.fast.prevnet"",""process"":{""name"":""10.140.224.10"",""entity_id"":""-""},""@timestamp"":""2023-09-06T12:39:08.743Z"",""message"":"""",""event"":{""severity"":5,""timezone"":""-03:00""},""tags"":[""FALHO_grok_syslogpadrao""],""dtp"":{""cliente"":""dtp"",""sistema"":""syslogpadrao"",""cp"":""cpsp"",""type"":""syslog"",""ambiente"":""prod"",""categoria"":""syslogpadrao""},""ecs"":{""version"":""8.0.0""},""input"":{""type"":""syslog""},""@version"":""1"",""data_stream"":{""type"":""logs"",""dataset"":""infra-dtp"",""namespace"":""syslog-8.8.1""},""dtp.redis_server"":""n341p004960.fast.prevnet"",""agent"":{""version"":""8.8.1"",""type"":""filebeat"",""id"":""8f59890e-9596-466c-9974-ad9cbdee7ade"",""name"":""n321p004957"",""ephemeral_id"":""2909f05b-eeab-4687-926f-c6c1a57b73ab""},""host"":{""name"":""n321p004957""},""type"":""syslog"",""log"":{""syslog"":{""facility"":{""code"":1,""name"":""user-level""},""severity"":{""code"":5,""name"":""notice""}},""source"":{""ip"":""10.148.224.95"",""address"":""10.148.224.95:36681"",""port"":""36681""}},""dtp.redis_key"":""dtp-syslog""}
{noformat}





o q acham ?","06/set/23 10:04 AM;61e09d6968926d0068d83362;Prezado Elias Mussi, 

Vamos avaliar os logs e em breve te retornamos a respeito. Enquanto isso, gostaria de solicitar alteração da severidade do presente chamado para Severidade 3, dado o que informa o TR:

*_c) Severidade 3 – quando se verifica uma perda de menor relevância de_*

*_funcionalidades em programas ou sistemas, causando apenas inconveniências para_*

*_a realização de atividades exercidas ou pela devida prestação dos serviços pela_*

*_DATAPREV._*



Atenciosamente,

Paulo Henrique",06/set/23 10:06 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;eu abri como a mais baixa .. vc quer subir ela ? ,"06/set/23 10:07 AM;5f9829d0dbf337006c7dc32d;Mussi, 

Uma dúvida esse trecho de log que vc mandou ele é um único evento? ou são 4 eventos? 

Porque estou perguntando isso? Em um dos eventos tem o Syslog iniciando os demais não?  Minha suspeita é que pode estar ocorrendo um Json inválido ai na estrutura e o Elastic acaba dando erro de limit mapping. 



Pode validar essa informação? ","06/set/23 10:10 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;sao exemplos de mesmo log … repara q qdo se gera o json .. ele nao vem sempre da mesma sequencia … pode sempre variar 🙂  eu peguei uma sequencia aqui aleatoria … tenho uma 300 linhas iguais

","06/set/23 10:21 AM;5f9829d0dbf337006c7dc32d;Sim agora que vi aqui, 

Bom a minha sugestão é ir no Devtools e tentar inserir esse registro na mão para ver se o erro persiste. Importante é que se ele tiver pipeline precisa informar na inserção ok? Exemplo 



{noformat}POST my-data-stream/_doc?pipeline=my-pipeline
{
   ""syslog"":{
      ""msgid"":""-"",
      ""facility_label"":""user-level"",
      ""facility"":1,
      ""severity_label"":""Notice"",
      ""data"":{
         ""06/Sep/2023:12:38:55"":{
            
         }
      },
      ""version"":1,
      ""event"":{
         ""original"":""""
      },
      ""priority"":13
   },
   ""hostname"":""v171p002015.prevnet"",
   ""dtp.logstash_indexer"":""n321p004948.fast.prevnet"",
   ""process"":{
      ""name"":""10.140.224.10"",
      ""entity_id"":""-""
   },
   ""@timestamp"":""2023-09-06T12:38:56.907Z"",
   ""event"":{
      ""severity"":5,
      ""timezone"":""-03:00""
   },
   ""message"":"""",
   ""tags"":[
      ""FALHO_grok_syslogpadrao""
   ],
   ""ecs"":{
      ""version"":""8.0.0""
   },
   ""input"":{
      ""type"":""syslog""
   },
   ""dtp"":{
      ""cliente"":""dtp"",
      ""sistema"":""syslogpadrao"",
      ""cp"":""cprj"",
      ""type"":""syslog"",
      ""ambiente"":""prod"",
      ""categoria"":""syslogpadrao""
   },
   ""@version"":""1"",
   ""data_stream"":{
      ""type"":""logs"",
      ""dataset"":""infra-dtp"",
      ""namespace"":""syslog-8.8.1""
   },
   ""dtp.redis_server"":""n141p000132.fast.prevnet"",
   ""agent"":{
      ""version"":""8.8.1"",
      ""type"":""filebeat"",
      ""id"":""329ca6c2-cbc8-4148-b87b-110d1ac684ec"",
      ""name"":""n121p000134"",
      ""ephemeral_id"":""01216f68-2ff4-42d6-905b-462ae02491f1""
   },
   ""host"":{
      ""name"":""n121p000134""
   },
   ""type"":""syslog"",
   ""log"":{
      ""syslog"":{
         ""facility"":{
            ""code"":1,
            ""name"":""user-level""
         },
         ""severity"":{
            ""code"":5,
            ""name"":""notice""
         }
      },
      ""source"":{
         ""ip"":""10.132.224.97"",
         ""address"":""10.132.224.97:40600"",
         ""port"":""40600""
      }
   },
   ""dtp.redis_key"":""dtp-syslog""
}{noformat}","06/set/23 11:27 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;repara esse teste aqui  , usando um evento q peguei no erro do logstash … os q capiturei sao foda … 

{noformat}POST teste/_doc
{
    ""event"":{
      ""severity"":5,
      ""timezone"":""-03:00""
    },
    ""hostname"":""v371p002019.prevnet"",
    ""process"":{
      ""name"":""10.140.224.10"",
      ""entity_id"":""-""
    },
    ""type"":""syslog"",
    ""tags"":[
      ""FALHO_grok_syslogpadrao""
    ],
    ""syslog"":{
      ""event"":{
        ""original"":""""
      },
      ""facility"":1,
      ""version"":1,
      ""facility_label"":""user-level"",
      ""msgid"":""-"",
      ""severity_label"":""Notice"",
      ""data"":{
        ""06/Sep/2023:14:16:24"":{
          
        }
      },
      ""priority"":13
    },
    ""message"":"""",
    ""data_stream"":{
      ""dataset"":""infra-dtp"",
      ""namespace"":""syslog-8.8.1"",
      ""type"":""logs""
    },
    ""dtp"":{
      ""cliente"":""dtp"",
      ""categoria"":""syslogpadrao"",
      ""type"":""syslog"",
      ""sistema"":""syslogpadrao"",
      ""cp"":""cpsp"",
      ""ambiente"":""prod""
    },
    ""dtp.redis_server"":""n341p004960.fast.prevnet"",
    ""ecs"":{
      ""version"":""8.0.0""
    },
    ""dtp.logstash_indexer"":""n321p004938.fast.prevnet"",
    ""dtp.redis_key"":""dtp-syslog"",
    ""input"":{
      ""type"":""syslog""
    },
    ""log"":{
      ""syslog"":{
        ""facility"":{
          ""name"":""user-level"",
          ""code"":1
        },
        ""severity"":{
          ""name"":""notice"",
          ""code"":5
        }
      },
      ""source"":{
        ""address"":""10.148.224.95:36681"",
        ""ip"":""10.148.224.95"",
        ""port"":""36681""
      }
    },
    ""@timestamp"":2023-09-06T14: 16: 25.591Z,
    ""host"":{
      ""name"":""n321p004957""
    },
    ""agent"":{
      ""id"":""8f59890e-9596-466c-9974-ad9cbdee7ade"",
      ""name"":""n321p004957"",
      ""version"":""8.8.1"",
      ""type"":""filebeat"",
      ""ephemeral_id"":""a658805b-c57d-47fd-a29e-239394b50915""
    },
    ""@version"":""1""
  }
{noformat}

","06/set/23 11:57 AM;62a7957ba80881006f6130ed;Prezado Mussi, bom dia!

Solicito por gentileza a autorização para troca de severidade do presente chamado, *uma vez que não se trata mais de esclarecimento de dúvida* sobre documentação ou funcionalidade conforme versa a severidade 4. *O presente chamado apresenta características que se enquadram na severidade 3*, onde se verifica uma perda de menor relevância de funcionalidade em programas ou sistemas, causando apenas incoveniências para a realização de atividades exercidas. 


 


*Assim, buscando cumprir integralmente o que preceitua o item 14.1 do TR, solicito a autorização para alteração de severidade para a severidade 3.*

Desde já, agradeço o pronto atendimento!","06/set/23 2:03 PM;5f9829d0dbf337006c7dc32d;Mussi, 

Mas essa inserção que vc fez gerou algum erro? Fiquei na dúvida da resposta?","06/set/23 2:32 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Pode mudar Ana !1

me liga no zap tornis … to encafufado com esse treco","06/set/23 4:01 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Propomos a reunião as 16:40 para esclarecimentos. Segue o link da call: [https://us06web.zoom.us/j/85319105953?pwd=ZFJNNG9jV2FwdGJpMFFtYmF4TnlOUT09|https://us06web.zoom.us/j/85319105953?pwd=ZFJNNG9jV2FwdGJpMFFtYmF4TnlOUT09|smart-link] 



Atenciosamente,

Paulo Henrique","08/set/23 11:09 AM;61e09d6968926d0068d83362;Prezado Elias Mussi, bom dia.

Após reunião realizada hoje as 9hrs, ficou constatado que o erro que vem acontecendo tem relação com o parser e não com um bug do sistema. Além dessa constatação, ficou decidido que seriam dropados os eventos que tivessem o campo message: “null” para não fazer a ingestão de lixo pro elastic. 

Sendo assim solicito permissão para o fechamento do presente chamado.



Atenciosamente,

Paulo Henrique ","08/set/23 11:19 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;reparem a msg q sai do forwarder para o redis : 

msg em branco

{noformat}""{\""@timestamp\"":\""2023-09-08T14:09:08.674Z\"",\""@metadata\"":{\""beat\"":\""filebeat\"",\""type\"":\""_doc\"",\""version\"":\""8.8.1\"",\""truncated\"":false},\""ecs\"":{\""version\"":\""8.0.0\""},\""hostname\"":\""v271p002026.prevnet\"",\""syslog\"":{\""facility_label\"":\""user-level\"",\""msgid\"":\""-\"",\""version\"":1,\""data\"":{\""08/Sep/2023:14:09:07\"":{}},\""priority\"":13,\""severity_label\"":\""Notice\"",\""facility\"":1},\""process\"":{\""name\"":\""10.132.224.10\"",\""entity_id\"":\""-\""},\""log\"":{\""source\"":{\""address\"":\""10.140.224.97:38996\""}},\""dtp\"":{\""sistema\"":\""syslogpadrao\"",\""categoria\"":\""syslogpadrao\"",\""type\"":\""syslog2\"",\""cp\"":\""cpdf\"",\""ambiente\"":\""prod\"",\""cliente\"":\""dtp\""},\""input\"":{\""type\"":\""syslog\""},\""host\"":{\""name\"":\""n221p036888\""},\""agent\"":{\""ephemeral_id\"":\""1bc4b8b2-db25-45e5-bde6-bcb99cb8641b\"",\""id\"":\""58bbf018-5d91-4522-868e-360f23c32a89\"",\""name\"":\""n221p036888\"",\""type\"":\""filebeat\"",\""version\"":\""8.8.1\""},\""message\"":\""\"",\""event\"":{\""severity\"":5,\""timezone\"":\""-03:00\""}}""

""{\""@timestamp\"":\""2023-09-08T14:08:07.583Z\"",\""@metadata\"":{\""beat\"":\""filebeat\"",\""type\"":\""_doc\"",\""version\"":\""8.8.1\"",\""truncated\"":false},\""process\"":{\""name\"":\""10.132.224.10\"",\""entity_id\"":\""-\""},\""message\"":\""\"",\""hostname\"":\""v271p002026.prevnet\"",\""syslog\"":{\""facility_label\"":\""user-level\"",\""msgid\"":\""-\"",\""version\"":1,\""data\"":{\""08/Sep/2023:14:08:07\"":{}},\""priority\"":13,\""severity_label\"":\""Notice\"",\""facility\"":1},\""input\"":{\""type\"":\""syslog\""},\""dtp\"":{\""categoria\"":\""syslogpadrao\"",\""type\"":\""syslog2\"",\""cp\"":\""cpdf\"",\""ambiente\"":\""prod\"",\""cliente\"":\""dtp\"",\""sistema\"":\""syslogpadrao\""},\""event\"":{\""severity\"":5,\""timezone\"":\""-03:00\""},\""log\"":{\""source\"":{\""address\"":\""10.140.224.97:38996\""}},\""ecs\"":{\""version\"":\""8.0.0\""},\""host\"":{\""name\"":\""n221p036888\""},\""agent\"":{\""type\"":\""filebeat\"",\""version\"":\""8.8.1\"",\""ephemeral_id\"":\""ef9df33b-c6a5-4588-b8b5-00109f6fe9be\"",\""id\"":\""58bbf018-5d91-4522-868e-360f23c32a89\"",\""name\"":\""n221p036888\""}}""
{noformat}



com message:

{noformat}{""@timestamp"":""2023-09-08T14:08:20.745Z"",""@metadata"":{""beat"":""filebeat"",""type"":""_doc"",""version"":""8.8.1"",""truncated"":false},""syslog"":{""version"":1,""priority"":142,""severity_label"":""Informational"",""facility"":17,""facility_label"":""local1"",""msgid"":""-""},""process"":{""name"":""hcx-app"",""entity_id"":""-""},""log"":{""source"":{""address"":""10.140.224.97:38996""}},""input"":{""type"":""syslog""},""ecs"":{""version"":""8.0.0""},""message"":""[TopologyService_SvcThread-93684] [LEVEL=INFO] c.v.v.h.vsphere.UpdateInventoryJob v241p500.prevnet: Time taken to persist 86 unique updates: 496 ms."",""event"":{""severity"":6,""timezone"":""-03:00""},""dtp"":{""sistema"":""syslogpadrao"",""categoria"":""syslogpadrao"",""type"":""syslog2"",""cp"":""cpdf"",""ambiente"":""prod"",""cliente"":""dtp""},""host"":{""name"":""n221p036888""},""agent"":{""name"":""n221p036888"",""type"":""filebeat"",""version"":""8.8.1"",""ephemeral_id"":""ef9df33b-c6a5-4588-b8b5-00109f6fe9be"",""id"":""58bbf018-5d91-4522-868e-360f23c32a89""},""hostname"":""v271p002026.prevnet""}

{""@timestamp"":""2023-09-08T14:09:21.491Z"",""@metadata"":{""beat"":""filebeat"",""type"":""_doc"",""version"":""8.8.1"",""truncated"":false},""hostname"":""v271p002026.prevnet"",""process"":{""name"":""hcx-app"",""entity_id"":""-""},""log"":{""source"":{""address"":""10.140.224.97:38996""}},""dtp"":{""ambiente"":""prod"",""cliente"":""dtp"",""sistema"":""syslogpadrao"",""categoria"":""syslogpadrao"",""type"":""syslog2"",""cp"":""cpdf""},""input"":{""type"":""syslog""},""ecs"":{""version"":""8.0.0""},""host"":{""name"":""n221p036888""},""message"":""[TopologyService_SvcThread-93689] [LEVEL=INFO] c.v.v.h.vsphere.UpdateInventoryJob v241p500.prevnet: Time taken to process 91 raw updates: 6932 ms (includes wait time as well, if any.). Version: 2443684 Truncated: false."",""syslog"":{""msgid"":""-"",""version"":1,""priority"":142,""severity_label"":""Informational"",""facility"":17,""facility_label"":""local1""},""event"":{""timezone"":""-03:00"",""severity"":6},""agent"":{""version"":""8.8.1"",""ephemeral_id"":""1bc4b8b2-db25-45e5-bde6-bcb99cb8641b"",""id"":""58bbf018-5d91-4522-868e-360f23c32a89"",""name"":""n221p036888"",""type"":""filebeat""}}
{noformat}



por algum motivo qdo o msg e vazio ele cria aquele campo syslog.data … e me parece q qdo nao vem vazio nao cria.","08/set/23 11:21 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;a conf do filebeat para constar : 

{noformat}- type: syslog
  format: auto
  protocol.udp:
    host: ""0.0.0.0:514""
  #timezone: ""UTC""
  processors:
    - add_locale: ~
  fields_under_root: true
  fields:
    dtp.cp: ""cpdf""
    dtp.ambiente: ""prod""
    dtp.cliente: ""dtp""
    dtp.sistema: ""syslogpadrao""
    dtp.categoria: ""syslogpadrao""
    dtp.type: ""syslog""
{noformat}



e nao passa por filtro algum no logstash … e nem pipeline no ES","08/set/23 1:37 PM;5f9829d0dbf337006c7dc32d;Mussi, 



Vc e estranho pois o Filebeat não acrescenta o syslog.data isso não faz parte da transformação do modulo syslog e nem de algum ponto do filebeat. Estava olhando a documentação do ECS e dos pipelines dele em nenhum momento esse campo syslog.data é criado.  [https://www.elastic.co/guide/en/beats/filebeat/8.8/filebeat-input-syslog.html|https://www.elastic.co/guide/en/beats/filebeat/8.8/filebeat-input-syslog.html|smart-link]   e segue as rfcs, {{rfc3164}} or {{rfc5424}} que não esta previsto esse campo data. 
Bom outro ponto estranho e observando o código do Filebeat é que quando o campo no Syslog vem vazio ele atribui ao campo syslog.data o timestamp. Veja em: 
[https://github.com/elastic/beats/blob/80ed33b8cc69ed9f15d22a3e31e11ef7ffc65a3a/filebeat/input/syslog/input.go#L264|https://github.com/elastic/beats/blob/80ed33b8cc69ed9f15d22a3e31e11ef7ffc65a3a/filebeat/input/syslog/input.go#L264] 

Então o que esta acontecendo é: Como o Message vem vazio ele atribui a esse campo o syslog.data da coleta sacou? 

E por isso a treta!  Então recomendo que vc drop mesmo esse cara ou trate esse campo antes da indexação! 



Abraço","08/set/23 1:46 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;ae arrebenta em 🙂  cria um comportamento foda.

ja coloquei o drop no logstash …

mas uma outra solucao seria colocar  um processo la no input-syslog …

algo como : 

{noformat}processors:
  - drop_event:
      when:
        message == """"{noformat}

","08/set/23 1:46 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;pode fechar esse evento.

brigadao

",,,,,,,,,,,,,,,,,Itens concluídos
problemas com restore de snapshot (S3) em outro cluster,DATAPREV-24,10940,Suporte técnico remoto: esclarecimento de dúvidas,Aguardando cliente,DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Baixa,,Vinícius Prysthon,621538d1347c690072f6b56f,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,02/ago/23 10:47 AM,07/ago/23 10:44 AM,04/ago/23 2:37 PM,,,0,"Boas,

turma .. estou configurando um cluster para poder fazer restore de dados do S3.

mas estou tendo problemas … 

no cluster de PROD tudo está ok .. os snaps sendo feitos no S3 bunitim

ae entao peguei as conf dele e repliquei no cluster de vai fazer o restore : 



{noformat}get _snapshot/s3_vpn/
{
  ""s3_vpn"": {
    ""type"": ""s3"",
    ""uuid"": ""y8u9_08uTCS7cwqyagaGbw"",
    ""settings"": {
      ""bucket"": ""bk-elk"",
      ""path_style_access"": ""true"",
      ""base_path"": ""prd_elk8_vpn"",
      ""endpoint"": ""s3-sp.ecs.prevnet:9020"",
      ""protocol"": ""http""
    }
  }
}{noformat}

dando uma adaptada : 

{noformat}
PUT _snapshot/s3_vpn
{
    ""type"": ""s3"",
    ""settings"": {
      ""bucket"": ""bk-elk"",
      ""path_style_access"": ""true"",
      ""base_path"": ""prd_elk8_vpn"",
      ""endpoint"": ""s3-sp.ecs.prevnet:9020"",
      ""protocol"": ""http"",
      ""readonly"": true
    }
  }{noformat}



o problema é q no cluster de restore nao lista os snaps

{noformat}POST _snapshot/s3_vpn/_verify
{
  ""nodes"": {
    ""XxFv3Xi1SqqyaFnbXNyIvA"": {
      ""name"": ""n321p005266""
    },
    ""qbe4HWsbT-yHh2hp6AFKXg"": {
      ""name"": ""n321p005265""
    },
    ""Z7uZA0OMSDuX_2jbWgPWcQ"": {
      ""name"": ""n321p005264""
    }
  }
}


get _snapshot/s3_vpn/*?verbose=true
{
  ""snapshots"": [],
  ""total"": 0,
  ""remaining"": 0
}{noformat}



estou em um beco sem saida aqui 😕 



o que fazer ?



abraços",,Paulo Henrique Morato Góes,Rodrigo Tornis,Vinícius Prysthon,,,61e09d6968926d0068d83362,5f9829d0dbf337006c7dc32d,621538d1347c690072f6b56f,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i005q3:,,português,ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb(ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb),fabio(fabio),ug:0bf5b6db-b989-4171-bb50-b139a583a751(ug:0bf5b6db-b989-4171-bb50-b139a583a751),ug:0e2daf0a-9fff-4018-89ac-200a262b74f9(ug:0e2daf0a-9fff-4018-89ac-200a262b74f9),,,,,,,,,,,64:47,,,,,,,,2023-08-02 14:01:25.065,,"02/ago/23 11:01 AM;621538d1347c690072f6b56f;Prezado Elias Mussi, bom dia!

Solicito permissão para *alteração da severidade do chamado*, uma vez que trata-se de esclarecimento de dúvidas, sendo, portanto, um chamado de severidade 4, conforme consta no item 14.1 do TR:

_d) Severidade 4 – quando se verifica como necessária a prestação de informações, aperfeiçoamentos ou_ *_esclarecimentos_* _sobre documentação ou_ *_funcionalidades, porém sem prejudicar diretamente a operação dos programas ou sistemas da DATAPREV._*

Desde já, agradecemos a cooperação.



Vinícius Prysthon","02/ago/23 11:03 AM;621538d1347c690072f6b56f;Aproveito para sugerir uma reunião para conversarmos e entendermos melhor o que está acontecendo.

Podemos reunir às 14h, o que acha?

Aguardo seu retorno.



Atenciosamente,

Vinícius Prysthon",02/ago/23 12:30 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;combinado so mandar o link,"02/ago/23 1:54 PM;621538d1347c690072f6b56f;Prezado Elias Mussi,

Segue link para reunião: [https://us06web.zoom.us/j/81379408035|https://us06web.zoom.us/j/81379408035|smart-link] 



Atenciosamente,

Vinícius Prysthon","02/ago/23 3:34 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;{noformat}n321p005265:/root/elk8/scripts/snapshots/s3/s3cmd-2.3.0:./s3cmd ls s3://bk-elk/teste/
                          DIR  s3://bk-elk/teste/indices/
2023-08-02 17:46         9673  s3://bk-elk/teste/index-0
2023-08-02 17:46            8  s3://bk-elk/teste/index.latest
2023-08-02 17:46      1782917  s3://bk-elk/teste/meta-g5xedgISRma_V0KBPGFH4g.dat
2023-08-02 17:46         1330  s3://bk-elk/teste/snap-g5xedgISRma_V0KBPGFH4g.dat
n321p005265:/root/elk8/scripts/snapshots/s3/s3cmd-2.3.0:./s3cmd ls s3://bk-elk/prd_elk8_vpn/
                          DIR  s3://bk-elk/prd_elk8_vpn/indices/
2023-08-02 05:00       101952  s3://bk-elk/prd_elk8_vpn/index-93
2023-08-02 05:00            8  s3://bk-elk/prd_elk8_vpn/index.latest
2023-04-24 05:00          212  s3://bk-elk/prd_elk8_vpn/meta--0t2OZatTBq1vyjlMphlHQ.dat
2023-04-04 05:00          212  s3://bk-elk/prd_elk8_vpn/meta--MEOTmDFQIiOCPJAnyvIVw.dat
....{noformat}

","02/ago/23 8:47 PM;621538d1347c690072f6b56f;Prezado Elias Mussi, boa noite!

Grato pelo envio das evidências. Informo que já foi aberto um ticket com a Elastic para que seja investigado o motivo desse comportamento.

Aproveito para reforçar o pedido de alteração da severidade do chamado, uma vez que trata-se de esclarecimento de dúvidas, sendo, portanto, um chamado de severidade 4, conforme consta no item 14.1 do TR:

_d) Severidade 4 – quando se verifica como necessária a prestação de informações, aperfeiçoamentos ou_ *_esclarecimentos_* _sobre documentação ou_ *_funcionalidades, porém sem prejudicar diretamente a operação dos programas ou sistemas da DATAPREV._*



Aguardo seu retorno.

Atenciosamente,

Vinícius Prysthon ","04/ago/23 2:31 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;{noformat}                                                                                                                                                                  [29/111]
[2023-08-04T14:29:26,838][DEBUG][c.a.request              ] [n321p005266] Sending Request: PUT http://s3-sp.ecs.prevnet:9020 /bk-elk/prd_elk8_vpn/tests-epglGklTS9qfaDRkEmA
2PA/data-XxFv3Xi1SqqyaFnbXNyIvA.dat Headers: (amz-sdk-invocation-id: 98afefc7-b816-7b55-b354-35275d546f0e, Content-Length: 22, Content-Type: application/octet-stream, User
-Agent: aws-sdk-java/1.12.270 Linux/4.18.0-240.el8.x86_64 OpenJDK_64-Bit_Server_VM/20.0.1+9-29 java/20.0.1 vendor/Oracle_Corporation cfg/retry-mode/legacy, x-amz-acl: priv
ate, x-amz-storage-class: STANDARD, )                                                                                                                                      
[2023-08-04T14:29:26,840][DEBUG][c.a.a.AWS4Signer         ] [n321p005266] AWS4 Canonical Request: '""PUT                                                                    
/bk-elk/prd_elk8_vpn/tests-epglGklTS9qfaDRkEmA2PA/data-XxFv3Xi1SqqyaFnbXNyIvA.dat                                                                                          
                                                                                                                                                                           
amz-sdk-invocation-id:98afefc7-b816-7b55-b354-35275d546f0e                                                                                                                 
amz-sdk-request:attempt=1;max=4                                                                                                                                            
amz-sdk-retry:0/0/500                                                                                                                                                      
content-length:195                                                                                                                                                         
content-type:application/octet-stream                                                                                                                                      
host:s3-sp.ecs.prevnet:9020                                                                                                                                                
user-agent:aws-sdk-java/1.12.270 Linux/4.18.0-240.el8.x86_64 OpenJDK_64-Bit_Server_VM/20.0.1+9-29 java/20.0.1 vendor/Oracle_Corporation cfg/retry-mode/legacy              
x-amz-acl:private                                                                                                                                                          
x-amz-content-sha256:STREAMING-AWS4-HMAC-SHA256-PAYLOAD                                                                                                                    
x-amz-date:20230804T172926Z                                                                                                                                                
x-amz-decoded-content-length:22                                                                                                                                            
x-amz-storage-class:STANDARD                                                                                                                                               
                                                                                                                                                                           
amz-sdk-invocation-id;amz-sdk-request;amz-sdk-retry;content-length;content-type;host;user-agent;x-amz-acl;x-amz-content-sha256;x-amz-date;x-amz-decoded-content-length;x-am
z-storage-class                                                                                                                                                            
STREAMING-AWS4-HMAC-SHA256-PAYLOAD""                                                                                                                                        
[2023-08-04T14:29:26,840][DEBUG][c.a.a.AWS4Signer         ] [n321p005266] AWS4 String to Sign: '""AWS4-HMAC-SHA256                                                          
20230804T172926Z                                                                                                                                                           
20230804/sp/s3/aws4_request                                                                                                                                                
83eba77d6bc32e988a7672da2c2e8a78d4a690cf4b0575214a1665e7e543fb39""                                                                                                          
[2023-08-04T14:29:26,852][TRACE][c.a.a.AwsChunkedEncodingInputStream] [n321p005266] 109 byte read from the stream.                                                         
[2023-08-04T14:29:26,852][TRACE][c.a.a.AwsChunkedEncodingInputStream] [n321p005266] 86 byte read from the stream.                                                          
[2023-08-04T14:29:26,902][DEBUG][c.a.r.ClockSkewAdjuster  ] [n321p005266] Reported server date (from 'Date' header): Fri, 04 Aug 2023 17:29:26 GMT
[2023-08-04T14:29:26,903][DEBUG][c.a.request              ] [n321p005266] Received successful response: 200, AWS Request ID: 0a929903:1886d1dfad3:7b81b:898
[2023-08-04T14:29:26,903][DEBUG][c.a.requestId            ] [n321p005266] x-amzn-RequestId: not available
[2023-08-04T14:29:26,903][DEBUG][c.a.requestId            ] [n321p005266] AWS Request ID: 0a929903:1886d1dfad3:7b81b:898
[2023-08-04T14:29:26,903][DEBUG][c.a.requestId            ] [n321p005266] AWS Extended Request ID: 41734975e5d6afd20839d464f3fd49ead4327d919b6d0626d5f4015b754dcf25
[2023-08-04T14:29:26,903][DEBUG][c.a.latency              ] [n321p005266] ServiceName=[Amazon S3], StatusCode=[200], ServiceEndpoint=[http://s3-sp.ecs.prevnet:9020], Reque
stType=[PutObjectRequest], AWSRequestID=[0a929903:1886d1dfad3:7b81b:898], HttpClientPoolPendingCount=0, RetryCapacityConsumed=0, HttpClientPoolAvailableCount=0, RequestCou
nt=1, HttpClientPoolLeasedCount=0, ResponseProcessingTime=[0.409], ClientExecuteTime=[66.481], HttpClientSendRequestTime=[4.945], HttpRequestTime=[61.037], ApiCallLatency=
[65.56], RequestSigningTime=[0.84], CredentialsRequestTime=[0.013, 0.001], HttpClientReceiveResponseTime=[49.597], 
[2023-08-04T14:29:26,905][DEBUG][c.a.request              ] [n321p005266] Sending Request: GET http://s3-sp.ecs.prevnet:9020 /bk-elk/prd_elk8_vpn/tests-epglGklTS9qfaDRkEmA
2PA/master.dat Headers: (amz-sdk-invocation-id: 11b41c51-236f-a96e-039a-a592657e62e7, Content-Type: application/octet-stream, User-Agent: aws-sdk-java/1.12.270 Linux/4.18.
0-240.el8.x86_64 OpenJDK_64-Bit_Server_VM/20.0.1+9-29 java/20.0.1 vendor/Oracle_Corporation cfg/retry-mode/legacy, ) 
[2023-08-04T14:29:26,905][DEBUG][c.a.a.AWS4Signer         ] [n321p005266] AWS4 Canonical Request: '""GET
/bk-elk/prd_elk8_vpn/tests-epglGklTS9qfaDRkEmA2PA/master.dat

amz-sdk-invocation-id:11b41c51-236f-a96e-039a-a592657e62e7
amz-sdk-request:attempt=1;max=4
amz-sdk-retry:0/0/500
content-type:application/octet-stream
host:s3-sp.ecs.prevnet:9020
user-agent:aws-sdk-java/1.12.270 Linux/4.18.0-240.el8.x86_64 OpenJDK_64-Bit_Server_VM/20.0.1+9-29 java/20.0.1 vendor/Oracle_Corporation cfg/retry-mode/legacy
x-amz-content-sha256:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 
x-amz-date:20230804T172926Z

amz-sdk-invocation-id;amz-sdk-request;amz-sdk-retry;content-type;host;user-agent;x-amz-content-sha256;x-amz-date
e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855""
[2023-08-04T14:29:26,905][DEBUG][c.a.a.AWS4Signer         ] [n321p005266] AWS4 String to Sign: '""AWS4-HMAC-SHA256
20230804T172926Z
20230804/sp/s3/aws4_request
dd1dacabcae513f4d2cc62a007dd14374a284bb916ffedfe6cfb60ac334ecd7e""
[2023-08-04T14:29:26,918][DEBUG][c.a.r.ClockSkewAdjuster  ] [n321p005266] Reported server date (from 'Date' header): Fri, 04 Aug 2023 17:29:26 GMT
[2023-08-04T14:29:26,918][DEBUG][c.a.request              ] [n321p005266] Received successful response: 200, AWS Request ID: 0a929903:1886d1dfad3:7b930:76c
[2023-08-04T14:29:26,919][DEBUG][c.a.requestId            ] [n321p005266] x-amzn-RequestId: not available
[2023-08-04T14:29:26,919][DEBUG][c.a.requestId            ] [n321p005266] AWS Request ID: 0a929903:1886d1dfad3:7b930:76c
[2023-08-04T14:29:26,919][DEBUG][c.a.requestId            ] [n321p005266] AWS Extended Request ID: 49ca3a2dc00be581381b0c410944e36d0255312affde5ef7740df6063afda4de
[2023-08-04T14:29:26,919][DEBUG][c.a.latency              ] [n321p005266] ServiceName=[Amazon S3], StatusCode=[200], ServiceEndpoint=[http://s3-sp.ecs.prevnet:9020], Reque
stType=[GetObjectRequest], AWSRequestID=[0a929903:1886d1dfad3:7b930:76c], HttpClientPoolPendingCount=0, RetryCapacityConsumed=0, HttpClientPoolAvailableCount=1, RequestCou
nt=1, HttpClientPoolLeasedCount=0, ResponseProcessingTime=[0.304], ClientExecuteTime=[14.607], HttpClientSendRequestTime=[0.086], HttpRequestTime=[12.752], ApiCallLatency=
[14.187], RequestSigningTime=[0.405], CredentialsRequestTime=[0.001, 0.001], HttpClientReceiveResponseTime=[11.966], 
{noformat}



trace do comando : POST _snapshot/vpn/_verify","04/ago/23 2:43 PM;5f9829d0dbf337006c7dc32d;Prezado Mussi, 

Precisamos também dos logs do EMC S3 para avaliar a requisição feita pelo Elastic a solução. Em paralelo testamos com outras soluções que implementam o protocolo S3 e não obtivemos esse comportamento, ou seja, seria importante o pessoal da DELL ajudar nesse troubleshooting uma vez que os testes que fizemos em LAB obtiveram sucesso. 

Seria possível acionar o pessoa da DELL para avaliar essa situação? Gostaria de agendar contigo uma call para apresentar os testes.  Qual o melhor dia e horário? 

Att,

Rodrigo Tornis","04/ago/23 2:50 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;estou tentando com a turma q cuida do ecs aqui mais info.

me liga ae 😉","04/ago/23 2:59 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Beleza, seguimos no aguardo dessas informações da turma do ECS.

Enquanto isso, segue o link para a call:

[https://meet.google.com/sku-fqvr-ffj|https://meet.google.com/sku-fqvr-ffj|smart-link] 

Atenciosamente,

Paulo Henrique","07/ago/23 10:44 AM;621538d1347c690072f6b56f;Prezado Elias Mussi, bom dia!

Conforme conversado em reunião realizada na sexta-feira (04), ficou evidenciado um problema no ECS, sendo então decidido seguir pela utilização de múltiplos buckets.

Dessa forma, solicitamos autorização para o fechamento deste chamado.

Atenciosamente,

Vinícius Prysthon",,,,,,,,,,,,,,,,,,,,,,,,,,,,Em andamento
filebeat panic error,DATAPREV-23,10936,Suporte técnico remoto: Incidente,Aguardando cliente,DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Muito Baixa,,Paulo Henrique Morato Góes,61e09d6968926d0068d83362,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,21/jul/23 12:57 AM,26/jul/23 1:30 PM,19/set/23 4:16 PM,,,0,"Turma, to observando esse erro aqui com o input.tcp do filebeat

versao 8.8.0 e 8.8.1

{noformat}Jul 21 00:52:16 n321p004875 filebeat[1639466]: panic: runtime error: index out of range [0] with length 0
Jul 21 00:52:16 n321p004875 filebeat[1639466]: goroutine 2098 [running]:
Jul 21 00:52:16 n321p004875 filebeat[1639466]: github.com/elastic/beats/v7/filebeat/input/tcp.contains({0xc0006f2c97, 0x25, 0x25}, {0xc0006af3b0?, 0x1, 0x1?}, {0x558d7b1c7578, 0x0, 0xc000af8ef4?})
Jul 21 00:52:16 n321p004875 filebeat[1639466]: #011github.com/elastic/beats/v7/filebeat/input/tcp/input.go:353 +0x205
Jul 21 00:52:16 n321p004875 filebeat[1639466]: github.com/elastic/beats/v7/filebeat/input/tcp.procNetTCP({0x558d77e585a7, 0xe}, {0xc0006af3b0, 0x1, 0x1}, 0x0, {0x558d7b1c7578, 0x0, 0x0})
Jul 21 00:52:16 n321p004875 filebeat[1639466]: #011github.com/elastic/beats/v7/filebeat/input/tcp/input.go:323 +0x315
Jul 21 00:52:16 n321p004875 filebeat[1639466]: github.com/elastic/beats/v7/filebeat/input/tcp.(*inputMetrics).poll(0xc0003033b0, {0xc0006af3a0, 0x0, 0x1}, {0xc0006af3b0, 0x1, 0x1}, 0xc000aef613?, 0xc0006af190)
Jul 21 00:52:16 n321p004875 filebeat[1639466]: #011github.com/elastic/beats/v7/filebeat/input/tcp/input.go:267 +0x32e
Jul 21 00:52:16 n321p004875 filebeat[1639466]: created by github.com/elastic/beats/v7/filebeat/input/tcp.newInputMetrics
Jul 21 00:52:16 n321p004875 filebeat[1639466]: #011github.com/elastic/beats/v7/filebeat/input/tcp/input.go:228 +0xa35
Jul 21 00:52:16 n321p004875 systemd[1]: filebeat.service: Main process exited, code=exited, status=2/INVALIDARGUMENT
Jul 21 00:52:16 n321p004875 systemd[1]: filebeat.service: Failed with result 'exit-code'.
Jul 21 00:52:16 n321p004875 systemd[1]: filebeat.service: Service RestartSec=100ms expired, scheduling restart.
Jul 21 00:52:16 n321p004875 systemd[1]: filebeat.service: Scheduled restart job, restart counter is at 5.
{noformat}



{noformat}- type: tcp
  host: ""0.0.0.0:5999""
  enabled: true
{noformat}



mesmo sem trafego chegando nessa porta de tempo e tempo o filebeat da um restart com esse erro acima.

se mudo de tcp para udp o erro para",,Paulo Henrique Morato Góes,,,,,61e09d6968926d0068d83362,,,,,,,,,,,,,21/jul/23 10:10 AM;ug:0e9c7acd-4c78-4ef6-beb2-b00f34bddd59;image-20230721-130828.png;https://bktech.atlassian.net/rest/api/3/attachment/content/11034,26/jul/23 12:09 PM;ug:0e9c7acd-4c78-4ef6-beb2-b00f34bddd59;image-20230726-150935.png;https://bktech.atlassian.net/rest/api/3/attachment/content/11035,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i005p7:,,português,,,,,,,,,,,,,,,23:03,,,,,,,,2023-07-21 13:10:34.476,,"21/jul/23 10:10 AM;61e09d6968926d0068d83362;Prezado Elias Mussi, bom dia.

Esse comportamento realmente foi observado pela Elastic e notou-se ser um bug mesmo. A Elastic adicionou esse bug ao repositório do GitHub deles:

[https://github.com/elastic/beats/pull/35637|https://github.com/elastic/beats/pull/35637|smart-link] 

Pelo que eu vi aqui, esse bug tinha ficado para ser fixado na versão 8.8.2, que foi lançada dia 29/06/2023. E no patch notes está relatado que foi fixado:

[https://www.elastic.co/guide/en/beats/libbeat/8.8/release-notes-8.8.2.html|https://www.elastic.co/guide/en/beats/libbeat/8.8/release-notes-8.8.2.html|smart-link] 

!image-20230721-130828.png|width=850,height=277!

Sendo assim, como você relatou estar utilizando a versão 8.8.0 e 8.8.1, basta que a versão seja atualizada para que o bug não ocorra mais.

Atenciosamente,

Paulo Henrique","21/jul/23 7:06 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Boas,

vi isso tb …  coloquei a 8.8.2 para testar e parou de restartar com panic … o unico colateral nesse teste foi : 

{noformat}Jul 21 10:56:07 n321p004875 filebeat[1650953]: {""log.level"":""warn"",""@timestamp"":""2023-07-21T13:56:07.936Z"",""log.logger"":""input.tcp"",""log.origin"":{""file.name"":""tcp/input.go"",""file.line"":299},""message"":""failed to get tcp6 stats from /proc: /proc/net/tcp6 entry not found for [00000000FFFF00000000000000000000:283F]"",""service.name"":""filebeat"",""id"":""6EB4C102F4E0399A"",""host"":""0.0.0.0:10303"",""ecs.version"":""1.6.0""}
Jul 21 10:56:07 n321p004875 filebeat[1650953]: {""log.level"":""warn"",""@timestamp"":""2023-07-21T13:56:07.936Z"",""log.logger"":""input.tcp"",""log.origin"":{""file.name"":""tcp/input.go"",""file.line"":299},""message"":""failed to get tcp6 stats from /proc: /proc/net/tcp6 entry not found for [00000000FFFF00000000000000000000:2845]"",""service.name"":""filebeat"",""id"":""5872C6C9A4153FA7"",""host"":""0.0.0.0:10309"",""ecs.version"":""1.6.0""}
Jul 21 10:56:07 n321p004875 filebeat[1650953]: {""log.level"":""warn"",""@timestamp"":""2023-07-21T13:56:07.940Z"",""log.logger"":""input.tcp"",""log.origin"":{""file.name"":""tcp/input.go"",""file.line"":299},""message"":""failed to get tcp6 stats from /proc: /proc/net/tcp6 entry not found for [00000000FFFF00000000000000000000:176F]"",""service.name"":""filebeat"",""id"":""61E9ADA6C2E2638E"",""host"":""0.0.0.0:5999"",""ecs.version"":""1.6.0""}
Jul 21 10:56:07 n321p004875 filebeat[1650953]: {""log.level"":""warn"",""@timestamp"":""2023-07-21T13:56:07.940Z"",""log.logger"":""input.tcp"",""log.origin"":{""file.name"":""tcp/input.go"",""file.line"":299},""message"":""failed to get tcp6 stats from /proc: /proc/net/tcp6 entry not found for [00000000FFFF00000000000000000000:2840]"",""service.name"":""filebeat"",""id"":""C14B93EF60A1E5FC"",""host"":""0.0.0.0:10304"",""ecs.version"":""1.6.0""}
Jul 21 10:56:07 n321p004875 filebeat[1650953]: {""log.level"":""warn"",""@timestamp"":""2023-07-21T13:56:07.941Z"",""log.logger"":""input.tcp"",""log.origin"":{""file.name"":""tcp/input.go"",""file.line"":299},""message"":""failed to get tcp6 stats from /proc: /proc/net/tcp6 entry not found for [00000000FFFF00000000000000000000:284B]"",""service.name"":""filebeat"",""id"":""5E0A30A3AE8C0514"",""host"":""0.0.0.0:10315"",""ecs.version"":""1.6.0""}
Jul 21 10:56:07 n321p004875 filebeat[1650953]: {""log.level"":""warn"",""@timestamp"":""2023-07-21T13:56:07.941Z"",""log.logger"":""input.tcp"",""log.origin"":{""file.name"":""tcp/input.go"",""file.line"":299},""message"":""failed to get tcp6 stats from /proc: /proc/net/tcp6 entry not found for [00000000FFFF00000000000000000000:2848]"",""service.name"":""filebeat"",""id"":""838FE3E8F8D8995B"",""host"":""0.0.0.0:10312"",""ecs.version"":""1.6.0""}
{noformat}

que fica aparecendo para dedeu heheh  nao trataram ipv6 direito :)



outra coisa … to percebendo q no 8.8.2 o modulo do paloalto para de funcionar depois de algum tempo….



ta estranho … minha impressao é q a versao nao ta aguentando o tranco como a antiga



conseguem mapear se isso procede ?  tente em prod e tive de voltar para 8.8.1 pq simplesmente a fila morria



e ae o bagulho fica grave","22/jul/23 10:36 AM;61e09d6968926d0068d83362;Prezado Elias Mussi, bom dia.

Esse warning que está aparecendo é um comportamento que precisamos investigar melhor porque por se tratar de um comportamento de uma atualização recente não possuem muitas informações a respeito. Vi que você até ja reportou no git da Elastic, e também vou abrir um chamado na Elastic. Para adiantar, gostaria de pedir já o arquivo de configuração do filebeat porque com certeza vai ser algo que vão pedir.

Também quero solicitar que você nos envie um arquivo sos report que coleta alguns diagnósticos do sistema e serviços do SO. Segue abaixo um link explicando como gerar esse arquivo:

[https://www.thegeekdiary.com/centos-rhel-how-to-collect-sosreport/|https://www.thegeekdiary.com/centos-rhel-how-to-collect-sosreport/|smart-link]

Atenciosamente,

Paulo Henrique","26/jul/23 12:09 PM;61e09d6968926d0068d83362;Prezado Elias Mussi, boa tarde.

Hoje a Elastic lançou uma nova versão, a 8.9, onde mais uma vez trouxeram uma atualização no filebeat a respeito do IPv6 e TCP e UDP. Poderia testar a atualização e ver se foi corrigido o erro que estava aparecendo?

[https://www.elastic.co/guide/en/beats/libbeat/current/release-notes-8.9.0.html|https://www.elastic.co/guide/en/beats/libbeat/current/release-notes-8.9.0.html|smart-link] 

!image-20230726-150935.png|width=848,height=421!

Atenciosamente,

Paulo Henrique","26/jul/23 1:16 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;oremos 🙂 

sei q nao tem uma resposta , mas vai que …

alguma previsao de saida da 8.9.1 ? essa leva 8.8 foi traumatizante … ta dando vontade de esperar antes de virar o cluster","26/jul/23 1:30 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Não existe uma previsão de lançamento da 8.9.1 pois a Elastic não divulga as datas previstas para seus lançamentos. Contudo, a princípio, o erro a respeito do presente chamado foi resolvido na 8.9.

Entendo a sua preocupação por a 8.8 ter trazido algumas dores de cabeça, mas o que posso informar a respeito de futuros lançamentos é: observando as datas de lançamentos anteriores, quando existe algum erro maior, o release seguinte sai entre 2 ou 3 semanas, porém quando não existe, demora entre 5 e 6 semanas para versões X.X.X.

Atenciosamente,

Paulo Henrique",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Em andamento
falha na ingestao de logs do postgresql,DATAPREV-22,10932,Suporte técnico remoto: Incidente,Aguardando cliente,DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Muito Baixa,,Vinícius Prysthon,621538d1347c690072f6b56f,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,27/jun/23 5:06 PM,04/jul/23 10:40 AM,19/set/23 4:16 PM,,,0,"Turma,

fomos adicionar um server postgresql (11.3), e nao obtive sucesso …

(seguimos a doc q fica la no integrations do kibana, com log em csv e tals)

o log e gravado com um field : error.message contenco : 

{{Provided Grok expressions do not match field value: [2023-06-27 16:42:30.442 -03,,,26231,\""10.70.145.71:30418\"",649b3ba6.6677,1,\""\"",2023-06-27 16:42:30 -03,,0,LOG,00000,\""connection received: host=10.70.145.71 port=30418\"",,,,,,,,,\""\""]}}



o que pode ser ?

mais detalhes



fui testar o ingest pepiline : 

* [
* {
* ""_index"": ""index"",
* ""_id"": ""id"",
* ""_source"": {
* ""message"": ""2023-06-27 16:06:43.160 -03,,,20176,,649b2b00.4ed0,14,,2023-06-27 15:31:28 -03,,0,LOG,00000,""checkpoint complete: wrote 133 buffers (0.8%); 0 WAL file(s) added, 0 removed, 0 recycled; write=13.332 s, sync=0.000 s, total=13.337 s; sync files=27, longest=0.000 s, average=0.000 s; distance=565 kB, estimate=565 kB"",,,,,,,,,""""""
* }
* }
* ]

e tive esse erro aqui : 

* ´´´{
""docs"": [ 
{ 
""processor_results"": [ 
{ 
""processor_type"": ""csv"", 
""status"": ""success"", 
""doc"": { 
""_index"": ""index"", 
""_id"": ""id"", 
""_version"": ""-3"", 
""_source"": { 
""message"": ""2023-06-27 15:56:34.645 -03,,,20176,,649b2b00.4ed0,10,,2023-06-27 15:31:28 -03,,0,LOG,00000,""checkpoint complete: wrote 48 buffers (0.3%); 0 WAL file(s) added, 0 removed, 0 recycled; write=4.812 s, sync=0.001 s, total=4.821 s; sync files=14, longest=0.001 s, average=0.000 s; distance=269 kB, estimate=309 kB"",,,,,,,,,"""""" 
}, 
""_ingest"": { 
""pipeline"": ""_simulate_pipeline"", 
""timestamp"": ""2023-06-27T19:01:55.174Z"" 
} 
} 
}, 
{ 
""processor_type"": ""remove"", 
""status"": ""success"", 
""doc"": { 
""_index"": ""index"", 
""_id"": ""id"", 
""_version"": ""-3"", 
""_source"": {}, 
""_ingest"": { 
""pipeline"": ""_simulate_pipeline"", 
""timestamp"": ""2023-06-27T19:01:55.174Z"" 
} 
} 
}, 
{ 
""processor_type"": ""grok"", 
""status"": ""success"", 
""doc"": { 
""_index"": ""index"", 
""_id"": ""id"", 
""_version"": ""-3"", 
""_source"": {}, 
""_ingest"": { 
""pipeline"": ""_simulate_pipeline"", 
""timestamp"": ""2023-06-27T19:01:55.174Z"" 
} 
} 
}, 
{ 
""processor_type"": ""convert"", 
""status"": ""success"", 
""doc"": { 
""_index"": ""index"", 
""_id"": ""id"", 
""_version"": ""-3"", 
""_source"": {}, 
""_ingest"": { 
""pipeline"": ""_simulate_pipeline"", 
""timestamp"": ""2023-06-27T19:01:55.174Z"" 
} 
} 
}, 
{ 
""processor_type"": ""date"", 
""status"": ""error"", 
""error"": { 
""root_cause"": [ 
{ 
""type"": ""illegal_argument_exception"", 
""reason"": ""field [tempcsv] not present as part of path [tempcsv.session_start_time]"" 
} 
], 
""type"": ""illegal_argument_exception"", 
""reason"": ""field [tempcsv] not present as part of path [tempcsv.session_start_time]"" 
} 
} 
] 
} 
] 
}´´´

mais algumas linhas de log  : 

* 2023-06-27 16:11:29.181 -03,,,20176,,649b2b00.4ed0,15,,2023-06-27 15:31:28 -03,,0,LOG,00000,""checkpoint starting: time"",,,,,,,,,""""
2023-06-27 16:11:32.093 -03,,,20176,,649b2b00.4ed0,16,,2023-06-27 15:31:28 -03,,0,LOG,00000,""checkpoint complete: wrote 29 buffers (0.2%); 0 WAL file(s) added, 0 removed, 0 recycled; write=2.907 s, sync=0.000 s, total=2.911 s; sync files=13, longest=0.000 s, average=0.000 s; distance=131 kB, estimate=522 kB"",,,,,,,,,"""" 
2023-06-27 16:12:17.324 -03,""geridaudit_intranet"",""hgeraud"",20332,""10.0.186.67:58294"",649b2b57.4f6c,3,""idle"",2023-06-27 15:32:55 -03,,0,LOG,00000,""disconnection: session time: 0:39:21.486 user=geridaudit_intranet database=hgeraud host=10.0.186.67 port=58294"",,,,,,,,,""PostgreSQL JDBC Driver"" 
2023-06-27 16:12:17.344 -03,""geridaudit_intranet"",""hgeraud"",20331,""10.0.186.67:58292"",649b2b57.4f6b,3,""idle"",2023-06-27 15:32:55 -03,,0,LOG,00000,""disconnection: session time: 0:39:21.650 user=geridaudit_intranet database=hgeraud host=10.0.186.67 port=58292"",,,,,,,,,""PostgreSQL JDBC Driver"" 
2023-06-27 16:12:24.878 -03,""geridaudit_intranet"",""hgeraud"",20219,""10.0.186.118:44578"",649b2b19.4efb,3,""idle"",2023-06-27 15:31:53 -03,,0,LOG,00000,""disconnection: session time: 0:40:31.571 user=geridaudit_intranet database=hgeraud host=10.0.186.118 port=44578"",,,,,,,,,""PostgreSQL JDBC Driver"" 
2023-06-27 16:12:27.666 -03,""geridaudit_intranet"",""hgeraud"",20245,""10.0.186.119:33258"",649b2b25.4f15,3,""idle"",2023-06-27 15:32:05 -03,,0,LOG,00000,""disconnection: session time: 0:40:22.618 user=geridaudit_intranet database=hgeraud host=10.0.186.119 port=33258"",,,,,,,,,""PostgreSQL JDBC Driver"" 
2023-06-27 16:12:30.260 -03,""geridaudit_intranet"",""hgeraud"",20260,""10.0.186.120:57848"",649b2b2c.4f24,3,""idle"",2023-06-27 15:32:12 -03,,0,LOG,00000,""disconnection: session time: 0:40:18.225 user=geridaudit_intranet database=hgeraud host=10.0.186.120 port=57848"",,,,,,,,,""PostgreSQL JDBC Driver"" 
2023-06-27 16:12:41.939 -03,""geridaudit_intranet"",""hgeraud"",20309,""10.0.186.121:49658"",649b2b42.4f55,3,""idle"",2023-06-27 15:32:34 -03,,0,LOG,00000,""disconnection: session time: 0:40:07.181 user=geridaudit_intranet database=hgeraud host=10.0.186.121 port=49658"",,,,,,,,,""PostgreSQL JDBC Driver"" 
2023-06-27 16:12:41.940 -03,""geridaudit_intranet"",""hgeraud"",20308,""10.0.186.121:49656"",649b2b42.4f54,3,""idle"",2023-06-27 15:32:34 -03,,0,LOG,00000,""disconnection: session time: 0:40:07.326 user=geridaudit_intranet database=hgeraud host=10.0.186.121 port=49656"",,,,,,,,,""PostgreSQL JDBC Driver"" 
2023-06-27 16:16:29.186 -03,,,20176,,649b2b00.4ed0,17,,2023-06-27 15:31:28 -03,,0,LOG,00000,""checkpoint starting: time"",,,,,,,,,"""" 
2023-06-27 16:16:32.799 -03,,,20176,,649b2b00.4ed0,18,,2023-06-27 15:31:28 -03,,0,LOG,00000,""checkpoint complete: wrote 36 buffers (0.2%); 0 WAL file(s) added, 0 removed, 0 recycled; write=3.609 s, sync=0.000 s, total=3.613 s; sync files=14, longest=0.000 s, average=0.000 s; distance=189 kB, estimate=488 kB"",,,,,,,,,""""

",,Fabio Schmidt,Paulo Henrique Morato Góes,Vinícius Prysthon,,,5ea9bfda0590bb0b7bedbaa1,61e09d6968926d0068d83362,621538d1347c690072f6b56f,,,,,,,,,,,28/jun/23 11:55 AM;ug:0e9c7acd-4c78-4ef6-beb2-b00f34bddd59;Screenshot 2023-06-28 at 7.30.14 AM.png;https://bktech.atlassian.net/rest/api/3/attachment/content/11023,29/jun/23 10:15 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;local-diagnostics-20230629-125958.zip;https://bktech.atlassian.net/rest/api/3/attachment/content/11024,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i005ob:,,português,ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb(ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb),ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77(ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77),fabio(fabio),,,,,,,,,,,,-9:35,,,,,,,,2023-06-27 20:30:27.287,,"27/jun/23 5:30 PM;61e09d6968926d0068d83362;Prezado Elias Mussi, boa tarde.

Parece que você está enfrentando um problema ao adicionar um servidor PostgreSQL ao Elasticsearch usando o Kibana. O log de erro indica que as expressões Grok fornecidas não correspondem ao valor do campo.

A partir da amostra do log de erro que você forneceu, não está claro qual parte desse valor você está tentando extrair usando as expressões Grok. Para ajudar a resolver o problema, precisaremos saber qual é o padrão Grok que você está usando atualmente. Se você puder fornecer mais informações sobre o padrão Grok que está usando, posso ajudá-lo a verificar se há algum problema nele e sugerir uma solução. Também é útil saber se você fez alguma modificação nas configurações padrão do Elasticsearch e do Kibana relacionadas à integração com o PostgreSQL.



Atenciosamente,

Paulo Henrique","27/jun/23 7:18 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;modulo postgresql do filebeat

segui o padrao q esta na doc de integração

sem modificações","28/jun/23 10:21 AM;61e09d6968926d0068d83362;Prezado Elias Mussi, bom dia.

Certo, sendo assim, pode ter alguma questão em relação ao ingest pipeline que está sendo utilizado. Peço para que me envie a configuração do ingest pipeline que utilizou para testar.



Atenciosamente,

Paulo Henrique","28/jun/23 11:55 AM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Em chamado com a Elastic a respeito desse tema eles pediram uma amostra dos logs de csv originais para darem uma olhada.

Para ter o evento original no documento para solução de problemas, você também pode habilitar Preserve original event na política de integração de log do PostgreSQL.



!Screenshot 2023-06-28 at 7.30.14 AM.png|width=798,height=323!

Então você deverá conseguir ver o evento original no documento. Por favor, vá para a seção ""Discover"", expanda esse evento e obtenha o JSON do documento completo para que possamos dar uma olhada.



Atenciosamente,

Paulo Henrique","28/jun/23 1:03 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;velho … laaaaaa qdo abri o chamado ja coloquei um monte de evento original do banco.

ajuda neh",28/jun/23 1:04 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;fora q meu setup e no filebeat direto e nao com elastic-agent,"28/jun/23 2:07 PM;61e09d6968926d0068d83362;Os logs foram enviados para a Elastic. Mas sigo aguardando o ingest pipeline que também foi requisitado anteriormente.

Atenciosamente,

Paulo Henrique","28/jun/23 2:55 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;é a pipeline do filebeat 

nao criei nem mexi em nada

to me sentindo um cachorrinho correndo atras do rabo","28/jun/23 10:33 PM;61e09d6968926d0068d83362;Segue resposta da Elastic:

Parece que as mensagens originais não podem ser analisadas corretamente usando os pipelines de ingestão padrão.



Você poderia, por favor, coletar os seguintes dados?



Dados de diagnóstico do cluster, que incluirão o pipeline de ingestão e estatísticas do nó sobre o pipeline de ingestão, quantos erros ocorreram nos pipelines de ingestão. Aqui está a base de conhecimento sobre a coleta de dados de diagnóstico do cluster: [https://support.elastic.co/knowledge/0HYIte2u|https://support.elastic.co/knowledge/0HYIte2u|smart-link]

Dados de diagnóstico do agente, que fornecerão os detalhes da política de integração. Aqui está o guia: [https://www.elastic.co/guide/en/fleet/current/fleet-troubleshooting.html#_collect_elastic_agent_diagnostics_bundle|https://www.elastic.co/guide/en/fleet/current/fleet-troubleshooting.html#_collect_elastic_agent_diagnostics_bundle|smart-link]

Obrigado.","29/jun/23 10:16 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;[^local-diagnostics-20230629-125958.zip]

segue o diagnostico  gigante o arquivo

foi rodado no ambiente onde estou testando a ingestao de dados antes de mandar para prod

qto ao agente, vc sabe q nao usamos o elastic-agent aqui neh ! me ajuda a te ajudar.","29/jun/23 11:04 AM;5ea9bfda0590bb0b7bedbaa1;Prezado Elias Mussi, bom dia!

Grato pelo envio do diagnóstico do cluster.

A Elastic solicita também o diagnóstico do agente porque quando solicitado não foi enviado nem o arquivo do módulo ativo no filebeat nem o arquivo do ingest pipeline utilizado. 

Sabemos que foi informado terem utilizado os padrões da documentação, porém são arquivos necessários como evidências para acelerar o processo de investigação e resolução da situação, até mesmo para poderem executar testes e revalidarem os passos que estão citados na documentação.

Agradecemos a colaboração e solicitamos o envio do arquivo de configuração do módulo e o ingest pipeline. ","29/jun/23 12:36 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;{noformat}# Module: postgresql
# Docs: https://www.elastic.co/guide/en/beats/filebeat/8.5/filebeat-module-postgresql.html

- module: postgresql
  # All logs
  log:
    enabled: true

    # Set custom paths for the log files. If left empty,
    # Filebeat will choose the paths depending on your OS.
    #var.paths:
    var.paths:
      - ""/pglog1/*/*.csv""

    input.fields_under_root: true
    input.fields:
      dtp.cp: ""cpdf""
      dtp.ambiente: ""homolog""
      dtp.cliente: ""dtp""
      dtp.sistema: ""hgeraud""
      dtp.categoria: ""bancodedados""
      dtp.type: ""postgresql""
{noformat}



o pipeline é o original 

deve ser esse filebeat-8.8.1-postgresql-log-pipeline



as conf do banco foram :

{{logging_collector = 'on'; }}

{{log_destination = 'csvlog';}}

{{log_statement = 'none';}}

{{log_checkpoints = on;}}

{{log_connections = on;}}

{{log_disconnections = on;}}

{{log_lock_waits = on;}}

{{log_min_duration_statement = 2000;}}","30/jun/23 12:49 AM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Estava procurando coisas que poderia auxiliar a encontrar o motivo do problema e acabei encontrando no github público da elastic arquivos de logs que eles utiizaram para testar os pipelines para ingestão de dados do postgresql.

Vou anexar um deles que se encaixa no seu cenário e gostaria de solicitar que você teste a ingestão desses logs para que a gente possa saber se o problema que está ocorrendo é algo relacionado ao pipeline que está sendo utilizado ou alguma questão de configurações necessárias no banco do postgresql.

{noformat}2021-01-03 20:00:46.695 UTC,,,28,,5ff2226e.1c,1,,2021-01-03 20:00:46 UTC,,0,LOG,00000,""database system was shut down at 2021-01-03 20:00:42 UTC"",,,,,,,,,""""
2021-01-03 20:00:46.708 UTC,,,1,,5ff2226e.1,1,,2021-01-03 20:00:46 UTC,,0,LOG,00000,""database system is ready to accept connections"",,,,,,,,,""""
2021-01-03 20:01:00.349 UTC,,,35,""172.24.0.1:38352"",5ff2227c.23,1,"""",2021-01-03 20:01:00 UTC,,0,LOG,00000,""connection received: host=172.24.0.1 port=38352"",,,,,,,,,""""
2021-01-03 20:01:02.701 UTC,,,36,""172.24.0.1:38356"",5ff2227e.24,1,"""",2021-01-03 20:01:02 UTC,,0,LOG,00000,""connection received: host=172.24.0.1 port=38356"",,,,,,,,,""""
2021-01-03 20:01:02.727 UTC,""postgres"",""postgres"",36,""172.24.0.1:38356"",5ff2227e.24,2,""authentication"",2021-01-03 20:01:02 UTC,3/2,0,LOG,00000,""connection authorized: user=postgres database=postgres"",,,,,,,,,""""
2021-01-03 20:01:07.094 UTC,""postgres"",""postgres"",36,""172.24.0.1:38356"",5ff2227e.24,3,""idle"",2021-01-03 20:01:02 UTC,3/3,0,LOG,00000,""statement: SELECT 1;"",,,,,,,,,""psql""
2021-01-03 20:01:07.724 UTC,""postgres"",""postgres"",36,""172.24.0.1:38356"",5ff2227e.24,4,""idle"",2021-01-03 20:01:02 UTC,,0,LOG,00000,""disconnection: session time: 0:00:05.023 user=postgres database=postgres host=172.24.0.1 port=38356"",,,,,,,,,""psql""
2021-01-03 20:01:08.894 UTC,,,1,,5ff2226e.1,2,,2021-01-03 20:00:46 UTC,,0,LOG,00000,""received smart shutdown request"",,,,,,,,,""""
2021-01-03 20:01:08.899 UTC,,,1,,5ff2226e.1,3,,2021-01-03 20:00:46 UTC,,0,LOG,00000,""background worker """"logical replication launcher"""" (PID 34) exited with exit code 1"",,,,,,,,,""""
2021-01-03 20:01:08.899 UTC,,,29,,5ff2226e.1d,1,,2021-01-03 20:00:46 UTC,,0,LOG,00000,""shutting down"",,,,,,,,,""""
2021-01-03 20:01:08.901 UTC,,,29,,5ff2226e.1d,2,,2021-01-03 20:00:46 UTC,,0,LOG,00000,""checkpoint starting: shutdown immediate"",,,,,,,,,""""
2021-01-03 20:01:08.910 UTC,,,29,,5ff2226e.1d,3,,2021-01-03 20:00:46 UTC,,0,LOG,00000,""checkpoint complete: wrote 0 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.000 s, sync=0.000 s, total=0.010 s; sync files=0, longest=0.000 s, average=0.000 s; distance=0 kB, estimate=0 kB"",,,,,,,,,""""
2021-01-03 20:01:08.919 UTC,,,1,,5ff2226e.1,4,,2021-01-03 20:00:46 UTC,,0,LOG,00000,""database system is shut down"",,,,,,,,,""""{noformat}

Atenciosamente,

Paulo Henrique","30/jun/23 10:09 AM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Também gostaria de adicionar perguntas que foram realizadas pela Elastic a respeito do chamado:

P: Além das mensagens de erro, o que especificamente está falhando e como está falhando? Falha do trabalho (job) ou o painel (dashboard) não está carregando, etc. Algumas capturas de tela seriam ótimas para esclarecer nossa compreensão do problema atual.

P :Como vocês não estão usando o agente, vocês podem compartilhar o arquivo filebeat.yml, a configuração e o log do filebeat. (Habilitar o log DEBUG seria melhor).

Atenciosamente,

Paulo Henrique","01/jul/23 10:46 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;ou …



testem ae tb : 

{noformat}PUT _ingest/pipeline/filebeat-8.8.1-postgresql-log-pipeline
{
  ""description"": ""Pipeline for parsing PostgreSQL logs."",
  ""processors"": [
    {
      ""set"": {
        ""field"": ""event.ingested"",
        ""value"": ""{{_ingest.timestamp}}""
      }
    },
    {
      ""grok"": {
        ""field"": ""message"",
        ""patterns"": [
          ""^%{DATETIME:postgresql.log.timestamp}%{CHAR:separator}%{GREEDYDATA:raw_message}""
        ],
        ""pattern_definitions"": {
          ""DATETIME"": ""%{TIMESTAMP_ISO8601} %{NUMBER:event.timezone}"",
          ""CHAR"": ""."",
          ""GREEDYDATA"": ""(.|\n|\t)*""
        }
      }
    },
    {
      ""pipeline"": {
        ""if"": ""ctx.separator != ','"",
        ""name"": ""filebeat-8.8.1-postgresql-log-pipeline-log""
      }
    },
    {
      ""pipeline"": {
        ""name"": ""filebeat-8.8.1-postgresql-log-pipeline-csv"",
        ""if"": ""ctx.separator == ','""
      }
    },
    {
      ""date"": {
        ""field"": ""postgresql.log.timestamp"",
        ""formats"": [
          ""yyyy-MM-dd HH:mm:ss.SSS zz"",
          ""yyyy-MM-dd HH:mm:ss zz"",
          ""yyyy-MM-dd HH:mm:ss.SSS X"",
          ""yyyy-MM-dd HH:mm:ss X""
        ],
        ""target_field"": ""@timestamp""
      }
    },
    {
      ""script"": {
        ""lang"": ""painless"",
        ""source"": ""ctx.event.duration = Math.round(ctx.temp.duration * params.scale)"",
        ""params"": {
          ""scale"": 1000000
        },
        ""if"": ""ctx.temp?.duration != null""
      }
    },
    {
      ""remove"": {
        ""field"": ""temp.duration"",
        ""ignore_missing"": true
      }
    },
    {
      ""set"": {
        ""value"": ""event"",
        ""field"": ""event.kind""
      }
    },
    {
      ""append"": {
        ""field"": ""event.category"",
        ""value"": [
          ""database""
        ]
      }
    },
    {
      ""set"": {
        ""field"": ""event.type"",
        ""value"": [
          ""info""
        ],
        ""if"": ""ctx?.postgresql?.log?.sql_state_code == null || (ctx.postgresql.log.sql_state_code ==~ /^0[012].*/)""
      }
    },
    {
      ""set"": {
        ""field"": ""event.type"",
        ""value"": [
          ""error""
        ],
        ""if"": ""ctx?.postgresql?.log?.sql_state_code != null && ! (ctx.postgresql.log.sql_state_code ==~ /^0[012].*/)""
      }
    },
    {
      ""append"": {
        ""field"": ""related.user"",
        ""value"": ""{{user.name}}"",
        ""if"": ""ctx?.user?.name != null""
      }
    },
    {
      ""remove"": {
        ""field"": [
          ""separator""
        ]
      }
    },
    {
      ""remove"": {
        ""field"": [
          ""raw_message""
        ]
      }
    }
  ],
  ""on_failure"": [
    {
      ""set"": {
        ""field"": ""error.message"",
        ""value"": ""{{ _ingest.on_failure_message }}""
      }
    }
  ]
}{noformat}



comparando com o original, mexi nas linhas : 18 e adicionei as 42 e 43","01/jul/23 11:33 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;de forma mais simples 🙂 

alterei o ingest filebeat-8.8.1-postgresql-log-pipeline

dessa forma

{noformat}--- /dev/fd/63	2023-07-01 11:31:36.741676764 -0300
+++ /dev/fd/62	2023-07-01 11:31:36.741676764 -0300
@@ -10,7 +10,7 @@
       ""field"": ""message"",
       ""pattern_definitions"": {
         ""CHAR"": ""."",
-        ""DATETIME"": ""[-0-9]+ %{TIME} %{WORD:event.timezone}"",
+        ""DATETIME"": ""%{TIMESTAMP_ISO8601} %{NUMBER:event.timezone}"",
         ""GREEDYDATA"": ""(.|\n|\t)*""
       },
       ""patterns"": [
@@ -35,7 +35,9 @@
       ""field"": ""postgresql.log.timestamp"",
       ""formats"": [
         ""yyyy-MM-dd HH:mm:ss.SSS zz"",
-        ""yyyy-MM-dd HH:mm:ss zz""
+        ""yyyy-MM-dd HH:mm:ss zz"",
+        ""yyyy-MM-dd HH:mm:ss.SSS X"",
+        ""yyyy-MM-dd HH:mm:ss X""
       ],
       ""target_field"": ""@timestamp""
     }
{noformat}

","01/jul/23 11:48 AM;621538d1347c690072f6b56f;Prezado Elias Mussi, bom dia!

Tivemos retorno da Elastic nessa manhã, e alguns pontos foram observados:

* Nas configs do banco, parece faltar o parâmetro ‘log_line_prefix’. Você chegou a setar esse parâmetro?

{noformat}log_line_prefix = '%m [%p] %q%u@%d '{noformat}

* Parece faltar também o parâmetro ‘log_duration’:

{noformat}log_duration = 'on'{noformat}

Essas configurações estão disponíveis em: [https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-postgresql.html#_supported_log_formats|https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-postgresql.html#_supported_log_formats|smart-link]

Sobre a mensagem enviada:

{noformat}""message"": ""2023-06-27 16:06:43.160 -03,,,20176,,649b2b00.4ed0,14,,2023-06-27 15:31:28 -03,,0,LOG,00000,""checkpoint complete: wrote 133 buffers (0.8%); 0 WAL file(s) added, 0 removed, 0 recycled; write=13.332 s, sync=0.000 s, total=13.337 s; sync files=27, longest=0.000 s, average=0.000 s; distance=565 kB, estimate=565 kB"",,,,,,,,,""""""
{noformat}

Nos foi passado isso:

“Parece (posso estar errado) que há um problema ao preencher as colunas no final do arquivo CSV. Eu consigo ver que o pipeline de ingestão espera que 23 campos estejam separados por vírgulas, mas parece que temos cerca de 29 em sua mensagem de exemplo (contando as vírgulas vazias).”

Poderia, por gentileza, avaliar esses pontos e nos dar um retorno?

Atenciosamente,

Vinícius Prysthon 

","01/jul/23 11:52 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;no banco ta com 

log_line_prefix = '%t_%u_%p_%c_%x_' 



e o duration off



usa essas linhas mais recentes

{noformat}2023-07-01 00:20:56.337 -03,,,12394,,649c984c.306a,651,,2023-06-28 17:30:04 -03,,0,LOG,00000,""checkpoint starting: time"",,,,,,,,,""""
2023-07-01 00:21:00.152 -03,,,12394,,649c984c.306a,652,,2023-06-28 17:30:04 -03,,0,LOG,00000,""checkpoint complete: wrote 38 buffers (0.2%); 0 WAL file(s) added, 0 removed, 0 recycled; write=3.811 s, sync=0.001 s, total=3.815 s; sync files=13, longest=0.001 s, average=0.000 s; distance=180 kB, estimate=235 kB"",,,,,,,,,""""
2023-07-01 10:48:35.143 -03,,,57571,""10.146.17.67:51518"",64a02eb3.e0e3,1,"""",2023-07-01 10:48:35 -03,,0,LOG,00000,""connection received: host=10.146.17.67 port=51518"",,,,,,,,,""""
2023-07-01 10:48:35.179 -03,""ucd_aut"",""hgeraud"",57571,""10.146.17.67:51518"",64a02eb3.e0e3,2,""authentication"",2023-07-01 10:48:35 -03,63/2339,0,LOG,00000,""connection authorized: user=ucd_aut database=hgeraud"",,,,,,,,,""""
2023-07-01 10:48:35.247 -03,""ucd_aut"",""hgeraud"",57571,""10.146.17.67:51518"",64a02eb3.e0e3,3,""BIND"",2023-07-01 10:48:35 -03,63/2342,0,ERROR,42501,""permission denied for function get_nproc"",,,,,,""select get_nproc()"",,,""PostgreSQL JDBC Driver""
2023-07-01 10:48:51.421 -03,""ucd_aut"",""hgeraud"",57571,""10.146.17.67:51518"",64a02eb3.e0e3,4,""idle"",2023-07-01 10:48:35 -03,,0,LOG,00000,""disconnection: session time: 0:00:16.330 user=ucd_aut database=hgeraud host=10.146.17.67 port=51518"",,,,,,,,,""PostgreSQL JDBC Driver""
2023-07-01 10:51:04.697 -03,,,12394,,649c984c.306a,653,,2023-06-28 17:30:04 -03,,0,LOG,00000,""checkpoint starting: time"",,,,,,,,,""""
2023-07-01 10:51:05.013 -03,,,12394,,649c984c.306a,654,,2023-06-28 17:30:04 -03,,0,LOG,00000,""checkpoint complete: wrote 3 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.301 s, sync=0.000 s, total=0.315 s; sync files=3, longest=0.000 s, average=0.000 s; distance=4 kB, estimate=212 kB"",,,,,,,,,""""
{noformat}

","01/jul/23 3:24 PM;61e09d6968926d0068d83362;Prezado Elias Mussi, boa tarde.

Estive analisando as modificações que você esteve fazendo no pipeline. Contudo, observando os processors que são processados ao testar o ingest pipeline, sendo:

{noformat}""processor_type"": ""csv"",""processor_type"": ""remove"", ""processor_type"": ""grok"", ""processor_type"": ""convert"",""processor_type"": ""date"" {noformat}

Na verdade o pipeline que está sendo utilizado é o “filebeat-8.8.1-postgresql-log-pipeline-csv“:

{noformat}""filebeat-8.8.1-postgresql-log-pipeline-csv"" : {
    ""processors"" : [
      {
        ""csv"" : {
          ""trim"" : true,
          ""field"" : ""raw_message"",
          ""separator"" : "","",
          ""target_fields"" : [
            ""user.name"",
            ""postgresql.log.database"",
            ""process.pid"",
            ""tempcsv.connection_from"",
            ""postgresql.log.session_id"",
            ""tempcsv.session_line_num"",
            ""postgresql.log.command_tag"",
            ""tempcsv.session_start_time"",
            ""postgresql.log.virtual_transaction_id"",
            ""postgresql.log.transaction_id"",
            ""log.level"",
            ""postgresql.log.sql_state_code"",
            ""tempcsv.message"",
            ""postgresql.log.detail"",
            ""postgresql.log.hint"",
            ""postgresql.internal_query"",
            ""tempcsv.internal_query_pos"",
            ""postgresql.log.context"",
            ""postgresql.log.query"",
            ""tempcsv.query_pos"",
            ""postgresql.log.location"",
            ""postgresql.log.application_name"",
            ""postgresql.log.backend_type""
          ],
          ""ignore_missing"" : true
        }
      },
      {
        ""remove"" : {
          ""field"" : ""message"",
          ""ignore_missing"" : false
        }
      },
      {
        ""grok"" : {
          ""patterns"" : [
            ""^%{DATA:postgresql.log.client_addr}(:%{NUMBER:postgresql.log.client_port:int})?$""
          ],
          ""field"" : ""tempcsv.connection_from"",
          ""ignore_missing"" : true
        }
      },
      {
        ""convert"" : {
          ""field"" : ""postgresql.log.session_line_num"",
          ""type"" : ""long"",
          ""ignore_missing"" : true
        }
      },
      {
        ""date"" : {
          ""formats"" : [
            ""yyyy-MM-dd HH:mm:ss.SSS zz"",
            ""yyyy-MM-dd HH:mm:ss zz""
          ],
          ""field"" : ""tempcsv.session_start_time"",
          ""target_field"" : ""postgresql.log.session_start_time""
        }
      },
      {
        ""convert"" : {
          ""field"" : ""postgresql.log.transaction_id"",
          ""type"" : ""long"",
          ""ignore_missing"" : true
        }
      },
      {
        ""grok"" : {
          ""patterns"" : [
            ""^duration: %{NUMBER:temp.duration:float} ms$"",
            ""^duration: %{NUMBER:temp.duration:float} ms  %{POSTGRESQL_QUERY_STEP:postgresql.log.query_step} %{DATA:postgresql.log.query_name}: %{GREEDYDATA:message}$"",
            ""^duration: %{NUMBER:temp.duration:float} ms  %{POSTGRESQL_QUERY_STEP:postgresql.log.query_step}: %{GREEDYDATA:message}$"",
            ""^(%{POSTGRESQL_QUERY_STEP:postgresql.log.query_step}: )?%{GREEDYDATA:message}$""
          ],
          ""pattern_definitions"" : {
            ""GREEDYDATA"" : ""(.|\n|   )*"",
            ""POSTGRESQL_QUERY_STEP"" : ""(parse|bind|statement|fastpath function call|execute|execute fetch from)""
          },
          ""field"" : ""tempcsv.message"",
          ""ignore_missing"" : true
        }
      },
      {
        ""grok"" : {
          ""ignore_missing"" : true,
          ""patterns"" : [
            ""^%{DATA:postgresql.log.client_addr}(:%{NUMBER:postgresql.log.client_port:int})?$""
          ],
          ""field"" : ""tempcsv.connection_from""
        }
      },
      {
        ""remove"" : {
          ""field"" : ""tempcsv""
        }
      }
    ],
    ""description"" : ""Pipeline for parsing PostgreSQL CSV logs.""
  }{noformat}

E de acordo com o erro, o problema ocorre ao chegar no “processor: date”, onde:

{noformat}field [tempcsv] not present as part of path [tempcsv.session_start_time]{noformat}

Fiz aqui alguns testes e tentei desenvolver um outro pipeline para efeito de teste, gostaria que você utilizasse ele em seu ambiente e me reportasse o resultado. Segue abaixo:

{noformat}PUT _ingest/pipeline/postgresql-pipeline
{
  ""description"": ""Pipeline for parsing PostgreSQL logs in CSV format"",
  ""processors"": [
    {
      ""csv"": {
        ""field"": ""message"",
        ""target_fields"": [""timestamp"", ""user"", ""database"", ""process_id"", ""remote_host"", ""session_id"", ""sequence"", ""message_type"", ""log_time"", ""log_sequence"", ""log_type"", ""log_code"", ""log_message"", ""detail"", ""hint"", ""internal_query"", ""internal_query_pos"", ""context"", ""query"", ""query_pos"", ""location"", ""application_name""]
      }
    },
    {
      ""date"": {
        ""field"": ""timestamp"",
        ""target_field"": ""@timestamp"",
        ""formats"": [""yyyy-MM-dd HH:mm:ss.SSS Z""]
      }
    },
    {
      ""remove"": {
        ""field"": ""message""
      }
    }
  ]
}
{noformat}

Atenciosamente,

Paulo Henrique","01/jul/23 4:17 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Sim ... mas no fluxo entra no 1.o e desvia para o csv. O erro ocorre no 1.o é tem de mexer no date do 2.o tb   
  
 Mexendo nos dois parece ok. A meleca q n posso mexer no banco para gerar mais logs diferentes 
----
 
 {color:#172B4D}{color}{adf}{""type"":""expand"",""content"":[{""type"":""paragraph"",""content"":[{""type"":""text"",""text"":""Obter o ""},{""type"":""text"",""text"":""Outlook para Android"",""marks"":[{""type"":""link"",""attrs"":{""href"":""https://aka.ms/AAb9ysg""}}]}]}],""attrs"":{""title"":""Signature""}}{adf}","01/jul/23 5:03 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Entendi. Acredito que o melhor para darmos sequência neste chamado seria marcamos uma reunião na segunda-feira para visualizarmos o comportamento e testarmos os pipelines em conjunto para pensarmos nas possibilidades que podemos ter. Sendo assim, proponho que a gente tenha essa reunião segunda-feira às 16:00.

Atenciosamente,

Paulo Henrique","03/jul/23 3:39 PM;621538d1347c690072f6b56f;Prezado Elias Mussi, boa tarde!

Conforme conversado nos últimos dias, foi encontrada uma solução alterando o pipeline de ingestão utilizado para tratamento dos logs do PostgreSQL. A mudança se deu na adição de linhas no processador _grok_ e _date_. Essa alteração foi repassada para a Elastic com objetivo de validação da mudança, que foi avaliada e aprovada. Assim, a configuração poderá se manter dessa forma, sem problema algum para a ingestão dos dados.

Portanto, uma vez que foi solucionada a falha na ingestão e a mudança no pipeline foi validada pela fabricante, solicito o fechamento do chamado.

Atenciosamente,

Vinícius Prysthon",03/jul/23 4:02 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;qdo eles vao saltar a correção oficial para o ingest ?,"03/jul/23 4:56 PM;5ea9bfda0590bb0b7bedbaa1;Prezado Elias Mussi, boa tarde!

Grato pelo retorno.

Conforme alinhameto técnico realizdo com a equipe de suporte, a Elastic não categorizou essa alteração como um erro no pipeline presente na documentação oficial, e sim como uma não compatibilidade entre o pipeline que é sugerido lá e o formato que os logs banco PostgreSQL que foram enviados ao Elasticsearch. 

Desta forma, solicitamos autorização para fechamento do chamado, uma vez que o problema que era o objeto deste ticket foi resolvido.

_Iremos criar uma thread interna com a equipe de suporte da Elastic para verificar possíveis alterações na documentação deste pipeline, assim como a recomendação da utilização de UTC por padrão. A equipe da Dataprev será informada sobre qualquer resultado dessa thread._","03/jul/23 8:26 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;olha …. se nao suportar o TZ (timezone) do cliente nao é um bug SÉRIO, só podem estar de sacanagem. alem do mais q na doc nao fala NADA em só suportar UTC.

o problema desse ticket foi resolvido por MIM neh ! faltou falar isso.

por mim nao fecha ate ter uma referencia direta da elastic sobre isso.","03/jul/23 8:36 PM;5ea9bfda0590bb0b7bedbaa1;Boa noite Mussi,

Grato pelo retorno.

Compreendemos a necessidade de uma discussão sobre a resolução do chamado. 

Desta forma, solicitamos a alteração da severidade deste chamado para severidade 4, uma vez que a solução foi aplicada para sanar o problema reportado e iremos tratar possíveis encaminhamentos para esse ticket.","03/jul/23 8:42 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;blz, de acordo","04/jul/23 10:40 AM;621538d1347c690072f6b56f;Prezado Elias Mussi, bom dia!

Conforme conversado a Elastic, será necessária uma investigação por parte do time técnico deles para entender o motivo da ingestão não ter dado certo uma vez que foi seguida a documentação estritamente.

A investigação se dará no sentido de entender se é necessária uma mudança em algum ponto da documentação ou se o comportamento está mais relacionado a um bug.

Sendo assim, será aberto um chamado de solução definitiva que receberá atualizações à medida que recebermos qualquer posicionamento da Elastic sobre o caso.

Para abertura do chamado de solução definitiva, solicito autorização para fechamento deste presente chamado.



Atenciosamente,

Vinícius Prysthon",,,,,,,,,,,Em andamento
Solução Definitiva - Chamado 20 - kibana nao filtra os svslislbr firlds - (PRAZO 45 DIAS - 22/09/2023),DATAPREV-21,10930,Suporte técnico remoto: Incidente,Solução definitiva - Severidade 3,DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Baixa,,Vinícius Prysthon,621538d1347c690072f6b56f,Vinícius Prysthon,621538d1347c690072f6b56f,Vinícius Prysthon,621538d1347c690072f6b56f,24/jun/23 1:27 PM,24/ago/23 10:34 AM,19/set/23 4:16 PM,,,0,"Prezados, boa tarde!

Tendo em vista que o [https://bktech.atlassian.net/browse/DATAPREV-20|https://bktech.atlassian.net/browse/DATAPREV-20|smart-link] se trata de um bug que necessita correção e desenvolvimento de atualizações específicas do software, e em conformidade com o que versa no item 14.4 do TR, o chamado atual implica na ocorrência para provimento da solução definitiva, com o prazo de acordo com a severidade do chamado original, de acordo com o TR (a imagem contendo informações desses prazos para solução definitiva se encontra anexada neste chamado).

Assim, foi aberto um chamado com a fornecedora a fim de resolver o bug, e nós utilizaremos desse artifício para informar ao time da Dataprev qualquer atualização que a Elastic nos forneça a respeito do caso.

 

Atenciosamente,

Vinícius Prysthon",,Ana Karolina Moreira Viana Catta Preta,,,,,62a7957ba80881006f6130ed,,,,,,,,,,,,,24/ago/23 10:34 AM;ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb;CE em resposta ao Ofício 019_23.pdf;https://bktech.atlassian.net/rest/api/3/attachment/content/11048,24/ago/23 10:34 AM;ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb;OFICIO 019-2023 BKTECH-assinado.pdf;https://bktech.atlassian.net/rest/api/3/attachment/content/11049,24/jun/23 1:26 PM;ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77;prazos-solucao-definitiva.png;https://bktech.atlassian.net/rest/api/3/attachment/content/11022,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i005nv:,,português,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae(qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae),,,,,,,,,,,,,,-1010:49,,,,,,,,2023-08-10 13:46:01.033,,24/jun/23 1:27 PM;621538d1347c690072f6b56f;!prazos-solucao-definitiva.png|thumbnail!,"10/ago/23 10:46 AM;62a7957ba80881006f6130ed;Por meio do Oifício nº 019/2023, foi solicitada à DATAPREV dilação de prazo para apresentação da solução definitiva do presente chamado, por mais 45 dias corridos.
Desta forma, através de Correspondência Externa/DIGR/009/2023, foi concedida a dilação de prazo iniciando em 09/08/2023 e finalizando em 22/09/2023, in verbis:

{{”Em resposta ao seu pleito realizado no ofício Nº 019/2023, a Dataprev concorda com a dilação do}}
{{prazo para apresentação da solução definitiva do chamado 21, por mais 45 dias corridos.}}
{{Sendo assim, o novo prazo inicia em 09/08/2023 e finda em 22/09/2023. Solicito que seja registrado}}
{{no chamado o pleito de dilação efetuado, bem como assim que o patch de correção for disponibilizado.""}}


[^OFICIO 019-2023 BKTECH-assinado.pdf]
[^CE em resposta ao Ofício 019_23.pdf]

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Em andamento
kibana nao filtra os svslisblr firlds ,DATAPREV-20,10929,Suporte técnico remoto: Incidente,Concluído(a),DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Baixa,,Vinícius Prysthon,621538d1347c690072f6b56f,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,23/jun/23 5:42 PM,24/jun/23 1:26 PM,19/set/23 4:16 PM,,,0,"Grato pelo envio das informações.

Gostaríamos de informar que foi confirmado pela Elastic que se trata de um bug, que já se encontra em correção.

Uma vez que esse ticket foi aberto com SEVERIDADE 4, para que possa ser registrado um ticket de SOLUÇÃO DEFINITIVA para correção do software, solicitamos que seja criado um novo ticket com SEVERIDADE 3, que contempla a correção de bugs. Assim que o novo chamado for aberto, o mesmo será tratado imediatamente com o encaminhamento para a solução definitiva.

Portanto, solicitamos a autorização para encerramento deste chamado após abertura do novo chamado, para início das tratativas informadas acima.",,Vinícius Prysthon,,,,,621538d1347c690072f6b56f,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i005nn:,,português,,,,,,,,,,,,,,,44:15,,,,,,,,2023-06-23 21:56:49.685,,"23/jun/23 6:56 PM;621538d1347c690072f6b56f;Prezado Elias Mussi, boa noite!

Conforme conversado no chamado [https://bktech.atlassian.net/browse/DATAPREV-18|https://bktech.atlassian.net/browse/DATAPREV-18|smart-link], o comportamento observado na aba “Discover” do Kibana onde campos não preenchidos aparecem em “Available Fields” se trata de um bug, que já está sendo discutido pela Elastic sobre sua correção.

Sendo assim, solicitamos o fechamento do presente chamado, mas informamos que manteremos o time da Dataprev atualizado a qualquer comunicado da Elastic a respeito desse bug em um novo chamado que será aberto a fim de buscarmos a resolução do problema de forma definitiva.

Aproveito para complementar que o aceite de fechamento no chamado anterior também se aplica pra que esse seja fechado e criado um chamado de solução definitiva.

 

Atenciosamente,

Vinícius Prysthon",24/jun/23 10:57 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;bora fechar so qdo da solucao definitiva.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Itens concluídos
dash de consumo de disco por camada (hot/w/c),DATAPREV-19,10926,Suporte técnico remoto: esclarecimento de dúvidas,Aguardando cliente,DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Muito Baixa,,Paulo Henrique Morato Góes,61e09d6968926d0068d83362,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,20/jun/23 10:19 AM,26/jun/23 11:41 AM,22/jun/23 10:46 AM,,,0,dash de consumo de disco por camada (hot/w/c),,Paulo Henrique Morato Góes,,,,,61e09d6968926d0068d83362,,,,,,,,,,,,,21/jun/23 3:41 PM;ug:0e9c7acd-4c78-4ef6-beb2-b00f34bddd59;Screenshot2023-06-20at11.47.37AM.png;https://bktech.atlassian.net/rest/api/3/attachment/content/11011,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i005mz:,,português,,,,,,,,,,,,,,,10:11,,,,,,,,2023-06-20 13:54:26.285,,"20/jun/23 10:54 AM;61e09d6968926d0068d83362;Prezado Elias Mussi, bom dia!

Solicito permissão para *alteração da severidade do chamado*, uma vez que trata-se de esclarecimento de dúvidas, sendo, portanto, um chamado de severidade 4, conforme consta no item 14.1 do TR:

_d) Severidade 4 – quando se verifica como necessária a prestação de informações, aperfeiçoamentos ou_ *_esclarecimentos_* _sobre documentação ou_ *_funcionalidades, porém sem prejudicar diretamente a operação dos programas ou sistemas da DATAPREV._*

 

Desde já, agradecemos a cooperação e informamos que estamos trabalhando desde já para atende-lo!",20/jun/23 11:43 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;ok para a mudança de severidade,"21/jun/23 3:41 PM;61e09d6968926d0068d83362;Prezado Elias Mussi, boa tarde.

Entramos em contato com a Elastic e nos foi informado que não existe um dashboard já built-in para a observação de consumo de disco por camada. Além das APIs que trazem essas informações, outro local que apresenta também é na aba “Nodes” no Stack Monitoring como apresenta a imagem anexada.

Dito isso, caso as informações apenas nessa aba não sejam suficientes para o que deseja, será necessária a construção manual de um Dashboard contendo essas métricas, utilizando o campo elasticsearch.node.roles, e assim, passa pela definição da Dataprev das informações que seriam relevantes a ter nesse dash.

Atenciosamente,

Paulo Henrique

!Screenshot2023-06-20at11.47.37AM.png|thumbnail!

","21/jun/23 4:15 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;pois é voltamos a estaca ZERO.

elasticsearch.node.roles em eventos com esse field nao traz nada de dados de uso de disco … só achei eles para ingest …

sera q falta alguma config para trazer ?

se os dados da aba me ajudasse eu nao teria pedido 😉

Abraços","21/jun/23 11:28 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Apenas informei o que foi repassado pela Elastic. Esse comportamento do campo “elasticsearch.node.roles“ devemos investigar o motivo pelo qual não esta puxando os dados dos nodes das outras roles. Primeiramente podemos observar se no arquivo metricbeat.yml possui alguma configuração que possa alterar esse comportamento.

Atenciosamente,

Paulo Henrique",22/jun/23 8:21 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;????  e ?,"22/jun/23 10:24 AM;61e09d6968926d0068d83362;Peço que me envie o metricbeat.yml, elasticsearch.yml e também uma evidência do Kibana que apresente o que informou anteriormente sobre só estar aparecendo eventos de ingest.

Atenciosamente,

Paulo Henrique","26/jun/23 11:41 AM;61e09d6968926d0068d83362;Prezado Elias Mussi, bom dia.

Conforme conversado em reunião no dia 26/06/2023 às 10:30, a criação desse dashboard se trata de uma demanda de consultoria técnica, e não mais um chamado de esclarecimento de dúvidas. Sendo assim, solicito permissão para fechamento do chamado.

Atenciosamente,

Paulo Henrique",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Em andamento
kibana : so aparecer no popular fields campos que tenham conteudos e nao todos que estao no mappins,DATAPREV-18,10925,Suporte técnico remoto: esclarecimento de dúvidas,Concluído(a),DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Baixa,,Vinícius Prysthon,621538d1347c690072f6b56f,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,20/jun/23 10:18 AM,09/ago/23 12:27 PM,19/set/23 4:16 PM,,,0,kibana : so aparecer no popular fields campos que tenham conteudos e nao todos que estao no mappins,,Fabio Schmidt,Paulo Henrique Morato Góes,Vinícius Prysthon,,,5ea9bfda0590bb0b7bedbaa1,61e09d6968926d0068d83362,621538d1347c690072f6b56f,,,,,,,,,,,21/jun/23 11:04 PM;ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77;3e008a2a-3fe1-4540-b194-4b94efb33096.jpeg;https://bktech.atlassian.net/rest/api/3/attachment/content/11012,20/jun/23 3:17 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Screenshot from 2023-06-20 15-16-08.png;https://bktech.atlassian.net/rest/api/3/attachment/content/11008,20/jun/23 3:17 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Screenshot from 2023-06-20 15-16-24.png;https://bktech.atlassian.net/rest/api/3/attachment/content/11009,20/jun/23 3:17 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Screenshot from 2023-06-20 15-17-23.png;https://bktech.atlassian.net/rest/api/3/attachment/content/11010,22/jun/23 8:20 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Screenshot from 2023-06-22 08-18-18.png;https://bktech.atlassian.net/rest/api/3/attachment/content/11015,22/jun/23 8:20 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Screenshot from 2023-06-22 08-18-30.png;https://bktech.atlassian.net/rest/api/3/attachment/content/11014,22/jun/23 8:20 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Screenshot from 2023-06-22 08-18-39.png;https://bktech.atlassian.net/rest/api/3/attachment/content/11013,22/jun/23 8:20 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Screenshot from 2023-06-22 08-19-00.png;https://bktech.atlassian.net/rest/api/3/attachment/content/11016,22/jun/23 8:20 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Screenshot from 2023-06-22 08-19-20.png;https://bktech.atlassian.net/rest/api/3/attachment/content/11017,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i005mr:,,português,,,,,,,,,,,,,,,47:57,,,,,,,,2023-06-20 13:54:12.105,,"20/jun/23 10:54 AM;61e09d6968926d0068d83362;Prezado Elias Mussi, bom dia!

Solicito permissão para *alteração da severidade do chamado*, uma vez que trata-se de esclarecimento de dúvidas, sendo, portanto, um chamado de severidade 4, conforme consta no item 14.1 do TR:

_d) Severidade 4 – quando se verifica como necessária a prestação de informações, aperfeiçoamentos ou_ *_esclarecimentos_* _sobre documentação ou_ *_funcionalidades, porém sem prejudicar diretamente a operação dos programas ou sistemas da DATAPREV._*

 

Desde já, agradecemos a cooperação e informamos que estamos trabalhando desde já para atende-lo!",20/jun/23 11:43 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;ok para a mudança de severidade,"20/jun/23 3:17 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;!Screenshot from 2023-06-20 15-16-24.png|width=1685,height=915!

!Screenshot from 2023-06-20 15-16-08.png|width=1685,height=915!

","20/jun/23 3:17 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;!Screenshot from 2023-06-20 15-17-23.png|width=1685,height=915!

","20/jun/23 4:32 PM;621538d1347c690072f6b56f;Prezados,

a fim de registro, coletamos a evidência necessária e reportamos à Elastic o problema. O ponto central é entender o porquê de campos que não estão preenchidos (pelo menos no timerange setado) estão aparecendo em “available fields” ao invés de estarem em “empty fields”, tornando confusa a exploração no Discover. Assim que a Elastic nos retornar, atualizo esse chamado.

Atenciosamente,

Vinícius Prysthon",21/jun/23 3:30 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;status deveria ser aguardando fornecedor 😉,"21/jun/23 11:04 PM;621538d1347c690072f6b56f;Prezado Elias Mussi, boa noite!

Já iniciamos a conversa com a Elastic pra verificar esse comportamento entre o “available fields” e o “empty fields”. Fui solicitado pela Elastic que enviasse duas capturas de tela, sendo:

# a primeira contendo um campo não preenchido estando nos available fields (por exemplo, algum field de métrica AWS Cloud); ou seja, tirar um print da tela mostrando que esse campo, apesar de não preenchido, estar em available fields
# a segunda captura seria um filtro utilizando esse mesmo field e puxando eventos em que ele existe, algo parecido com o que está na imagem anexada.

​Tendo essas duas telas capturadas, é possível enviar pra Elastic a evidência de que campos não preenchidos nem uma única vez no time range setado estão presentes em “available fields” ao invés de estarem em “empty fields”.

Atenciosamente,

Vinícius Prysthon 



!3e008a2a-3fe1-4540-b194-4b94efb33096.jpeg|thumbnail!

","22/jun/23 8:20 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;!Screenshot from 2023-06-22 08-19-20.png|width=1685,height=915!

!Screenshot from 2023-06-22 08-19-00.png|width=1685,height=915!

!Screenshot from 2023-06-22 08-18-39.png|width=1685,height=915!

!Screenshot from 2023-06-22 08-18-30.png|width=1685,height=915!

!Screenshot from 2023-06-22 08-18-18.png|width=1685,height=915!

","23/jun/23 5:02 PM;5ea9bfda0590bb0b7bedbaa1;Prezado Mussi, boa tarde!

Grato pelo envio das informações.

Gostaríamos de informar que foi confirmado pela Elastic que se trata de um bug, que já se encontra em correção.

Uma vez que esse ticket foi aberto com SEVERIDADE 4, para que possa ser registrado um ticket de SOLUÇÃO DEFINITIVA para correção do software, solicitamos que seja criado um novo ticket com SEVERIDADE 3, que contempla a correção de bugs. Assim que o novo chamado for aberto, o mesmo será tratado imediatamente com o encaminhamento para a solução definitiva. 

Portanto, solicitamos a autorização para encerramento deste chamado após abertura do novo chamado, para início das tratativas informadas acima.

Grato e à disposição!","24/jun/23 10:54 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;bora combinar de fechar esse apenas qdo a solução final chegar e adotar isso para os outros tb. e se deixa a referencia ao chamado externo, 20 se nao me engano.","08/ago/23 2:29 PM;5ea9bfda0590bb0b7bedbaa1;Prezado Mussi,
Solicito por gentileza a alteração da severidade do presente chamado para severidade 3, uma vez que se trata de um bug e conforme entendimentos firmados junto ao T.R, devemos realizar a presente tratativa.
Desde já, agradeço o pronto atendimento.",09/ago/23 12:25 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;de acordo,,,,,,,,,,,,,,,,,,,,,,,,,,,Itens concluídos
"busca em dois galhos do ldap : user_search.base_dn: ""ou=DATAPREV,dc=gov,dc=br"", ""ou=SISTEMA,dc=gov,dc=br""",DATAPREV-17,10924,Suporte técnico remoto: esclarecimento de dúvidas,Concluído(a),DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Muito Baixa,,Vinícius Prysthon,621538d1347c690072f6b56f,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,20/jun/23 10:18 AM,24/jun/23 11:03 AM,19/set/23 4:16 PM,,,0,"busca em dois galhos do ldap : user_search.base_dn: ""ou=DATAPREV,dc=gov,dc=br"", ""ou=SISTEMA,dc=gov,dc=br""",,Paulo Henrique Morato Góes,Vinícius Prysthon,,,,61e09d6968926d0068d83362,621538d1347c690072f6b56f,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i005mj:,,português,,,,,,,,,,,,,,,47:48,,,,,,,,2023-06-20 13:26:10.061,,"20/jun/23 10:26 AM;621538d1347c690072f6b56f;Prezado Elias Mussi, bom dia!

Em conversa com a Elastic, obtive a seguinte resposta:



We have had this question several times and there are some ideas as to how you could do this, but it really depends on the type of LDAP server and structure of your organization. The good news is that it’s pretty easy to test this. You can pick out a single node that doesn’t have any data, such as an un-elected master node. Then update the {{elasticsearch.yml}} for that one node, restart it to push the changes, then run a {{localhost}} curl against that host to see if it can authenticate with a user in both the DATAPREV and SISTEMA organization units.

As for how to configure this, you can either try simplifying your user search by removing the organizational units.

{noformat}user_search.base_dn: ""DC=gov,DC=br""
{noformat}

Or you can ditch {{user_search.base_dn}} in favor of {{user_dn_templates}}.

{noformat}user_dn_templates:
  - ""CN={0}, OU=DATAPREV, DC=gov, DC=br""
  - ""CN={0}, OU=SISTEMA, DC=gov, DC=br""{noformat}



For more information on this topic and those settings, see [LDAP user authentication|https://www.elastic.co/guide/en/elasticsearch/reference/7.17/ldap-realm.html] and [Security settings in Elasticsearch: LDAP realm settings|https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-settings.html#ref-ldap-settings].","20/jun/23 10:53 AM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Solicito permissão para *alteração da severidade do chamado*, uma vez que trata-se de esclarecimento de dúvidas, sendo, portanto, um chamado de severidade 4, conforme consta no item 14.1 do TR:

_d) Severidade 4 – quando se verifica como necessária a prestação de informações, aperfeiçoamentos ou_ *_esclarecimentos_* _sobre documentação ou_ *_funcionalidades, porém sem prejudicar diretamente a operação dos programas ou sistemas da DATAPREV._*

 

Desde já, agradecemos a cooperação e informamos que estamos trabalhando desde já para atende-lo!","20/jun/23 11:42 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;conf : 

{noformat}        #user_search.base_dn: ""ou=DATAPREV,dc=gov,dc=br""
        user_dn_templates:
          - ""uid={0}, ou=DATAPREV, dc=gov, dc=br""
          - ""uid={0}, ou=SISTEM, dc=gov, dc=br""
        #user_search.filter: ""(uid={0})""{noformat}



log de error : 



[2023-06-20T11:11:22,094][WARN ][o.e.x.s.a.RealmsAuthenticator] [n321p005265] Authentication to realm dataprev_ldap failed - authenticate failed (Caused by LDAPException(resultCode=49 (invalid credentials), errorMessage='invalid credentials', ldapSDKVersion=6.0.3, revision=405ee52a554f9867e81d4598a5b2f97beabeb29a))



log do AD q se repete (realm ad)

[2023-06-20T05:16:04,459][WARN ][o.e.x.s.a.RealmsAuthenticator] [n321p005265] Authentication to realm dataprev_ad failed - authenticate failed (Caused by LDAPException(resultCode=49 (invalid credentials), diagnosticMessage='80090308: LdapErr: DSID-0C090439, comment: AcceptSecurityContext error, data 532, v4563^@', ldapSDKVersion=6.0.3, revision=405ee52a554f9867e81d4598a5b2f97beabeb29a))



{noformat}xpack:
  security:
    authc:
      realms:
        active_directory:
          dataprev_ad:
            order: ""10""
            domain_name: ""digital.corp""
            url: ""ldaps://digital.corp:636""
            bind_dn: ""DTPELK""
            ssl.certificate_authorities: ""/etc/elasticsearch/certs/ca.crt""
            ssl.verification_mode: ""none""
            metadata: ""department""
        ldap:
          dataprev_ldap:
            order: ""2""
            url: ""ldaps://ldap.prevnet:636""
            bind_dn: ""uid=user,ou=Users,ou=Global,dc=gov,dc=br""
            user_search.base_dn: ""ou=DATAPREV,dc=gov,dc=br""
            user_search.filter: ""(uid={0})""
            group_search.base_dn: ""ou=Grupos,ou=Global,dc=gov,dc=br""
            ssl.certificate_authorities: ""/etc/elasticsearch/certs/ca.crt""
            ssl.verification_mode: ""none""
            metadata: ""businessCategory""
          dataprev_ldap_sistemas:
            order: ""3""
            url: ""ldaps://ldap.prevnet:636""
            bind_dn: ""uid=user,ou=Users,ou=Global,dc=gov,dc=br""
            user_search.base_dn: ""ou=sistemas,dc=gov,dc=br""
            user_search.filter: ""(uid={0})""
            group_search.base_dn: ""ou=Grupos,ou=Global,dc=gov,dc=br""
            ssl.certificate_authorities: ""/etc/elasticsearch/certs/ca.crt""
            ssl.verification_mode: ""none""
            metadata: ""businessCategory""
{noformat}

",20/jun/23 11:43 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;ok para a mudança de severidade,"20/jun/23 2:45 PM;621538d1347c690072f6b56f;Para fins de registro, a Elastic sugeriu dois caminhos a serem seguidos, sendo o segundo mais relevante à realidade da Dataprev. Foi feita a configuração, que retornou erro de “invalid credentials”, que será retornado para a Elastic. Por ora, foi dividida a configuração em dois realms, que corrigiu o problema. Farei atualizações aqui à medida que recebermos uma resposta da Elastic.

Atenciosamente,

Vinícius Prysthon",21/jun/23 3:29 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;status deveria ser aguardando fornecedor 😉,"21/jun/23 10:40 PM;621538d1347c690072f6b56f;Prezado Elias Mussi, boa noite!

Peço desculpas, o status não deveria ter alterado quando respondi ao chamado, já fizemos essa correção. 

Aproveito essa mensagem para solicitar, por gentileza, toda a configuração do AD/LDAP quando o ‘user_dn_templates’ foi setado em:

{noformat}        #user_search.base_dn: ""ou=DATAPREV,dc=gov,dc=br""
        user_dn_templates:
          - ""uid={0}, ou=DATAPREV, dc=gov, dc=br""
          - ""uid={0}, ou=SISTEM, dc=gov, dc=br""
        #user_search.filter: ""(uid={0})""{noformat}

Ou seja, preciso da configuração desde o “xpack”. Estamos investigando junto com a Elastic esse erro de autenticação quando o parâmetro ‘user_dn_templates’ foi utilizado.

Atenciosamente,

Vinícius Prysthon 

","22/jun/23 8:14 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;h1. Configure AD e ldap

xpack:
  security:
    authc:
      realms:
        active_directory:
          dataprev_ad:
            order: ""10""
            domain_name: ""digital.corp""
            url: ""ldaps://digital.corp:636""
            bind_dn: ""DTPELK""
            ssl.certificate_authorities: ""/etc/elasticsearch/certs/ca.crt""
            ssl.verification_mode: ""none""
            metadata: ""department""
        ldap:
          dataprev_ldap:
            order: ""2""
            url: ""ldaps://ldap.prevnet:636""
            bind_dn: ""uid=user,ou=Users,ou=Global,dc=gov,dc=br""
            user_search.base_dn: ""ou=DATAPREV,dc=gov,dc=br""
            user_search.filter: ""(uid={0})""
            group_search.base_dn: ""ou=Grupos,ou=Global,dc=gov,dc=br""
            ssl.certificate_authorities: ""/etc/elasticsearch/certs/ca.crt""
            ssl.verification_mode: ""none""
            metadata: ""businessCategory""","22/jun/23 6:08 PM;621538d1347c690072f6b56f;Prezado Elias Mussi, boa tarde!

Em conversa com a Elastic, nos solicitaram uma call pra entender melhor a demanda e poder ajudar com mais rapidez. Portanto, gostaria de saber da sua disponibilidade para uma call amanhã às 10h.

Fomos informados também que, nessa call, é importante estar presente alguém com permissões de administrador do AD/LDAP, caso seja necessário acessar alguma informação.

Fico no aguardo da sua resposta.

Atenciosamente,

Vinícius Prysthon ",23/jun/23 8:33 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;blz  bora fazer na sala q ja temos,"23/jun/23 9:45 AM;621538d1347c690072f6b56f;Prezado Elias Mussi, bom dia!

Certo, a Elastic nos enviou o link de uma sala para reunirmos, podemos fazer por lá? Segue link:

[https://elastic.zoom.us/j/96343391819?pwd=TkFxS0RqUS9vT1F1SGNVR2I1SmpDdz09|https://elastic.zoom.us/j/96343391819?pwd=TkFxS0RqUS9vT1F1SGNVR2I1SmpDdz09|smart-link] 

Password: 356535

Nos vemos às 10h.

Atenciosamente,

Vinícius Prysthon","23/jun/23 9:48 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;nao consegue trazer eles para a sala q ja temos marcada nao ?  ae ja imendamos na nossa q ja esta marcada

","23/jun/23 12:22 PM;621538d1347c690072f6b56f;Prezado Elias Mussi, boa tarde!

Conforme conversado em reunião onde estiveram presentes Dataprev, Elastic e BKTech, fomos orientados pelo fabricante a adicionar um _realm_ à configuração do LDAP no _elasticsearch.yml_ apontando para o mesmo servidor do LDAP, mas utilizando a OU=sistemas como _base_dn_, tendo assim, dois realms configurados.

Portanto, gostaria de solicitar o fechamento desse chamado uma vez que foi acordado que esse caminho funcionaria e a solução foi encontrada.

Aguardo sua autorização.

Atenciosamente,

Vinícius Prysthon",24/jun/23 10:52 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;fecha ae,,,,,,,,,,,,,,,,,,,,,,,,,Itens concluídos
duvida sobre mapeamento key  value encadeato,DATAPREV-16,10922,Suporte técnico remoto: esclarecimento de dúvidas,Concluído(a),DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Muito Baixa,,Vinícius Prysthon,621538d1347c690072f6b56f,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,02/jun/23 9:17 AM,12/jun/23 12:53 PM,04/ago/23 2:37 PM,,,0,"Boas,

turma …  repara esse caso aqui : 

{code:json} ""dtp_app""=>{
      ""gitaly_calls""=>1,
      ""params""=>[
        {
          ""key""=>""utf8"",
          ""value""=>""âœ“""
        },
        {
          ""key""=>""merge_request"",
          ""value""=>{
            ""assignee_id""=>""478"",
            ""description""=>"""",
            ""label_ids""=>[
              """"
            ],
            ""target_branch""=>""desenvolvimento"",
            ""lock_version""=>""1"",
            ""force_remove_source_branch""=>""1""
          }
        },{code}



dtp_app.params esta com o formato nested … ate ae tudo bem … o problema é qdo um “value”  recebe mais algo dentro dele (linha 10) … ae tomo esse erro aqui : 

 ""reason""=>""[1:1073] failed to parse field [dtp_app.params.value] of type [keyword] in document with id



qual a melhor forma de mapear esse caso ( ate mesmo .. tem como “ajustar” esse key / value para um outra forma ?

tipo  o log me envia como assima … e o mapping transformar no par correspondente ?

de params.key, params.value  para   params.VALOR_KEY => VALOR_value ?



Abraços",,josue,Paulo Henrique Morato Góes,Rodrigo Tornis,Vinícius Prysthon,,62a79211d0132d00687f77aa,61e09d6968926d0068d83362,5f9829d0dbf337006c7dc32d,621538d1347c690072f6b56f,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i005m3:,,português,fabio(fabio),ug:0e9c7acd-4c78-4ef6-beb2-b00f34bddd59(ug:0e9c7acd-4c78-4ef6-beb2-b00f34bddd59),ug:0e2daf0a-9fff-4018-89ac-200a262b74f9(ug:0e2daf0a-9fff-4018-89ac-200a262b74f9),,,,,,,,,,,,20:16,,,,,,,,2023-06-03 18:33:53.334,,"03/jun/23 3:33 PM;5f9829d0dbf337006c7dc32d;[~accountid:qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae] , 

Sugerimos a seguinte estratégia: 

1 - Usar dynamic templates para endereçar os campos e seus tipos 

2 - Desabilitar a indexação desses campos configurando o parâmetro enabled=false, isso faz com que o Elastic ingnore a validação do dado e o mesmo é armazenado mas sem condição de busca por seus valores

Segue abaixo um teste para a sua validação e ver se essa estratégia resolveria: 

{noformat}PUT teste
{
  ""mappings"": {
    ""dynamic_templates"": [{
        ""campos_nested"": {
          ""path_match"": ""dtp_app.params.*"",
          ""mapping"": {
            ""type"": ""object"",
            ""enabled"": false
          }
        }
      }
    ],
    ""properties"": {
      ""dtp_app"": {
        ""type"": ""nested""
      }
    }
  }
}

POST teste/_doc
{
  ""dtp_app"":{
      ""gitaly_calls"":1,
      ""params"":[
        {
          ""key"":""utf8"",
          ""value"":""âœ“""
        }
      ]
  }
}

POST teste/_doc
{
  ""dtp_app"":{
      ""gitaly_calls"":1,
      ""params"":[
        {
          ""key"":""merge_request"",
          ""value"":{
            ""assignee_id"":""478"",
            ""description"":"""",
            ""label_ids"":[
              """"
            ],
            ""target_branch"":""desenvolvimento"",
            ""lock_version"":""1"",
            ""force_remove_source_branch"":""1""
          }
        }
      ]
  }
}{noformat}

 

Observe que desta forma qualquer informação que vier em key ou value será armazenado, porém, se a capacidade de busca. 

Caso essa estratégia não atenda, ai sugerimos o tratamento prévio usando o ingest pipeline ou pelo Logstash para que normalize a informação antes de sua indexação! 



Att, 

Rodrigo Tornis","05/jun/23 8:50 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Boas,



dtp_app e dp tipo object 😉

sabe se existe um ingest para o gitlab ?  procurei um tempo atras e nao achei nada.

ou mesmo algum parser do logstash ja pronto … to achando q é melhor tratar nesse nivel, ate mesmo para as informações ficarem “ identificaveis”



Abraços","05/jun/23 9:36 AM;5f9829d0dbf337006c7dc32d;Sim, o dtp_app é object mas abaixo dele não necessariamente 😉! 

Nesse caso o que sugiro é vc usar um parcer que coloca todos os campos abaixo de dtp_app como string e não como objecto para evitar o problema que vc esta tendo. Sugiro vc usar o codec plain ou line para os campos abaixo de dtp_app. 

Exemplo simples:  

filter {

      {{codec => line { format => ""%{message}"" }}}     

}



Outra forma seria ler os eventos em formato plain, copiar o campo dtp_app em um campo único com o codec line e para os demais usar o json format para colocar os objetos conforme a especificação da dataprev 

Se precisar podemos fazer uma call para explicar melhor! ","05/jun/23 9:37 AM;5f9829d0dbf337006c7dc32d;Sim, o dtp_app é object mas abaixo dele não necessariamente 😉! 

Nesse caso o que sugiro é vc usar um parcer que coloca todos os campos abaixo de dtp_app como string e não como objecto para evitar o problema que vc esta tendo. Sugiro vc usar o codec plain ou line para os campos abaixo de dtp_app.

Exemplo simples: 

filter {

{{codec => line { format => ""%{message}"" }}}

}

 

Outra forma seria ler os eventos em formato plain, copiar o campo dtp_app em um campo único com o codec line e para os demais usar o json format para colocar os objetos conforme a especificação da dataprev

Se precisar podemos fazer uma call para explicar melhor!","05/jun/23 10:50 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;rapa …  

arruma uma hora ae para me ligar q agora nao entendi 🙂 

[]ca0","05/jun/23 7:11 PM;621538d1347c690072f6b56f;Prezado Elias Mussi,

Podemos falar amanhã às 11h30 sobre isso. Estará disponível nesse horário?",06/jun/23 8:41 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;blz  combinado,06/jun/23 11:34 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;cade o link 🙂 ,"06/jun/23 11:38 AM;621538d1347c690072f6b56f;Prezado Elias Mussi,

Segue link para a reunião: [https://meet.google.com/cfi-scjf-gou|https://meet.google.com/cfi-scjf-gou|smart-link]

Atenciosamente,

Vinícius Prysthon ","06/jun/23 12:36 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Conforme conversado em reunião realizada às 11h30 do dia 06/06/2023, a solução para o momento foi desabilitar o dynamic mapping, ainda que os campos não possam ser buscáveis. 

Outra possível solução para que seja possível o mecanismo de busca é algo parecido com:

{noformat}filter {

  if [dtp_app][params][key] {
    mutate => {
      add_field => { [dtp_app][params].[%{[dtp_app][params][key]}] => [dtp_app][params][key]
  }
  # dtp_app.params.merge_request
  mutate => {
    add_field => { [dtp_app][params].[%{[dtp_app][params][key]}][value] => [dtp_app][params][value]
  }
  # dtp_app.params.merge_request.value = keyword ou object

  remove_field => [dtp_app][params][value]  

}{noformat}

Com o esclarecimento da dúvida feito, solicitamos autorização para fechamento do chamado.



Atenciosamente,

Paulo Henrique","09/jun/23 12:46 PM;62a79211d0132d00687f77aa;Prezado Elias Mussi,

Gostaria de solicitar, com base nos esclarecimentos fornecidos durante a última interação, a autorização para o fechamento do chamado em questão.","12/jun/23 12:51 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;fecha ae … 

mas só funcionou legal devido ao encadeamento o ruby code",,,,,,,,,,,,,,,,,,,,,,,,,,,Itens concluídos
filebeat versao 8.8 nao compativel com RHEL das series 6.x,DATAPREV-15,10920,Suporte técnico remoto: Incidente,Concluído(a),DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Normal,,Paulo Henrique Morato Góes,61e09d6968926d0068d83362,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,27/mai/23 5:10 PM,26/jul/23 1:11 PM,19/set/23 4:16 PM,,,0,"Boas,

Um grade revés as na atualização do filebeat de 8.7 para 8.8.

ele parou de funcionar nos RHel da serie 6.X tenho mais de 1300 maquinas q ainda rodam com essa versão de SO. ( até a 8.7 rodava)

to vendo muito desse erro aqui :

{quote}Config OK

FATAL: kernel too old

/bin/bash: line 1: 7348 Aborted (core dumped) /usr/share/filebeat/bin/filebeat-god -r / -n -p /var/run/filebeat.pid -- /usr/share/filebeat/bin/filebeat -c /etc/filebeat/filebeat.yml --path.home /usr/share/filebeat --path.config /etc/filebeat --path.data /var/lib/filebeat --path.logs /var/log/filebeat [FAILED]{quote}

agora temos 2 problemas com o filebeat 😞 

e não falaram nada nos release e breaking 😞 e olha q li antes de fazer o upgrade.",,Fabio Schmidt,Paulo Henrique Morato Góes,Vinícius Prysthon,,,5ea9bfda0590bb0b7bedbaa1,61e09d6968926d0068d83362,621538d1347c690072f6b56f,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i005ln:,,português,ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb(ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb),ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77(ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77),fabio(fabio),ug:0e2daf0a-9fff-4018-89ac-200a262b74f9(ug:0e2daf0a-9fff-4018-89ac-200a262b74f9),,,,,,,,,,,5:03,,,,,,,,2023-05-27 22:02:01.162,,"27/mai/23 7:02 PM;61e09d6968926d0068d83362;Prezado Elias Mussi, boa noite.

Primeiramente solicitamos a alteração da severidade para que esteja adequado ao que consta no TR:

*_Os níveis de severidade são descritos abaixo:_*

*_a) Severidade 1 – quando ocorre a perda ou paralisação de atividades exercidas ou de_*

*_serviços relevantes prestados pela DATAPREV, configurando-se como emergência._*

*_Uma solicitação de serviço de Severidade 1 pode possuir uma ou mais das seguintes_*

*_características_*

▪ *_Dados corrompidos;_*

▪ *_Uma função crítica não está disponível;_*

▪ *_O sistema se desliga repentinamente causando demoras excessivas e_*

*_intermitências para utilização de recursos;_*

▪ *_O sistema falha repetidamente após tentativas de reinicialização;_*

▪ *_O sistema continua em execução permanente (congelado) necessitando ser_*

*_reiniciado._*

*_b) Severidade 2 – quando se verifica uma grave perda de funcionalidade em_*

*_programas ou sistemas da DATAPREV, sem, no entanto, interromper em sua_*

*_totalidade a prestação do serviço._*

*_c) Severidade 3 – quando se verifica uma perda de menor relevância de_*

*_funcionalidades em programas ou sistemas, causando apenas inconveniências para_*

*_a realização de atividades exercidas ou pela devida prestação dos serviços pela_*

*_DATAPREV._*

*_d) Severidade 4 – quando se verifica como necessária a prestação de informações,_*

*_aperfeiçoamentos ou esclarecimentos sobre documentação ou funcionalidades,_*

*_porém sem prejudicar diretamente a operação dos programas ou sistemas da_*

*_DATAPREV._*



De acordo com o descrito no TR, entendemos a situação como um chamado de severidade 2, não 1. Sendo assim, solicitamos a permissão para que seja alterada a severidade para a correta.

Atenciosamente,

Paulo Henrique","27/mai/23 7:36 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;precisamos fazer um de / para das opções la 🙂  eu abri na alta pensando q seria severidade 2 mesmo.

ela apareceu como 1 ae ?

eu imaginei critico como 1, alta como 2.

fora q uma vez eu cismei q tinha aberto como baixa q seria na minha vista um 4 … e apareceu ae como 1, mas foi a 1.a vez de posso ter comido mosca 🙂  .

mas é 2 mesmo … manda bala

mas to preocupado pq tenho 1,3k maquinas com rhel 6 ainda no parque q nao tem mais o filebeat rodando … ate o 8.7 era de boa.

Abraços","27/mai/23 7:44 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Vamos buscar melhorar as opções para que fique mais claro para a abertura. 

A respeito do chamado, realmente nas releases notes da versão 8.8 não apresenta nenhuma situação de modificação de compatibilidade do Filebeat com o RHEL 6.x, entretanto observei aqui e de acordo com a matriz de compatibilidade da Elastic, teoricamente desde a versão 8.x já não existiria mais a compatibilidade do Filebeat em máquinas com RHEL 6.x.

Segue abaixo o link em que é apresentada as compatiblidades das soluções da ferramenta:

[https://www.elastic.co/pt/support/matrix|https://www.elastic.co/pt/support/matrix|smart-link]



Atenciosamente,

Paulo Henrique","27/mai/23 8:01 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;isso eu sei … mas tava rodando ate a ontem 😕  era para eles nao deixarem nem instalar o pacote como ocorre com rhel 5 neh !!  lasca nois .

enxe o saco deles ae … pq esse tipo de coisa é foda

da uma pessima impressão.

gostaria de um retorno deles do pq parou de rodar … pq nao e bloqueado na instalacao e se tem como voltar. isso no minimo.



abraços","27/mai/23 11:21 PM;5ea9bfda0590bb0b7bedbaa1;Prezado Elias Mussi,

Agradecemos o retorno.

Conforme a matriz de compatibilidade da Elastic, a versão do RHEL 6 não é mais suportada. Gostaríamos de solicitar o enceramento deste chamado uma vez que está fora da matriz de compatibilidade.

Visando sempre reforçar a nossa parceria, continuaremos  à disposição para apoio consultivo nesta demanda.

Gratos e à disposição. ","28/mai/23 7:54 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;parceria com uma resposta dessas ? arrebenta em. vou ter um retorno dos questinamentos anterior ?

e bora lembrar q durante o processo de implantação em nenhum momento a dataprev foi informada q poderia ter esse tipo de problema de compatibilidade.","28/mai/23 12:07 PM;621538d1347c690072f6b56f;Prezado Elias Mussi,

Bom dia!

Ontem mesmo fizemos contato com a Elastic cobrando uma posição sobre esse comportamento. Questionei sobre o porquê disso ter acontecido só agora na atualização da 8.7 para a 8.8 se a compatibilidade acabou na 8.0. Perguntei também se seria possível fazer uma espécie de downgrade para que fosse possível continuar com o filebeat rodando nessas máquinas RHEL 6.x. Abaixo, segue resposta da Elastic:





Hello there,



Thanks for contacting Elastic Support. We are in receipt of your ticket.



As you noticed RHEL/CentOS 6.x is not supported since 8.0 as mentioned in our Support Matrix: https://www.elastic.co/support/matrix

This mean some dependencies are recompiled with newer version of the libs which are apparently not compatible anymore with the kernel you are using.



Unfortunately, it’s not possible to downgrade the Elastic Stack once upgraded to a newer version. But you can try to downgrade Filebeat to 8.7 (by reinstalling it) and see if it mitigates the issue. It looks like the issue you shared is directly related to Filebeat and you mentioned it was working on 8.7 so I would attempt to reinstall Filebeat 8.7.



That being said, I recommend to upgrade to a supported OS to avoid such issues.



Please let us know if you have any questions.



Thanks.



Best regards,

Francois-Clement

Elastic Support





Portanto, o recomendado foi que façamos o teste de downgrade para a versão anterior, de forma que seja possível continuar recebendo eventos dessas máquinas. Seguimos à disposição caso possua mais algum questionamento.

Atenciosamente,

Vinícius Prysthon","28/mai/23 12:21 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Boas,



eles poderiam bloquear a instalacao se nao suporta o so mais neh 🙂 

mas bora lá …  quais opções tenho ?

preciso que funcione nos rhel6. 

a versão 8.7 tem aquele bug … 

pelo q vi só a versao 7.17 tem suporte a rhel6 e nao deve durar muito tempo e não sei se tem o bug nela

preciso da melhor solução para o problema … falar que não tem suporte apena não resolve meu problema.



Abraços","28/mai/23 2:11 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Se você deseja que o Filebeat funcione no RHEL 6, pelo que a Elastic coloca na matriz de compatibilidade, teria que retornar para o 7.17, contudo, já deixamos registrado que um downgrade desse tamanho não segue as recomendações de melhores práticas da fabricante. 

Para esse donwgrade não trazer outros problemas, é necessário que exista a separação dos indíces dos que estão na versão 8 e desses que seriam da 7.17, porque pode dar problema de compatibilidade com o ECS. Então, para todos os serviços que estão no RHEL 6, vai ser necessário criar a versão do 7.17.

Você perguntou sobre a melhor solução, e com certeza olhando pelo lado da ferramenta, a melhor não é esse downgrade e sim fazer uma mudança de SO para algum que é compatível com a versão mais atual da ferramenta, que foi justamente o relatado na resposta da própria Elastic.

Infelizmente, como qualquer outro tipo de ferramenta que vá fazendo melhorias para trazer novas funcionalidades, a Elastic também vai deixando seus componentes incompatíveis com sistemas operacionais mais antigos como o RHEL 6. Por algum motivo continuou funcionando por algumas versões da atualização 8, mas não podemos ficar dependendo de algo incerto.



Atenciosamente,

Paulo Henrique","29/mai/23 9:36 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;boas,

é por ae mesmo a discussão … preciso de ajuda para construir o melhor desenho de coleta.

isso pq, em ambientes complexos (são mais de 10k maquinas aqui) e com sistemas de longos ciclos de vida, se terá sempre uma imansidão de versoes de SO.

Aqui mesmo são muitosss  tem rhel 4 ainda rodando.

nos rhel 5 eu tive de manter os logstash 6.2 coletando.

se for para ter suporte nos rhle 6, prefiro carregar eles com o 7.17 (que tem suporte ainda), o resto mais novo vai da versão mais recente.

eu ja mantenho a versao dos “beats” nos datastreams .. ja imaginava um cenario assim, fora os processos de mudança de versao.

agora … qual o impacto disso ? 

como essa mescla pode me atrapalhar ?  

posso coletar com o 7 e tentar mandar para o ingest mais recente ?

 qual a melhor forma de fazer isso dentro da minha arquitetura ?

bora esquentar esse quadro 🙂 

Abraços","29/mai/23 5:03 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

 Segue abaixo as mudanças que foram realizadas e funcionalidades que foram depreciadas na versão 8. 

Observe que a versão 8.X do Elastic ainda dá suporte a versão 7.17.X, comforme a matriz de compatibilidade dos produto da Elastic. Vide em ([https://www.elastic.co/pt/support/matrix#matrix_compatibility).|https://www.elastic.co/pt/support/matrix#matrix_compatibility).|smart-link]  

No entanto, é recomendável que se mantenha a versão conforme o documento de migração da Elastic que diz: 

Upgrade between minor versions
As a general rule, you can upgrade between minor versions (for example, 8.x to 8.y, where x < y) by simply installing the new release and restarting the Beat process. Beats typically maintain backwards compatibility for configuration settings and exported fields. Please review the release notes for potential exceptions.

Upgrading between non-consecutive major versions (e.g. 6.x to 8.x) is not supported.

Nesse caso, a criação dos índices, templates, mapping e etc, possuem a versão do agente que esta submetendo o evento, apesar de existir a compatibilidade entre a versão 7.17 e a 8.X. 

Também realizamos um instalação com versões diferentes para avaliar o impacto no uso de uma versão anterior de beats com uma versão de Elastic atual, nesse caso a 8.6 e constatamos que não há impacto seguindo a recomendação acima de manter a estrutura de templates e índices. Também averiguamos através do upgrade assistant se existia alguma incompatibilidade com o convívio entre as versões que não houve incompatibilidade.  

Todavia, segue as mudanças realizadas na versão 8 em relação as versões anteriores: 

1 - Sobre as mudanças em relação as versões anteriores:

Breaking changes in 8.0;

See the release notes for a complete list of breaking changes, bug fixes, and enhancements, including changes to beta or experimental functionality.

Beats logs are now ECS-compliant;

All Elastic Beats now write ECS-compliant logs in JSON format, and the config options for enabling ECS/JSON are removed. The new log name format is {beatname}-{date}(-n)?.ndjson. For example (from oldest to newest): filebeat-20200101.ndjson, filebeat-20200101-1.ndjson, filebeat-20200101-2.ndjson, and so on.

With this change, we now provide a uniform logging scheme across Elastic products, which makes it easier to develop ECS-aware visualizations for behaviors like correlating metrics or events across products.

If you have any monitoring configurations that expect the old log format, you must update them to use the new log format.

Beats now store events in data streams instead of indices;

All Beats shippers now store events in data streams in Elasticsearch instead of indices regardless of the Elasticsearch version. This change affects the underlying data storage method used in Elasticsearch; it does not change the naming convention. The name of the data stream follows the format {beatname}-{version}, and the index pattern is {beatname}-{version}. Do not confuse this change with the naming convention we use in Elastic integrations.

As part of this change, some index lifecycle managment (ILM) options (rollover_alias and pattern) are removed because data streams do not require index aliases. Also the setup.template.type option is removed because it no longer makes sense.

If you are loading JSON index templates by specifying a file in setup.template.json.path, make sure you move from the legacy format to composable index templates.

Data streams support create operations only. Thus, there is no way to use operation types like ""index"" or ""delete"" when sending events to Elasticsearch.

For more information, refer to the Elasticsearch documentation about data streams.

Homebrew formulae no longer available for Beats;

Starting with version 8.0.0, Elastic no longer publishes Homebrew formulae for Beats. You must use a different installation method. See the Beats download pages for available installation methods.

Removal of older platforms from Beats support matrix;

Starting with version 8.0.0, the following operating systems are no longer supported by Beats due to lack of vendor support for the operating system. In order to have the best product experience, you are encouraged to move to a vendor-supported version of the operating system.

These platforms are:

Centos 6
Centos 8
RHEL 6
Ubuntu 14.04
Ubuntu 16.04
Debian 8
Windows 7 SP1
Windows Server 2008 R2
MacOS 10.13
MacOS 10.14
Filebeat filesets are disabled by default;

Prior to version 8.0.0, modules had some filesets that were enabled by default. This meant that in some cases you could enable a module from the command line and run it without modifying the configuration in the modules.d directory. However this caused problems for some users.

Starting in version 8.0.0, filesets are disabled by default. You must explicitly enable the filesets you want Filebeat to use.

1.1 - Mudanças feitas nas versões do Filebeat 8 em relação as versões anteriores:

* Beats version 8.0.0;
Breaking changes;

Affecting all Beats

Remove the deprecated xpack.monitoring.* settings. Going forward only monitoring.* settings may be used. 9424 18608
Remove deprecated/undocumented IncludeCreatorMetadata setting from kubernetes metadata config options. 28006
Remove deprecated fields from kubernetes module. 28046
Remove deprecated config option aws_partition. 28120
Improve stats API by adding host metadata. 27963
Libbeat: logp package forces ECS compliant logs. Logs are JSON formatted. Options to enable ECS/JSON have been removed. 15544 28573
Remove auto from the available options of setup.ilm.enabled and set the default value to true. 28671
add_process_metadata processor: Replace usage of deprecated process.ppid field with process.parent.pid. 28620
add_docker_metadata processor: Replace usage of deprecated process.ppid field with process.parent.pid. 28620
Use data streams instead of indices for storing events from Beats. 28450
Remove option setup.template.type and always load composable template with data streams. 28450
Remove several ILM options (rollover_alias and pattern) as data streams do not require index aliases. 28450
Populate index template’s default_fields setting with ECS fields only. 28596 28215
Remove deprecated --template and --ilm-policy flags. Use --index-management instead. 28870
Remove logging.files.suffix option, and default to datetime endings in log file names. The format of the new name is {beatname}-{date}(-n)?.ndjson. Example log file names from oldest to newest: filebeat-20200101.ndjson, filebeat-20200101-1.ndjson, filebeat-20200101-2.ndjson. 28927
Align kubernetes configuration settings. 29908
Change log file extension for Beats and Elastic Agent to .ndjson. If you are collecting the logs, you must change the path configuration to /path/to/logs/{beatname}*.ndjson to avoid any issues. 28927
Remove legacy support for SSLv3. 30071
Filebeat

Add while_pattern type to multiline reader. 19662
auditd dataset: Use process.args to store program arguments instead of auditd.log.aNNN fields. 29601
Remove deprecated old awscloudwatch input name. 29844
Metricbeat

Remove network and diskio metrics from ec2 metricset. 28316
Rename read/write_io.ops_per_sec to read/write.iops in rds metricset. 28350
system/process metricset: Replace usage of deprecated process.ppid field with process.parent.pid. 28620
Packetbeat

event.category no longer contains the value network_traffic because this is not a valid ECS event category value. 20556
Remove deprecated TLS fields in favor of tls.server.x509 and tls.client.x509 ECS fields. 28487
HTTP: The field http.request.method will maintain its original case. 28620
Winlogbeat

Remove top level hash property from sysmon events. 20653
Move module processing from local Javascript processor to ingest node. 29184 29435

* Beats version 8.1.0;
Breaking changes;
Filebeat
Remove Recorded Future fileset integration from threatintel module. 30564
* Beats version 8.2.0;

Breaking changes;
Affecting all Beats
Fix mapping of parent process information provided by add_process_metadata. 29874 30727

* 8.3.0
Deprecated;
Heartbeat - Bump node.js version for synthetics to 16.15.0. 31675
* 8.4
Breaking changes;
Heartbeat - Browser monitors (beta) now write to the synthetics-* index prefix. 32064 - Setting a custom index for a given monitor is now deprecated. Streams are preferred. 32064 - Browser monitors now default to a max concurrency of two.
* Beats version 8.5.0;
Breaking changes;
Affecting all Beats
Upgrade to Go 1.18. Certificates signed with SHA-1 are now rejected. See the Go 1.18 release notes for details. 32493
Fix formatting of MAC hardware addresses populated by the add_host_metadata processor. 32264 32265

Deprecated;
Heartbeat - Deprecate zip_url and local monitor options. 33123

2 - Sobre as mudanças no ECS da versão 8.X em relação as versões anteriores: 

Schema changes;
Breaking changes;
Remove host.user.* field reuse. #1439
Remove deprecation notice on http.request.method. #1443
Migrate log.origin.file.line from integer to long. #1533
Remove log.original field. #1580
Remove process.ppid field. #1596

Added;
Added faas.* field set as beta. #1628, #1755
Improvements;
Wildcard type field migration GA. #1582
match_only_text type field migration GA. #1584
Threat indicator fields GA from RFC 0008. #1586
Tooling and artifact changes;

Breaking Changes;
Removing deprecated --oss from generator #1404
Removing use-cases directory #1405
Remove Go code generator. #1567
Remove template generation for ES6. #1680
Update folder structure for generated ES artifacts. #1700, #1762
Updated support for overridable composable settings template. #1737

Improvements;
Align input options for --include and --subset arguments #1519
Remove remaining Go deps after removing Go code generator. #1585
Add explicit default_field: true for Beats artifacts. #1633
Reorganize docs directory structure. #1679
Added support for analyzer definitions for text fields. #1737
Bugfixes;
Fixed the default_field flag for root fields in Beats generator. #1711

* 8.1.0;
Schema changes;
Added;
Added two new fields (sha384,tlsh) to hash schema and one field to pe schema (pehash). #1678
Added email.* beta field set. #1688, #1705

Removed;
Removing process.target.* reuses from experimental schema. #1666
Removing RFC 0014 pe.* fields from experimental schema. #1670
Tooling and artifact changes;

Improvements;
Update refs from master to main in [USAGE.md|http://USAGE.md] etc #1658
Clean up trailing spaces and additional newlines in schemas #1667
Use higher compression as default in composable index template settings. #1712
Bump dependencies. #1782

Bugfixes;
Fix invalid documentation link generation in component templates _meta. #1728

* 8.2.0;

Schema changes;
Added;

Add beta container.* metric fields. #1789
Add six new syslog fields to log.syslog.*. #1793
Added [faas.id|http://faas.id], [http://faas.name|http://faas.name|smart-link]  and faas.version fields as beta. #1796
Added linux event model beta fields and reuses to support RFC 0030. #1842, #1847, #1884
Added threat.feed.dashboard_id, threat.feed.description, [http://threat.feed.name|http://threat.feed.name|smart-link] , threat.feed.reference fields. #1844

Improvements;
email.* field set now GA. #1794, #1841
Tooling and artifact changes;

Added;
Adding optional field attribute, pattern. #1834
Added support for re-using a fieldset as an array. #1838
Added --force-docs option to generator. #1879

Improvements;
Update refs from master to main in [USAGE.md|http://USAGE.md] etc #1658
Clean up trailing spaces and additional newlines in schemas #1667
Use higher compression as default in composable index template settings. #1712

* 8.3.0;

Schema changes;

Added;
Added pattern attribute to .mac fields. #1871
Add [orchestrator.cluster.id|http://orchestrator.cluster.id]. #1875
Add [orchestrator.resource.id|http://orchestrator.resource.id]. #1878
Add orchestrator.resource.parent.type. #1889
Add orchestrator.resource.ip. #1889
Add container.image.hash.all. #1889
Add service.node.role. #1916
Advanced container.* metric fields to GA. #1927

Important;
After adding service.node.role, it was realized that we intend for this field to have multiple values, and therefore we will be removing role and replacing with roles at the earliest opportunity. Please do not use service.node.role.

* 8.5.0;

Schema changes;

Added;
Adding risk.* fields as experimental. #1994, #2010
Adding [http://process.io|http://process.io|smart-link] .* as beta fields. #1956, #2031
Adding process.tty.rows and process.tty.columns as beta fields. #2031
Changed process.env_vars field type to be an array of keywords. #2038
process.attested_user and process.attested_groups as beta fields. #2050
Added risk.* fieldset to beta. #2051, #2058
Moved Linux event model fields to GA. #2082

Improvements;
Advances threat.enrichments.indicator to GA. #1928
Added ios and android as valid values for os.type #1999
Tooling and artifact changes;

Bugfixes;
Added Deprecation Warning for misspell task #1993
Fix typo in client schema #2014

* 8.6.0;

Schema changes;

Added;
Adding vulnerability option for event.category. #2029
Added device.* field set as beta. #2030
Added tlp.version to threat #2074
Added fields for executable object format metadata for ELF, Mach-O and PE #2083

Improvements;
Added CLEAR and AMBER+STRICT as valid values for threat.indicator.marking.tlp and enrichments.indicator.marking.tlp to accept new [TLP 2.0|https://www.first.org/tlp/] markings #2022, #2074

* 8.7.0;

Schema changes;

Bugfixes;
Remove duplicated client.domain definition #2120

Added;
Adding name field to threat.indicator #2121
Adding api option to event.category #2147
Adding library option to event.category #2154

Improvements;
Description for [host.name|http://host.name] definition updated to encourage use of FDQN #2122
Tooling and artifact changes;

Improvements;
Updated usage docs to include threat.indicator.url.domain and changed indicator.marking.tlp and indicator.enrichments.marking.tlp from ""WHITE"" to ""CLEAR"" to align with TLP 2.0. #2124
Bump gitpython from 3.1.27 to 3.1.30 in /scripts. #2139



Atenciosamente,

Paulo Henrique","26/jul/23 1:10 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;pode fechar esse treco aqui 🙂 

usando o 7.17.nos rhel 6 e uma porrada com 8.6 😕 vai migrando devagar para baixo ",,,,,,,,,,,,,,,,,,,,,,,,,,,Itens concluídos
distribuição alocacao de disco na camada hot ...,DATAPREV-14,10918,Suporte técnico remoto: esclarecimento de dúvidas,Concluído(a),DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Muito Baixa,,Paulo Henrique Morato Góes,61e09d6968926d0068d83362,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,18/mai/23 8:55 PM,12/jun/23 12:53 PM,19/set/23 4:16 PM,,,0,"Boas,



turma .. ando preocupado com a distribuição de disco da alocação na camada hot

reparem : 

{noformat}shards disk.used disk.percent disk.avail node
   111   945.8gb           92     77.6gb n321p004984
   116   906.6gb           88    116.8gb n321p004966
   108   881.7gb           86    141.7gb n321p004983
   110   869.9gb           84    153.5gb n321p004981
   104   868.6gb           84    154.8gb n321p004967
   109   856.5gb           83    166.9gb n321p004974
   107   829.2gb           81    194.2gb n321p004980
   110   835.3gb           81    188.1gb n321p004973
   111     821gb           80    202.4gb n321p004965
   105   803.7gb           78    219.7gb n321p004971
   102   791.4gb           77      232gb n321p004969
   112   764.6gb           74    258.8gb n321p004972
   107   766.8gb           74    256.6gb n321p004970
   122   749.5gb           73    273.9gb n321p004979
   128     691gb           67    332.4gb n321p004975
   118   681.2gb           66    342.2gb n321p004968
   117   678.2gb           66    345.2gb n321p004977
   113     653gb           63    370.4gb n321p004976
   118   613.7gb           59    409.7gb n321p004982
   116   599.7gb           58    423.7gb n321p004978{noformat}

juro q eserava uma distribuição mais homogenea  …

isso ja causou um parada na gravação de varios indices … sem aviso e erro (q eu tenha percebido)

essa situação me preocupa

o que podemos fazer , antes q aconteça denovo ?



Abraços",,josue,Paulo Henrique Morato Góes,,,,62a79211d0132d00687f77aa,61e09d6968926d0068d83362,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i005l7:,,português,,,,,,,,,,,,,,,63:21,,,,,,,,2023-05-19 03:31:43.218,,"18/mai/23 8:57 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;ahh só para adicionar, eu ja vim diminuindo a qtde de dias em hot nas minhas polticas, mas essa diferença tao grande na alucação me preocupa.","19/mai/23 12:31 AM;61e09d6968926d0068d83362;Prezado Elias Mussi, boa noite!

Entendo que você já alterou a política da camada hot, mas poderia nos enviar como está a configuração da política atualmente para que possamos analisar? 

Aguardo retorno!","19/mai/23 9:14 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;{code:json}{""policy"":{""phases"":{""warm"":{""min_age"":""9d"",""actions"":{""forcemerge"":{""max_num_segments"":1,""index_codec"":""best_compression""},""readonly"":{},""set_priority"":{""priority"":50},""shrink"":{""max_primary_shard_size"":""50gb""}}},""cold"":{""min_age"":""30d"",""actions"":{""allocate"":{""number_of_replicas"":0,""include"":{},""exclude"":{},""require"":{}},""readonly"":{},""set_priority"":{""priority"":0}}},""hot"":{""min_age"":""0ms"",""actions"":{""rollover"":{""max_age"":""1d""},""set_priority"":{""priority"":100}}},""delete"":{""min_age"":""65d"",""actions"":{""delete"":{""delete_searchable_snapshot"":true}}}}}}
{code}

politica gera



e a politica para o fw (muito dados )

{code:json}{""policy"":{""phases"":{""warm"":{""min_age"":""5d"",""actions"":{""forcemerge"":{""max_num_segments"":1,""index_codec"":""best_compression""},""readonly"":{},""set_priority"":{""priority"":50},""shrink"":{""max_primary_shard_size"":""50gb""}}},""cold"":{""min_age"":""30d"",""actions"":{""allocate"":{""number_of_replicas"":0,""include"":{},""exclude"":{},""require"":{}},""readonly"":{},""set_priority"":{""priority"":0}}},""hot"":{""min_age"":""0ms"",""actions"":{""rollover"":{""max_age"":""1d""},""set_priority"":{""priority"":100}}},""delete"":{""min_age"":""65d"",""actions"":{""delete"":{""delete_searchable_snapshot"":true}}}}}}{code}



(ah esse json são os que uso para criar/alterar as politicas)

e mesmo assim a diferença me assusta .. nesse momento : 

  shards disk.used disk.percent disk.avail node
   105   920.7gb           89    102.7gb n321p004983
   111     915gb           89    108.4gb n321p004984
   115   879.1gb           85    144.3gb n321p004966
   110   879.3gb           85    144.1gb n321p004973
   109   864.7gb           84    158.7gb n321p004981
   104   865.4gb           84      158gb n321p004967
   104   856.5gb           83    166.9gb n321p004980
   109   839.2gb           81    184.2gb n321p004974
   110   788.1gb           77    235.3gb n321p004965
   105   775.6gb           75    247.8gb n321p004971
   121   769.1gb           75    254.3gb n321p004979
   106   735.9gb           71    287.5gb n321p004970
   102   710.3gb           69    313.1gb n321p004969
   117     704gb           68    319.4gb n321p004982
   114   690.5gb           67    332.9gb n321p004972
   119   682.5gb           66    340.9gb n321p004977
   117   676.1gb           66    347.3gb n321p004968
   127   650.8gb           63    372.6gb n321p004975
   115   634.8gb           62    388.6gb n321p004978
   113   628.9gb           61    394.5gb n321p004976





Abraços","19/mai/23 11:29 AM;61e09d6968926d0068d83362;Prezado Elias Mussi, bom dia!

Analisamos aqui os arquivos enviados em relação a política e não apresenta nenhuma situação que deva ser modificada. O entendimento para o que tem acontecido no cluster é que existe o balanceamento na distribuição dos shards entre os nós, porém como existem índices maiores que outros, isso faz com que existam shards que ocupem mais espaço em disco.

Sendo assim, a possibilidade que existe para “controlar” esse comportamento, é de manualmente realocar alguns shards em outros nodes. No link abaixo, existe uma explicação de como isso pode ser feito, juntamente com um exemplo para referência da API que deve ser realizada.

[https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-reroute.html|https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-reroute.html|smart-link] 



Atenciosamente,

Paulo Henrique","19/mai/23 11:34 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;vc falou o obvio para mim 

manualmente não é viavel

tem de haver uma outra forma q nao seje ficar brincando de realocar shards.

esse é o tipo de problema q ja pode ser escalado , isso pq ja ja um nó desses me acaba o disco e trava a gravação de indices .. e fatalmente nao saberemos quais são os travados

e ae ?","19/mai/23 2:21 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Para que o esclarecimento seja feito de forma mais fácil, sugerimos uma reunião virtual as 14:30 para que os pontos possam ser levantados.

Atenciosamente,

Paulo Henrique

[https://us06web.zoom.us/j/81260272214?pwd=ZVBOcUZPM3JvRjZSL3hDVUxHaGJCdz09|https://us06web.zoom.us/j/81260272214?pwd=ZVBOcUZPM3JvRjZSL3hDVUxHaGJCdz09|smart-link] ","19/mai/23 3:39 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Após reunião realizada no dia 19/05/2023 as 14:30, foram propostas duas estratégias para que a situação não comprometa a alocação nos discos e não trave a gravação dos índices, sendo definida em conjunto que a estratégia a ser adotada é de diminuir o tempo para acontecer o rollover dos dados presentes na camada hot para a camada warm, além da alteração do watermark que estava estabelecido anteriormente. Com isso, solicitamos permissão para fechamento do chamado após ser definida tal estratégia.

Atenciosamente,

Paulo Henrique","09/jun/23 12:51 PM;62a79211d0132d00687f77aa;Prezado Elias Mussi,

Gostaria de solicitar, com base nos esclarecimentos fornecidos durante a última interação, a autorização para o fechamento do chamado em questão.","12/jun/23 12:52 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;blz  fecha ae

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Itens concluídos
Solução Definitiva - Chamado 8 - filebeat dando erro e nao mandando eventos - (PRAZO 30 DIAS - 16/07/2023),DATAPREV-13,10916,Suporte técnico remoto: Incidente,Concluído(a),DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Normal,,Vinícius Prysthon,621538d1347c690072f6b56f,Vinícius Prysthon,621538d1347c690072f6b56f,Vinícius Prysthon,621538d1347c690072f6b56f,17/mai/23 4:02 PM,22/ago/23 11:01 AM,19/set/23 4:16 PM,,,0,"Prezados, boa tarde!

Tendo em vista que o “Chamado 8 - filebeat dando erro e nao mandando eventos” se trata de um bug que necessita correção e desenvolvimento de atualizações específicas do software, e em conformidade com o que versa no item 14.4 do TR, o chamado atual implica na ocorrência para provimento da solução definitiva, com o prazo de acordo com a severidade do chamado original, de acordo com o TR (a imagem contendo informações desses prazos para solução definitiva se encontra anexada neste chamado).

Assim, foi aberto um chamado com a fornecedora a fim de resolver o bug, e nós utilizaremos desse artifício para informar ao time da Dataprev qualquer atualização que a Elastic nos forneça a respeito do caso.

 

Atenciosamente,

Vinícius Prysthon",,Fabio Schmidt,josue,Vinícius Prysthon,,,5ea9bfda0590bb0b7bedbaa1,62a79211d0132d00687f77aa,621538d1347c690072f6b56f,,,,,,,,,,,22/jun/23 11:14 AM;ug:0bf5b6db-b989-4171-bb50-b139a583a751;CE - Resposta ao Oficio 011-2023 BkTech.pdf;https://bktech.atlassian.net/rest/api/3/attachment/content/11019,29/jun/23 3:53 PM;ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77;Captura de tela 2023-06-29 150337.png;https://bktech.atlassian.net/rest/api/3/attachment/content/11025,17/mai/23 4:02 PM;ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77;prazos-solucao-definitiva.png;https://bktech.atlassian.net/rest/api/3/attachment/content/10991,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i005kr:,,português,ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb(ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb),ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77(ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77),qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae(qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae),fabio(fabio),ug:0e2daf0a-9fff-4018-89ac-200a262b74f9(ug:0e2daf0a-9fff-4018-89ac-200a262b74f9),,,,,,,,,,-1366:19,,,,,,,,2023-05-27 17:31:00.173,,17/mai/23 4:02 PM;621538d1347c690072f6b56f;!prazos-solucao-definitiva.png|thumbnail!,"17/mai/23 4:38 PM;621538d1347c690072f6b56f;Prezados,

Em conversa com a Elastic, fomos informados de que a correção desse bug estará disponível na versão 8.8.0, que será lançada na próxima semana (estimativa dada pelo suporte da fabricante). Dito isso, ficaremos atentos para, assim que essa versão estiver disponível, avisarmos à equipe Dataprev para que o upgrade seja feito.



Atenciosamente,

Vinícius Prysthon","27/mai/23 2:31 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;{color:#172B4D}Boas,{color} {color:#172B4D}{color} 
  
{color:#172B4D} turminha, acabei de subir tudo aqui para a versao 8.8, e nao deu certo .. o erro permanece :{color}  {color:#172B4D}{color} 
  
{color:#172B4D} May 27 14:24:40 n221p026185 filebeat: {""log.level"":""info"",""@timestamp"":""2023-05-27T14:24:40.380-0300"",""log.logger"":""monitoring"",""log.origin"":{""file.name"":""log/log.go"",""file.line"":195},""message"":""Total metrics"",""service.name"":""filebeat"",""monitoring"":{""metrics"":{""beat"":{""cgroup"":{""cpu"":{""cfs"":{""period"":{""us"":100000},""quota"":{""us"":0}},""id"":""system.slice"",""stats"":{""periods"":0,""throttled"":{""ns"":0,""periods"":0}}},""cpuacct"":{""id"":""system.slice"",""total"":{""ns"":2838710602889821}},""memory"":{""id"":""system.slice"",""mem"":{""limit"":{""bytes"":9223372036854771712},""usage"":{""bytes"":830513152}}}},""cpu"":{""system"":{""ticks"":170,""time"":{""ms"":170}},""total"":{""ticks"":1030,""time"":{""ms"":1030},""value"":1030},""user"":{""ticks"":860,""time"":{""ms"":860}}},""handles"":{""limit"":{""hard"":4096,""soft"":4096},""open"":10},""info"":{""ephemeral_id"":""311878d0-c231-45f5-b416-45175bb74b96"",""name"":""filebeat"",""uptime"":{""ms"":149652},""version"":""8.8.0""},""memstats"":{""gc_next"":19868456,""memory_alloc"":10093936,""memory_sys"":37795080,""memory_total"":108964392,""rss"":65777664},""runtime"":{""goroutines"":10}},""filebeat"":{""events"":{""active"":0,""added"":88,""done"":88},""harvester"":{""closed"":2,""open_files"":0,""running"":0,""skipped"":0,""started"":2},""input"":{""log"":{""files"":{""renamed"":0,""truncated"":0}},""netflow"":{""flows"":0,""packets"":{""dropped"":0,""received"":0}}}},""libbeat"":{""config"":{""module"":{""running"":1,""starts"":3,""stops"":0},""reloads"":2,""scans"":2},""output"":{""batches"":{""split"":0},""events"":{""acked"":82,""active"":0,""batches"":26,""dropped"":0,""duplicates"":0,""failed"":0,""toomany"":0,""total"":82},""read"":{""bytes"":397,""errors"":0},""type"":""redis"",""write"":{""bytes"":98460,""errors"":0}},""pipeline"":{""clients"":0,""events"":{""active"":0,""dropped"":0,""failed"":0,""filtered"":6,""published"":82,""retry"":65,""total"":88},""queue"":{""acked"":82,""max_events"":4096}}},""registrar"":{""states"":{""cleanup"":0,""current"":4,""update"":88},""writes"":{""fail"":0,""success"":14,""total"":14}},""system"":{""cpu"":{""cores"":4},""load"":{color} 
{color:#172B4D} {color}{color:#172B4D}{""1"":0.22,""15"":0.12,""5"":0.19,""norm"":{""1"":0.055,""15"":0.03,""5"":0.0475}}}},""ecs.version"":""1.6.0""}}{color}{color:#172B4D}{color}
  
{color:#172B4D} 
  
 *May 27 14:24:45 n221p026185 filebeat: {""log.level"":""error"",""@timestamp"":""2023-05-27T14:24:45.090-0300"",""log.logger"":""input"",""log.origin"":{""file.name"":""input-logfile/manager.go"",""file.line"":182},""message"":""filestream input with ID 'rfb-bcnpjsincronizador-aplicacao' already exists, this will lead to data duplication, please use a different ID"",""service.name"":""filebeat"",""ecs.version"":""1.6.0""}* 

  
 *May 27 14:24:45 n221p026185 filebeat: {""log.level"":""error"",""@timestamp"":""2023-05-27T14:24:45.091-0300"",""log.logger"":""input"",""log.origin"":{""file.name"":""input-logfile/manager.go"",""file.line"":182},""message"":""filestream input with ID 'rfb-bcnpjsincronizador-pm2' already exists, this will lead to data duplication, please use a different ID"",""service.name"":""filebeat"",""ecs.version"":""1.6.0""}* 
  
n221p026185:/root:rpm -qa | grep filebeat 
filebeat-8.8.0-1.x86_64 
n221p026185:/root:{color} 
----
 
{color:#172B4D}{color}","27/mai/23 3:36 PM;621538d1347c690072f6b56f;Prezado Elias Mussi, boa tarde!

Ciente disso, assim que a versão 8.8.0 foi liberada, entramos em contato com a Elastic informando que o erro persistia. Fomos avisados de que essa correção virá na versão 8.8.1, que será liberada na metade de junho. Qualquer nova atualização a respeito do assunto, avisaremos.



Atenciosamente,

Vinícius Prysthon","27/mai/23 4:46 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Boas,

adiciona um revés para eles ae …

parece q mexeram em algo na versão 8.8 q fez ela nao rodar nos rhel da serie 6.x

to vendo muito desse erro aqui : 

{quote}Config OK 

FATAL: kernel too old 

/bin/bash: line 1: 7348 Aborted (core dumped) /usr/share/filebeat/bin/filebeat-god -r / -n -p /var/run/filebeat.pid -- /usr/share/filebeat/bin/filebeat -c /etc/filebeat/filebeat.yml --path.home /usr/share/filebeat --path.config /etc/filebeat --path.data /var/lib/filebeat --path.logs /var/log/filebeat [FAILED]{quote}

","29/mai/23 9:43 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;velim … pelo q vi eles colocaram essa correção para a 8.9 … ja to ferrado com a 8.8 hehehe  nao conseguem fazer eles lançarem uma 8.8.1 mais rapido nao ?

to com umas 1500 maquinas de 10k com esse erro 😞 

meu sonho seria uma 8.7.1 com esse erro corrigido e funcionando nos rhel 6.

Abraços

","29/mai/23 10:19 AM;621538d1347c690072f6b56f;Prezado Elias Mussi, bom dia!

Como mencionado anteriormente, conversamos com a Elastic na última sexta (26) e fomos informados de que essa correção virá sim na versão 8.8.1, como pode ver na thread do GitHub: [https://github.com/elastic/beats/pull/35134#issuecomment-1562774874|https://github.com/elastic/beats/pull/35134#issuecomment-1562774874|smart-link] 

Abaixo, segue a resposta da Elastic sobre o caso:



Hello Suporte,

According to the last comment in the GitHub issue, the backport pull request was missing in 8.8.0 and will be released in 8.8.1 which should go out by mid-June.
Please, let me know if you have any other questions.

Regards,
Luiz Santos



Assim, ficaremos no aguardo para que, na metade do próximo mês, isso já seja resolvido.

Atenciosamente,

Vinícius Prysthon",29/mai/23 11:19 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;rola uma pressao neles para essa 8.8.1 sair logo … no tempos deles em meio de junho sairia a 8.9,"01/jun/23 5:35 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;boas,

bora ampliar o problema …

to testando o filebeat 7.17.6 com as maquinas rhel 6.

e esse erro tambem ocorre lá : 

2023-06-01T17:32:28.400-0300    ERROR   [input] input-logfile/manager.go:183    filestream input with ID 'inss-apl09-benefant2' already exists, this will lead to data duplication, please use a different ID

temos de resolver isso lá tb.

Abraçcos","01/jun/23 10:25 PM;621538d1347c690072f6b56f;Prezado Elias Mussi, boa noite!

Ciente do problema. Estamos avaliando se há alguma versão anterior que não ocorra esse erro, uma vez que esta seria a única solução para utilização do filebeat nas máquinas RHEL 6.x. Assim que possível retorno com atualizações.

Atenciosamente,

Vinícius Prysthon","22/jun/23 11:13 AM;62a79211d0132d00687f77aa;Conforme o com comunicado em anexo emitido pela DATAPREV em resposta ao pleito realizado no ofício Nº 011/2023, a Dataprev concorda com a extensão do prazo para apresentação da solução definitiva do chamado 13, por mais 30 dias corridos. Sendo assim, o novo prazo iniciou em 17/06/2023 e finda em 16/07/2023. Será informado através do chamado, assim que o patch de correção for lançado.","22/jun/23 11:14 AM;62a79211d0132d00687f77aa;[^CE - Resposta ao Oficio 011-2023 BkTech.pdf]

Comunicado Externo enviado pela Dataprev.","29/jun/23 3:53 PM;621538d1347c690072f6b56f;Prezado Elias Mussi, boa tarde!

Informo que a versão 7.17.11 do Elastic foi disponibilizada hoje contendo a correção do erro referente aos filestreams nas máquinas RHEL 6.x.

Para acessar a versão nova, acesse: [https://www.elastic.co/pt/downloads/past-releases/elasticsearch-7-17-11|https://www.elastic.co/pt/downloads/past-releases/elasticsearch-7-17-11|smart-link] 

Abaixo, é possível verificar parte das notas de lançamento onde a correção é evidenciada.

!Captura de tela 2023-06-29 150337.png|width=656,height=245!

Solicito o teste dessa nova versão para verificarmos se o problema foi, de fato, solucionado.

Aguardo sua validação.

Atenciosamente,

Vinícius Prysthon",29/jun/23 4:54 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;bora testar 🙂 mas primeiro preciso de syncar meus repo internos,"14/jul/23 1:28 PM;5ea9bfda0590bb0b7bedbaa1;Prezado Elias Mussi, boa tarde!

Gostaríamos de solicitar o fechamento deste chamado, uma vez que a correção foi disponibilizada em 29 de Junho pela Elastic e conforme o prazo do ofício, o prazo para encerramento deste ticket se encerra na segunda-feira, 17/07.

Após o sync dos repositórios internos, poderá ser ser aberto um novo ticket para tratamento de qualquer solicitação ou dúvida.

Obrigado e à disposição.","14/jul/23 2:16 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;opa 



de acordo",,,,,,,,,,,,,,,,,,,,,,,Itens concluídos
error de parser,DATAPREV-12,10915,Suporte técnico remoto: esclarecimento de dúvidas,Concluído(a),DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Muito Baixa,,Vinícius Prysthon,621538d1347c690072f6b56f,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,17/mai/23 3:50 PM,26/jul/23 1:15 PM,19/set/23 4:16 PM,,,0,"{code:json}CouldnotindexeventtoElasticsearch.status: 400,
action: [
  ""create"",
  {
    : _id=>nil,
    : _index=>""logs-infra-dtp-syslog-8.7.0"",
    : routing=>nil
  },
  {
    ""@timestamp""=>2023-05-17T17: 15: 15.000Z,
    ""severity""=>""5"",
    ""input""=>{
      ""type""=>""udp""
    },
    ""dtp.logstash_indexer""=>""n321p004902.fast.prevnet"",
    ""@version""=>""1"",
    ""mnemonic""=>""PORT_DOWN"",
    ""msg""=>""port-channel28: Ethernet118/1/7 is down"",
    ""host""=>{
      ""hostname""=>""BRT"",
      ""name""=>""n321p004900""
    },
    ""message""=>""<189>: 2023 May 17 14:15:15 BRT: %ETH_PORT_CHANNEL-5-PORT_DOWN: port-channel28: Ethernet118/1/7 is down"",
    ""data_stream""=>{
      ""namespace""=>""syslog-8.7.0"",
      ""type""=>""logs"",
      ""dataset""=>""infra-dtp""
    },
    ""type""=>""syslog"",
    ""ecs""=>{
      ""version""=>""8.0.0""
    },
    ""dtp""=>{
      ""categoria""=>""syslogpadrao"",
      ""cp""=>""cpsp"",
      ""cliente""=>""dtp"",
      ""ambiente""=>""homolog"",
      ""type""=>""syslog"",
      ""sistema""=>""syslogpadrao""
    },
    ""facility""=>""ETH_PORT_CHANNEL"",
    ""timestamp""=>[
      ""May 17 14:15:15"",
      ""May 17 14:15:15""
    ],
    ""log""=>{
      ""source""=>{
        ""address""=>""10.151.64.7:43190""
      }
    },
    ""dtp.redis_key""=>""dtp-syslog"",
    ""dtp.redis_server""=>""n331p004899.fast.prevnet"",
    ""event""=>{
      
    },
    ""agent""=>{
      ""name""=>""n321p004900"",
      ""id""=>""7f3deb41-454a-4dc3-bbee-1e2ad3750115"",
      ""version""=>""8.7.0"",
      ""type""=>""filebeat"",
      ""ephemeral_id""=>""a32b8183-65f7-460c-bbb8-fb5ffe0eeee4""
    },
    ""tags""=>[
      ""_dateparsefailure""
    ]
  }
],
response: {
  ""create""=>{
    ""_index""=>"".ds-logs-infra-dtp-syslog-8.7.0-2023.05.06-000013"",
    ""_id""=>""UR21KogBjAk8BYlWeure"",
    ""status""=>400,
    ""error""=>{
      ""type""=>""mapper_parsing_exception"",
      ""reason""=>""failed to parse field [timestamp] of type [date] in document with id 'UR21KogBjAk8BYlWeure'. Preview of field's value: 'May 17 14:15:15'"",
      ""caused_by""=>{
        ""type""=>""illegal_argument_exception"",
        ""reason""=>""failed to parse date field [May 17 14:15:15] with format [strict_date_optional_time||epoch_millis]"",
        ""caused_by""=>{
          ""type""=>""date_time_parse_exception"",
          ""reason""=>""date_time_parse_exception: Failed to parse with all enclosed parsers""
        }
      }
    }
  }
}{code}





bem nessa linha …

{noformat}  ""reason""=>""failed to parse field [timestamp] of type [date] in document with id 'UR21KogBjAk8BYlWeure'. Preview of field's value: 'May 17 14:15:15'"",{noformat}



formato de date …

ta com cara q o formato q o switch me envia não é suportado … como faz ae ?",,Vinícius Prysthon,,,,,621538d1347c690072f6b56f,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i005kj:,,português,ug:0e9c7acd-4c78-4ef6-beb2-b00f34bddd59(ug:0e9c7acd-4c78-4ef6-beb2-b00f34bddd59),,,,,,,,,,,,,,42:22,,,,,,,,2023-05-17 19:28:39.563,,"17/mai/23 4:28 PM;621538d1347c690072f6b56f;Prezado Elias Mussi, boa tarde!

O problema, aparentemente, está no formato de date enviado, que não é suportado. Sugiro alterar para algum dos formatos suportados pelo Elastic, como por exemplo o {{date_optional_time}} _-_ {{yyyy-MM-dd'T'HH:mm:ss.SSSZ}} (você pode acessar os formatos suportados pelo link [https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html|https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-date-format.html|smart-link]). Essa alteração pode ser feita através de um ingest pipeline.



Atenciosamente,

Vinícius Prysthon","17/mai/23 4:37 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;padrao cisco (antigooo )  como faz ?  la pelo q sei nao tem como mudar esse formato de envio 😕 

","17/mai/23 6:46 PM;621538d1347c690072f6b56f;Prezado Elias Mussi,

Você diz que não tem como mudar o formato do campo antes do envio, certo? Então seria necessário a criação de um filtro no pipeline pra tratar esse date field. Poderia enviar, por gentileza, o pipeline utilizado no processo de indexação?



Atenciosamente, 

Vinícius Prysthon",24/jun/23 10:50 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;cai no pipeline do syslog,"25/jun/23 1:39 PM;621538d1347c690072f6b56f;Prezado Elias Mussi, boa tarde!

Isso, nesse caso você precisaria criar um filter no pipeline do syslog pra poder formatar esse campo de data em um formato reconhecido pelo Elasticsearch, como por exemplo o formato ISO 8601. O filtro ficaria algo parecido com:

{noformat}filter {
  date {
    match => [ ""timestamp"", ""MMM dd HH:mm:ss"", ""MMM  d HH:mm:ss"" ]
    target => ""@timestamp""
  }
}{noformat}

Atenciosamente,

Vinícius Prysthon ",26/jul/23 1:15 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;pode fechar essa ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Itens concluídos
Solução Definitiva - Chamado 10 - Kibana monitoring logstash - (PRAZO 45 DIAS - 30/06/2023),DATAPREV-11,10914,Suporte técnico remoto: Incidente,Concluído(a),DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Baixa,,Paulo Henrique Morato Góes,61e09d6968926d0068d83362,Paulo Henrique Morato Góes,61e09d6968926d0068d83362,Paulo Henrique Morato Góes,61e09d6968926d0068d83362,17/mai/23 3:45 PM,22/ago/23 10:28 AM,19/set/23 4:16 PM,,,0,"Prezados, boa tarde!

Tendo em vista que o “Chamado 10 - Kibana monitoring logstash” se trata de um bug que necessita correção e desenvolvimento de atualizações específicas do software, e em conformidade com o que versa no item 14.4 do TR, o chamado atual implica na ocorrência para provimento da solução definitiva, com o prazo de acordo com a severidade do chamado original, de acordo com o TR (a imagem contendo informações desses prazos para solução definitiva se encontra anexada neste chamado).

Assim, foi aberto um chamado com a fornecedora a fim de resolver o bug, e nós utilizaremos desse artifício para informar ao time da Dataprev qualquer atualização que a Elastic nos forneça a respeito do caso.



Atenciosamente,

Paulo Henrique

",,Paulo Henrique Morato Góes,,,,,61e09d6968926d0068d83362,,,,,,,,,,,,,22/mai/23 4:21 PM;ug:0e9c7acd-4c78-4ef6-beb2-b00f34bddd59;Generate HAR Archive of Network Timings_Details for a webpage _ Elastic Support.mhtml;https://bktech.atlassian.net/rest/api/3/attachment/content/10993,23/jun/23 2:43 PM;ug:0e9c7acd-4c78-4ef6-beb2-b00f34bddd59;chamado-monitoring.png;https://bktech.atlassian.net/rest/api/3/attachment/content/11021,02/jun/23 5:16 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;consulta.txt;https://bktech.atlassian.net/rest/api/3/attachment/content/11000,12/jun/23 2:44 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;consulta002-2h.txt.gz;https://bktech.atlassian.net/rest/api/3/attachment/content/11003,12/jun/23 2:44 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;consulta002-30m.txt.gz;https://bktech.atlassian.net/rest/api/3/attachment/content/11002,12/jun/23 2:44 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;consulta002.txt.gz;https://bktech.atlassian.net/rest/api/3/attachment/content/11001,16/jun/23 2:24 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;consulta3.txt;https://bktech.atlassian.net/rest/api/3/attachment/content/11005,16/jun/23 2:24 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;consulta3_2023-06-15.txt;https://bktech.atlassian.net/rest/api/3/attachment/content/11006,02/jun/23 5:16 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;metricbeat-20230531-593.ndjson.gz;https://bktech.atlassian.net/rest/api/3/attachment/content/10999,22/mai/23 4:59 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;n321p005265.fast.prevnet.zip;https://bktech.atlassian.net/rest/api/3/attachment/content/10994,17/mai/23 3:44 PM;ug:0e9c7acd-4c78-4ef6-beb2-b00f34bddd59;prazos-solucao-definitiva.png;https://bktech.atlassian.net/rest/api/3/attachment/content/10990,,,,,,,,,,,,,,,,,,,Dataprev,0|i005kb:,,português,ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb(ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb),ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77(ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77),qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae(qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae),fabio(fabio),ug:0e2daf0a-9fff-4018-89ac-200a262b74f9(ug:0e2daf0a-9fff-4018-89ac-200a262b74f9),,,,,,,,,,-1008:15,,,,,,,,2023-05-17 19:58:35.105,,17/mai/23 3:45 PM;61e09d6968926d0068d83362;!prazos-solucao-definitiva.png|thumbnail!,"17/mai/23 4:20 PM;61e09d6968926d0068d83362;Prezado Elias Mussi, boa tarde!

Após abrirmos o chamado com a Elastic para verificar essa questão, eles nos responderam o seguinte:

_“Você pode compartilhar logs desta instância LS? Vamos ver se podemos encontrar pistas sobre por que o monitoramento parou de funcionar na última 1 hora.”_

Então, gostaríamos de solicitar estes logs da instância do Logstash para que possamos enviar à Elastic investigar mais afundo o bug que ocorreu.



Aguardo retorno,

Paulo Henrique","17/mai/23 4:58 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;{color:#172B4D}Boas,{color} {color:#172B4D}{color} 
  
{color:#172B4D} é um cluster com 15 nós atras.{color} 
{color:#172B4D} e o monit nao parou ... so nao mostra ..{color}  
{color:#172B4D} e algo no kibana q ta cagado para visualizacao menor que 1h. qdo se coloca 2h aparace tudo{color} {color:#172B4D}{color} 
  
{color:#172B4D} sacou ?{color} 
----
 
{color:#172B4D}{color}","22/mai/23 4:21 PM;61e09d6968926d0068d83362;Prezado Elias Mussi, boa tarde!

Expliquei novamente para a Elastic que a situação não se tratava de falta de logs e sim algum problema na visualização do Kibana. Eles responderam requisitando compartilhar um arquivo HAR quando carregar a página de monitoramento do Logstash com o tempo setado de 1hr. Além disso, também requisitaram os logs do Kibana.

Caso queira um passo a passo para gerar o arquivo HAR a Elastic me enviou um que deve ser seguido para coletar as informações necessárias. Não consigo compartilhar o link para que você possa executar em sua máquina, porém gerei um arquivo HTML com a página contendo todo o passo a passo e segue como anexo.



Atenciosamente,

Paulo Henrique



[^Generate HAR Archive of Network Timings_Details for a webpage _ Elastic Support.mhtml]

","22/mai/23 4:59 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;{color:#172B4D}Boas,{color} {color:#172B4D}{color} 
  
{color:#172B4D} bora ver se via mail chega ae.{color} 
----
 
{color:#172B4D}{color}

[^n321p005265.fast.prevnet.zip] _(0.0 kB)_","25/mai/23 11:30 AM;61e09d6968926d0068d83362;Prezado Elias Mussi, bom dia!

Ao enviar o arquivo HAR para o suporte da Elastic, a resposta deles foi para verificarmos se o cluster possui documentos de cluster_status para a última hora, pois esse costuma ser o problema em casos como o relatado. Aparentemente o coletor é capaz de enviar as métricas, mas não o status do cluster.

Sendo assim, eles pediram para que a seguinte consulta seja realizada no DevTools:



{noformat}GET *.monitoring-*,*metricbeat*/_search
{
  ""size"": 1000,
  ""query"": {
    ""bool"": {
      ""filter"": [
        {
          ""term"": {
            ""type"": ""cluster_stats""
          }
        },
        {
          ""range"": {
            ""timestamp"": {
              ""gte"": ""now-60m""
            }
          }
        }
      ]
    }
  },
  ""_source"": [""cluster_uuid""], 
  ""collapse"": {
    ""field"": ""cluster_uuid""
  }
}{noformat}



Após realizar a consulta, por gentileza enviar o resultado dela para repassarmos ao suporte da Elastic.



Atenciosamente,

Paulo Henrique","26/mai/23 12:04 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;{color:#172B4D}Boas,{color} {color:#172B4D}{color} 
  
{color:#172B4D} segue o resultado :{color} 
{color:#172B4D} { 
 ""took"": 3016, 
 ""timed_out"": false, 
 ""_shards"": { 
 ""total"": 24, 
 ""successful"": 24, 
 ""skipped"": 0, 
 ""failed"": 0 
 }, 
 ""hits"": { 
 ""total"": { 
 ""value"": 360, 
 ""relation"": ""eq"" 
 }, 
 ""max_score"": null, 
 ""hits"": [ 
 { 
 ""_index"": "".monitoring-es-7-2023.05.26"", 
 ""_id"": ""wGnKVYgBHK9ze4kg3NUL"", 
 ""_score"": 0, 
 ""_source"": { 
 ""cluster_uuid"": ""P1-TMHoLTvGklUMqICxxkA"" 
 }, 
 ""fields"": { 
 ""cluster_uuid"": [ 
 ""P1-TMHoLTvGklUMqICxxkA"" 
 ] 
 } 
 } 
 ] 
 } }{color}
  {color:#172B4D}{color} 
  
{color:#172B4D} uma coisa que vale a pena observar nas figuras q mandei é q na qual aparece as informações , tem os dados dos ultimos minutos.{color} {color:#172B4D}{color} 
  
{color:#172B4D} Abraços{color} {color:#172B4D}{color} 
----
 
{color:#172B4D}{color}","26/mai/23 4:29 PM;61e09d6968926d0068d83362;Prezado Elias Mussi, boa tarde!

O resultado foi enviado e segue abaixo a resposta da própria Elastic, requisitando também o resultado quando o range da query é setado em 120m para comparar os resultados obtidos:

_Olá, Obrigado por compartilhar os resultados._

_O que acontece se você expandir o intervalo de tempo na mesma consulta, como você faz para visualizar as métricas na interface do usuário? Por exemplo, com 60 minutos ou 120 minutos._

_Eu quero verificar se vemos algum dado nos índices do monitoring-logstash._

_Por favor, compartilhe a saída conosco._



Atenciosamente,

Paulo Henrique","26/mai/23 4:58 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;{color:#172B4D}Boas,{color} {color:#172B4D}{color} 
  
{color:#172B4D} 60min :{color}  
{color:#172B4D} { 
 ""took"": 1275, 
 ""timed_out"": false, 
 ""_shards"": { 
 ""total"": 23, 
 ""successful"": 23, 
 ""skipped"": 0, 
 ""failed"": 0 
 }, 
 ""hits"": { 
 ""total"": { 
 ""value"": 360, 
 ""relation"": ""eq"" 
 }, 
 ""max_score"": null, 
 ""hits"": [ 
 { 
 ""_index"": "".monitoring-es-7-2023.05.26"", 
 ""_id"": ""KsRsWYgB2ug-twWOBUsG"", 
 ""_score"": 0, 
 ""_source"": { 
 ""cluster_uuid"": ""P1-TMHoLTvGklUMqICxxkA"" 
 }, 
 ""fields"": { 
 ""cluster_uuid"": [ 
 ""P1-TMHoLTvGklUMqICxxkA"" 
 ] 
 } 
 } 
 ] 
 } }{color}
  {color:#172B4D}{color} 
  {color:#172B4D}{color} 
  {color:#172B4D}{color} 
  
{color:#172B4D} 120m:{color} 
{color:#172B4D} { 
 ""took"": 723, 
 ""timed_out"": false, 
 ""_shards"": { 
 ""total"": 23, 
 ""successful"": 23, 
 ""skipped"": 0, 
 ""failed"": 0 
 }, 
 ""hits"": { 
 ""total"": { 
 ""value"": 715, 
 ""relation"": ""eq"" 
 }, 
 ""max_score"": null, 
 ""hits"": [ 
 { 
 ""_index"": "".monitoring-es-7-2023.05.26"", 
 ""_id"": ""4LpGWYgB2ug-twWOyZWw"", 
 ""_score"": 0, 
 ""_source"": { 
 ""cluster_uuid"": ""P1-TMHoLTvGklUMqICxxkA"" 
 }, 
 ""fields"": { 
 ""cluster_uuid"": [ 
 ""P1-TMHoLTvGklUMqICxxkA"" 
 ] 
 } 
 } 
 ] 
 } }{color}
  {color:#172B4D}{color} 
  
{color:#172B4D} 180:{color}  {color:#172B4D}{color} 
  
{color:#172B4D} { 
 ""took"": 498, 
 ""timed_out"": false, 
 ""_shards"": { 
 ""total"": 23, 
 ""successful"": 23, 
 ""skipped"": 0, 
 ""failed"": 0 
 }, 
 ""hits"": { 
 ""total"": { 
 ""value"": 1069, 
 ""relation"": ""eq"" 
 }, 
 ""max_score"": null, 
 ""hits"": [ 
 { 
 ""_index"": "".monitoring-es-7-2023.05.26"", 
 ""_id"": ""YpIIWYgBHK9ze4kgpzm7"", 
 ""_score"": 0, 
 ""_source"": { 
 ""cluster_uuid"": ""P1-TMHoLTvGklUMqICxxkA"" 
 }, 
 ""fields"": { 
 ""cluster_uuid"": [ 
 ""P1-TMHoLTvGklUMqICxxkA"" 
 ] 
 } 
 } 
 ] 
 } }{color}
  {color:#172B4D}{color} 
  {color:#172B4D}{color} 
  
{color:#172B4D} 600:{color} 
{color:#172B4D} { 
 ""took"": 929, 
 ""timed_out"": false, 
 ""_shards"": { 
 ""total"": 23, 
 ""successful"": 23, 
 ""skipped"": 0, 
 ""failed"": 0 
 }, 
 ""hits"": { 
 ""total"": { 
 ""value"": 3453, 
 ""relation"": ""eq"" 
 }, 
 ""max_score"": null, 
 ""hits"": [ 
 { 
 ""_index"": "".monitoring-es-7-2023.05.26"", 
 ""_id"": ""6XF-V4gBHK9ze4kgWzk3"", 
 ""_score"": 0, 
 ""_source"": { 
 ""cluster_uuid"": ""P1-TMHoLTvGklUMqICxxkA"" 
 }, 
 ""fields"": { 
 ""cluster_uuid"": [ 
 ""P1-TMHoLTvGklUMqICxxkA"" 
 ] 
 } 
 } 
 ] 
 } }{color} 
----
 
{color:#172B4D}{color}","30/mai/23 12:06 PM;61e09d6968926d0068d83362;Prezado Elias Mussi, boa tarde.

Após o envio dos resultados, a Elastic nos retornou com mais algumas requisições em busca de resolver o problema de visualização.

Primeiramente perguntaram se o Metricbeat também foi atualizado para a versão 8.x e se houve alguma mudança nas configurações desse componente. Também disseram que nas imagens e no arquivo HAR contém apenas requisições para o Elasticsearch, querem saber se então não tem nenhum monitoramento de dados.

Também solicitaram o resultado de mais essa query, parecida com a anterior mas com algumas informações adicionais:

{noformat}POST *:.monitoring-es-*,*:metrics-elasticsearch.cluster_stats-*,.monitoring-es-*,metrics-elasticsearch.cluster_stats-*/_search
{
  ""query"": {
    ""bool"": {
      ""filter"": [
        {
          ""bool"": {
            ""should"": [
              {
                ""term"": {
                  ""data_stream.dataset"": ""elasticsearch.cluster_stats""
                }
              },
              {
                ""term"": {
                  ""metricset.name"": ""cluster_stats""
                }
              },
              {
                ""term"": {
                  ""type"": ""cluster_stats""
                }
              }
            ]
          }
        },
        {
          ""range"": {
            ""timestamp"": {
              ""gte"":  ""now-15m"",
              ""lte"": ""now""
            }
          }
        }
      ]
    }
  },
  ""collapse"": {
    ""field"": ""cluster_uuid""
  },
  ""sort"": {
    ""timestamp"": {
      ""order"": ""desc"",
      ""unmapped_type"": ""long""
    }
  }
}{noformat}

Para finalizar, também solicitaram os logs do Metricbeat no modo debug para entender o comportamento.



Aguardo retorno,

Paulo Henrique",02/jun/23 3:37 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;receberam os logs ae ?,"02/jun/23 3:44 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Não recebi aqui não.","02/jun/23 5:14 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;o debug ficou um arq de 26mega

tem como anexar arquivo aqui no Jira ?

","02/jun/23 5:16 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;[^metricbeat-20230531-593.ndjson.gz]

","02/jun/23 5:16 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;[^consulta.txt]

ooo coro :)","12/jun/23 10:14 AM;61e09d6968926d0068d83362;Prezado Elias Mussi, bom dia.

A Elastic pediu mais algumas evidencias e algumas explicações mais claras sobre o erro. Para facilitar o retorno que darei pra eles com tudo que for necessário, gostaria de pedir uma reunião, para agilizar o processo. Como está a sua agenda hoje?



Atenciosamente,

Paulo Henrique","12/jun/23 11:04 AM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Caso não seja possível a realização da reunião, segue a resposta do time de engenheiro da Elastic com os questionamentos e solicitações feitas:

Hello there,
Thanks for sending it.

I have forwarded this to the engineering team, please find their response and asks:
 

I’m a little confused about what’s going on here.

The screenshot shows a Logstash view which is missing metrics in the graphs but *does* have some data in the summary bar. So there *is some* monitoring data.

Further, the HAR, follow up queries and Metricbeat logs all seem to be for the Elasticsearch cluster, not the Logstash instances. Notably also is the result of your queries in the opening post, they show a different cluster UUID than that one which is in the screenshot and the ones found in the last query result.

Can you verify which part of Stack Monitoring is affected? Elasticsearch, Logstash or both?
A HAR from the same page where the screenshot was taken would be great (or a confirmation that the HAR attached is from that same page).

I will provide the exact query that the UI performs for that view so we can see the result for the last hour and last two hours but we’ll likely need the logs from the Logstash Metricbeat process.

One final note, this looks like self managed, please ask the client to double-check the clock configuration of the machine where Metricbeat runs for monitoring Logstash. If the time there is shifted then the documents will be ingested with the wrong timestamps (we’ve seen this before). Alternatively, if they’ve set the timezone in Kibana wrong that may also affect the results.

The API for that page does two Elasticsearch queries: one for the summary and one for the metrics.
The summary seems to be fine in both time ranges, which means (probably) the cluster UUID is there, the Logstash instances are connected to that cluster and Metricbeat is able to collect Logstash node stats.

Let’s focus on the metrics query instead.
The metrics query needs the cluster UUID which is resolved first (in this case {{o1VCulAUQEKEn7fAoYwjTw}}).

This is the query that’s run (for the last hour, modified to also include 10 hits):

{noformat}GET .monitoring-logstash-*,metrics-logstash.stack_monitoring.*-*/_search
{
  ""size"": 10,
  ""query"": {
    ""bool"": {
      ""filter"": [
        {
          ""term"": {
            ""cluster_uuid"": ""o1VCulAUQEKEn7fAoYwjTw""
          }
        },
        {
          ""range"": {
            ""logstash_stats.timestamp"": {
              ""format"": ""epoch_millis"",
              ""gte"": ""now-1h"",
              ""lte"": ""now""
            }
          }
        },
        {
          ""bool"": {
            ""should"": [
              {
                ""term"": {
                  ""data_stream.dataset"": ""logstash.stack_monitoring.node_stats""
                }
              },
              {
                ""term"": {
                  ""metricset.name"": ""node_stats""
                }
              },
              {
                ""term"": {
                  ""type"": ""logstash_stats""
                }
              }
            ]
          }
        }
      ]
    }
  },
  ""aggs"": {
    ""check"": {
      ""date_histogram"": {
        ""field"": ""logstash_stats.timestamp"",
        ""fixed_interval"": ""10s""
      },
      ""aggs"": {
        ""metric"": {
          ""max"": {
            ""field"": ""logstash_stats.events.in""
          }
        },
        ""metric_deriv"": {
          ""derivative"": {
            ""buckets_path"": ""event_rate"",
            ""gap_policy"": ""skip"",
            ""unit"": ""1s""
          }
        },
        ""logstash_uuids"": {
          ""terms"": {
            ""field"": ""logstash_stats.logstash.uuid"",
            ""size"": 1000
          },
          ""aggs"": {
            ""event_rate_per_node"": {
              ""max"": {
                ""field"": ""logstash_stats.events.in""
              }
            }
          }
        },
        ""event_rate"": {
          ""sum_bucket"": {
            ""buckets_path"": ""logstash_uuids>event_rate_per_node"",
            ""gap_policy"": ""skip""
          }
        },
        ""indices"": {
          ""terms"": {
            ""field"": ""_index""
          }
        }
      }
    }
  }
}
{noformat}

If we could run this query (and the same but for the last 2 hours), we would be able to see what the API sees and trace that back to what the UI renders and figure out where the issue is.

​

Please let us know in case you have any questions about collecting it.

Thank you.","12/jun/23 2:44 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Boas,

como vcs sabem nao fizemos nenhum setup especial para timezone .. apenas a do SO mesmo 🙂 

n321p005265:/root:timedatectl status
               Local time: Mon 2023-06-12 13:23:05 -03
           Universal time: Mon 2023-06-12 16:23:05 UTC
                 RTC time: Mon 2023-06-12 13:23:05
                Time zone: America/Sao_Paulo (-03, -0300)
System clock synchronized: yes
              NTP service: inactive
          RTC in local TZ: yes

[^consulta002-30m.txt.gz]
[^consulta002-2h.txt.gz]
[^consulta002.txt.gz]

","16/jun/23 2:06 PM;61e09d6968926d0068d83362;Prezado Elias Mussi, boa tarde!

Segue abaixo a resposta da Elastic após o envio das últimas evidências:

I can see in all results that some buckets are missing metric values, so some of the results are actually showing the data in stack monitoring maybe this is not the main issue.

So I started comparing the results for 1h and 2h.
The first main difference we have is in the logstash service - the hostname is different: (n321p00493*9* for the 1h and n321p00493*8* for 2h)
An interesting difference I see also see is that inside the {{pipelines}} - the {{ephemeral_id}} is present only in the 2h results. Also the node.stats look different - in the 2h results, there is {{jvm}} data which is not present in the 1h result.

Overall those results look similar when I compared the metrics values but looks like they are hitting different logstash services which can be the issue.

Can you please give me more information about the 30-minute result- is the data available in stack monitoring for this interval?
I see that the 30-minute result has the same logstash service host name as the 2h one. But the {{jvm}} data is still missing in the node.stats data (in this case we have the {{ephemeral_id}}. Here we also have the same situation (as in 1h and 2h intervals) - some buckets are returning metrics data and some are returning:

{noformat}                    ""metric"": {
                        ""value"": null
                    },
                    ""event_rate"": {
                        ""value"": 0.0
                    },
                    ""metric_deriv"": {
                        ""value"": null,
                        ""normalized_value"": null
                    }
{noformat}

So what I am trying to understand first is which time interval (2h, 1h, 30m) using the same time range in the UI is showing data in stack monitoring and which doesn’t so I can analyze the details.

Another thing that can help me investigate this - can you please ask them to run this request (time range min/max should be the range when the monitoring data was missing):

{noformat}GET {MONITORING_CLUSTER_URL}/api/monitoring/v1/_health?min=2023-06-12T15:06:07.195Z&max=2023-06-12T18:06:07.195Z
{noformat}

and get the result from the browser so I can check for any additional errors.

​

Please let us know in case you have any questions about collecting it.

Thank you.


Dessa forma, se puder enviar essa nova consulta que nos foi requisitada, para eles chegarem em uma conclusão a respeito do ocorrido.



Aguardo retorno,

Paulo Henrique","16/jun/23 2:24 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;ummm

uma coisa q vale a pena passar para ele se for o caso é a conf do metricbeat q to usando.

* module: logstash
xpack.enabled: true
period: 60s
hosts:
** n321p004948.fast.prevnet:9600
** n321p004949.fast.prevnet:9600
** n321p004950.fast.prevnet:9600
** n321p004951.fast.prevnet:9600
** n321p004952.fast.prevnet:9600
** n321p004953.fast.prevnet:9600
** n321p004954.fast.prevnet:9600
** n321p004955.fast.prevnet:9600
** n321p004956.fast.prevnet:9600
** n321p004936.fast.prevnet:9600
** n321p004937.fast.prevnet:9600
** n321p004938.fast.prevnet:9600
** n321p004939.fast.prevnet:9600
** n321p004940.fast.prevnet:9600



esse mesmo tipo de falha ocorre para o dash do elasticsearch



rodei a consulta dele para o dado do dia q ele passou e para ontem no mesmo horario



[^consulta3_2023-06-15.txt]
[^consulta3.txt]

","23/jun/23 11:57 AM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Segue abaixo as observações feitas pela Elastic a partir da análise das últimas evidências:

Primeiramente, em relação aos erros 401 que temos para a última solicitação, você poderia perguntar ao cliente se eles ativaram alguns módulos e não forneceram o nome de usuário e senha na configuração dos módulos? Descobri que isso foi a causa dos mesmos erros em um problema anterior. Referência:

A primeira diferença principal que temos está no serviço do Logstash - o nome do host é diferente: (n321p004939 para 1h e n321p004938 para 2h).

Conforme relatado em um comentário anterior, isso é interessante porque a última resposta para o monitoramento de saúde relata um erro no Logstash apenas no relatório consulta3_2023-06-15, onde o erro de autenticação está ocorrendo especificamente no host n321p004938.

Além disso, em relação aos erros relatados para o Elasticsearch, o seguinte é interessante:

{ ""message"": """"""failed to get stack usage from Elasticsearch: error making http request: Get ""[+https://n321p004890.fast.prevnet:9200/_xpack/usage+|https://n321p004890.fast.prevnet:9200/_xpack/usage]"": net/http: request canceled (Client.Timeout exceeded while awaiting headers)"""""", ""lastSeen"": ""2023-06-15T18:01:35.230Z"" }

pois ocorreu apenas no relatório consulta3_2023-06-15.

Resumindo, esses problemas não devem estar explicitamente relacionados aos dados ausentes nesse intervalo de tempo. Perguntamos ao cliente se eles atualizaram recentemente a configuração do Metricbeat? Eu também sugeriria novamente configurar explicitamente as credenciais de autenticação para o módulo do Logstash e qualquer outro módulo configurado no Metricbeat, e verificar novamente a saúde atual do monitoramento para ver se isso resolve os problemas de autenticação.","23/jun/23 12:01 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

De acordo com o que foi falado pela Elastic, pediram para que fosse testado configurar explicitamente as credenciais de autenticação para o módulo do Logstash e qualquer outro módulo configurado no Metricbeat, e verificar novamente a saúde atual do monitoramento para ver se isso resolve os problemas de autenticação.

Aguardo o retorno, para que se o comportamento se manter mesmo assim, que eu possa informá-los e buscarmos outra possível solução.

Atenciosamente,

Paulo Henrique","23/jun/23 1:56 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;nao mistura as coisas ae …  nao tem nada haver com esse chamado aqui.  la o q vimos foi o erro de auth .. e nao isso aqui,.","23/jun/23 1:58 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Essa resposta foi sim referente a este chamado, de problemas em relação a visualização ao Monitoring não estar aparecendo dados quando escolhe o período de 1 hora.

Atenciosamente,

Paulo Henrique","23/jun/23 2:06 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;nao nao foi

","23/jun/23 2:43 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

O que foi discutido mais cedo é referente ao chamado 17 aberto por você aqui no Jira e na Elastic como: LDAP Authentication

Esse daqui é referente a solução definitiva do chamado 10 aberto por você a respeito do Monitoring do Logstash no Kibana, e que foi aberto na Elastic como Monitoring visualization not working properly, em que após eu enviar os dados da consulta que foi requisitada, eles nos enviaram a resposta que enviei anteriormente. A imagem apresenta o chamado no qual estou me referindo aqui dentro.

!chamado-monitoring.png|width=1430,height=689!

Espero que tenha ficado mais claro.

Atenciosamente,

Paulo Henrique","24/jun/23 10:56 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;sim … e mesmo assim eu to afirmando q nao foi recebido.

o erro 401 era em UM dos coletores … (e era do elasticsearch)

o comportamento persiste para todos inclusive.","27/jun/23 7:24 PM;61e09d6968926d0068d83362;Prezado Elias Mussi, boa noite.

Em contato com a Elastic, conseguimos uma reunião com um engenheiro para amanhã as 16hrs, para que possamos apresentar o erro de uma maneira mais clara e que a resolução ocorra de forma mais agilizada. Ressalto que o engenheiro para essa call é americano mas teremos uma equipe para apoiar e auxiliar no que for preciso na questão de entendimentos. Gostaria de saber se pode esse horário amanhã.



Atenciosamente,

Paulo Henrique","27/jun/23 10:50 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;rola as 14hs ou 15hs ? as 16hs eu tenho outra agenda.

","27/jun/23 11:16 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

O engenheiro informou que o horário dele começa às 16hrs daqui, pois o fuso horário é muito diferente. Sendo assim, caso não possa as 16hrs, temos que ver um horário depois, não da pra ser antes. Às 17hrs funciona?",28/jun/23 8:10 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;crava as 17hs entao,"28/jun/23 10:46 AM;61e09d6968926d0068d83362;Prezado Elias Mussi, bom dia.

Conversei aqui internamente e com a Elastic, devido algumas situações, será melhor marcamos para amanhã no mesmo horário, às 17 horas para que todos os envolvidos possam estar participando. Esse horário amanhã serve pra você?



Atenciosamente,

Paulo Henrique","28/jun/23 11:28 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;amanha  pode ser as 16hs

n tem problema apra mim ","28/jun/23 11:28 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;ou as 17 sem problemas

","28/jun/23 11:57 AM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Marcado para amanhã, 29/06, as 17hrs.

Atenciosamente,

Paulo Henrique","29/jun/23 4:54 PM;61e09d6968926d0068d83362;Segue o link para a reunião de 17:00:

[https://elastic.zoom.us/j/97716250905?pwd=UUthNEMrNkp6c3pFVFhmbjIwZ29VQT09|https://elastic.zoom.us/j/97716250905?pwd=UUthNEMrNkp6c3pFVFhmbjIwZ29VQT09|smart-link] 

Atenciosamente,

Paulo Henrique","30/jun/23 2:25 PM;61e09d6968926d0068d83362;Prezados, boa tarde!

Ontem, dia 29/06, tivemos uma reunião junto ao time de engenheiros da Elastic a fim de encontrar a solução para o problema desse chamado. Após diversos testes, foi constatado que a visualização se apresenta de forma correta quando o período de tempo definido nos arquivos yaml do metricbeat é de 10s, onde antes estava o “default” que era de 30s, o que estava impedindo os dados chegarem para a visualização correta no Kibana quando o tempo era menor que 25 minutos. Sendo assim, com a solução para este chamado ter sido encontrada, solicitamos o seu fechamento.

Atenciosamente,

Paulo Henrique",30/jun/23 3:55 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;isso ae. fecha esse … menos um 🙂 ,Itens concluídos
kibana monitoring logstash,DATAPREV-10,10913,Suporte técnico remoto: Incidente,Concluído(a),DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Baixa,,Paulo Henrique Morato Góes,61e09d6968926d0068d83362,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,17/mai/23 10:07 AM,17/mai/23 2:59 PM,19/set/23 4:16 PM,,,0,"versao 8.7.0

nao aparece dados para tempos menor que 2hs

na aba nodes tb nao faz o “short” corretamente dos valores",,Paulo Henrique Morato Góes,,,,,61e09d6968926d0068d83362,,,,,,,,,,,,,17/mai/23 10:06 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;logstash_filter_1h.png;https://bktech.atlassian.net/rest/api/3/attachment/content/10988,17/mai/23 10:06 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;logstash_filter_2h.png;https://bktech.atlassian.net/rest/api/3/attachment/content/10989,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i005k3:,,português,ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb(ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb),ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77(ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77),fabio(fabio),ug:0e2daf0a-9fff-4018-89ac-200a262b74f9(ug:0e2daf0a-9fff-4018-89ac-200a262b74f9),,,,,,,,,,,47:17,,,,,,,,2023-05-17 13:47:50.202,,"17/mai/23 10:07 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;!logstash_filter_1h.png|thumbnail!

!logstash_filter_2h.png|thumbnail!","17/mai/23 10:47 AM;61e09d6968926d0068d83362;Prezado Elias Mussi, bom dia!

Conforme conversado em reunião realizada as 09hrs do dia 17/05/2023, a situação apresentada aparenta ser um bug da ferramenta que será submetido junto as evidências, para a engenharia da Elastic a fim de solicitar que em uma futura versão o bug venha a ser solucionado. Contudo, deixamos registrado que é apenas um bug de tela que não afeta diretamente o funcionamento da ferramenta, sendo assim, enquanto essa situação está presente, recomendamos utilizar com o período de 2hrs que aparece as informações corretamente.

Sendo assim, solicitamos o fechamento do presente chamado, mas informamos que manteremos o time da Dataprev atualizado a qualquer comunicado da Elastic a respeito desse bug em um novo chamado que será aberto a fim de buscarmos a resolução do problema de forma definitiva.



Atenciosamente,

Paulo Henrique","17/mai/23 2:57 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;blz !!!

bora esperar os cara",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Itens concluídos
kibana - search fields names,DATAPREV-9,10912,Suporte técnico remoto: Incidente,Concluído(a),DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Baixa,,Paulo Henrique Morato Góes,61e09d6968926d0068d83362,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,17/mai/23 8:38 AM,17/mai/23 2:58 PM,19/set/23 4:16 PM,,,0,"ta rolando uma parada estranha com a busca de fields no discovery do kibana …

apesar dos campos estarem legais, ele n filtra.

vide a figura

",,Ana Karolina Moreira Viana Catta Preta,Paulo Henrique Morato Góes,,,,62a7957ba80881006f6130ed,61e09d6968926d0068d83362,,,,,,,,,,,,17/mai/23 8:37 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;MicrosoftTeams-image.png;https://bktech.atlassian.net/rest/api/3/attachment/content/10987,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i005jv:,,português,ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb(ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb),ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77(ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77),fabio(fabio),ug:0e2daf0a-9fff-4018-89ac-200a262b74f9(ug:0e2daf0a-9fff-4018-89ac-200a262b74f9),,,,,,,,,,,47:43,,,,,,,,2023-05-17 11:53:58.725,,17/mai/23 8:38 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;!MicrosoftTeams-image.png|thumbnail!,"17/mai/23 8:53 AM;62a7957ba80881006f6130ed;Prezado Mussi, bom dia!



Visando analisar melhor a situação informada, propomos uma call hoje as 09hrs, para tratar do assunto e poder melhor orienta-los quanto a solução.



Desta forma, encaminharei um convite através do e-mail com a sala virtual para nossa conversa.



Desde já, nos colocamos à disposição!","17/mai/23 10:13 AM;61e09d6968926d0068d83362;Prezado Elias Mussi, bom dia!

Após reunião realizada as 09hrs do dia 17/05/2023, descobrimos que o erro acontecia pelo conflito de campos no mapping do índice. Dessa forma, após as modificações feitas para retirar o conflito entre os campos, o erro na visualização foi corrigido e os campos aparecem corretamente.

Sendo assim, com a situação esclarecida e resolvida, solicitamos permissão para o fechamento do chamado.



Atenciosamente,

Paulo Henrique",17/mai/23 2:58 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;fecha ae,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Itens concluídos
filebeat dando erro e nao mandando eventos.,DATAPREV-8,10911,Suporte técnico remoto: Incidente,Concluído(a),DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Normal,,Vinícius Prysthon,621538d1347c690072f6b56f,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,16/mai/23 8:18 PM,17/mai/23 3:02 PM,19/set/23 4:16 PM,,,0,"Boas, 



to vendo um problema aqui q me deixa encafufado.

na maquina q cito abaixo … ela envia logs de sistema (module system), mas da erro nos inputs, que nao enviam nada





n221h025318:/etc/filebeat:cat /var/log/filebeat/filebeat-20230516-2.ndjson | grep erro
{""log.level"":""error"",""@timestamp"":""2023-05-16T20:05:03.613-0300"",""log.logger"":""input"",""log.origin"":{""[file.name|http://file.name]"":""input-logfile/manager.go"",""file.line"":182},""message"":""filestream input with ID 'rfb-bcnpjsincronizador-aplicacao' already exists, this will lead to data duplication, please use a different ID"",""[service.name|http://service.name]"":""filebeat"",""ecs.version"":""1.6.0""}
{""log.level"":""error"",""@timestamp"":""2023-05-16T20:05:03.613-0300"",""log.logger"":""input"",""log.origin"":{""[file.name|http://file.name]"":""input-logfile/manager.go"",""file.line"":182},""message"":""filestream input with ID 'rfb-bcnpjsincronizador-pm2' already exists, this will lead to data duplication, please use a different ID"",""[service.name|http://service.name]"":""filebeat"",""ecs.version"":""1.6.0""}


n221h025318:/etc/filebeat:rpm -qa | grep filebeat
filebeat-8.7.0-1.x86_64


n221h025318:/etc/filebeat:cat inputs.d/*

* type: filestream
id: rfb-bcnpjsincronizador-aplicacao
ignore_older: 168h
paths:
** /var/log/bcnpj/sincronizador.log
fields_under_root: true
fields:
dtp.cp: ""cpdf""
dtp.ambiente: ""homolog""
dtp.cliente: ""rfb""
dtp.sistema: ""bcnpjsincronizador""
dtp.categoria: ""aplicacao""
dtp.type: ""log4j""
encoding: ""ISO-8859-1""
parsers:
** ndjson:
  target: dtp_app
  keys_under_root: true
  expand_keys: true
  message_key: message
* type: filestream
id: rfb-bcnpjsincronizador-pm2
ignore_older: 168h
paths:
** /var/log/bcnpj/pm2_sincronizador.log
fields_under_root: true
fields:
dtp.cp: ""cpdf""
dtp.ambiente: ""homolog""
dtp.cliente: ""rfb""
dtp.sistema: ""bcnpjsincronizador""
dtp.categoria: ""pm2""
dtp.type: ""logsemtratamento""

n221h025318:/etc/filebeat:cat filebeat.yml | grep -v ^#

filebeat.config:
  modules:
    path: /etc/filebeat/modules.d/_.yml_
    _reload.enabled: false_
  _inputs:_
    _enabled: true_
    _path: /etc/filebeat/inputs.d/_.yml
    reload.enabled: false

output.redis:
  hosts: [""hredisdf.elk.prevnet:6379""]
  key: ""%{[dtp.cliente]:cliente}-%{[dtp.type]:default_list}""

…..



ta muito estranho essa parada … os IDsdos filestream sao diferentes … pq esse erro ?

(esse aqui merece ser um normal em … nao to conseguindo eviar log 🙂 )",,Ana Karolina Moreira Viana Catta Preta,Vinícius Prysthon,,,,62a7957ba80881006f6130ed,621538d1347c690072f6b56f,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i005jn:,,português,ug:0e9c7acd-4c78-4ef6-beb2-b00f34bddd59(ug:0e9c7acd-4c78-4ef6-beb2-b00f34bddd59),,,,,,,,,,,,,,18:25,,,,,,,,2023-05-17 02:31:37.367,,"16/mai/23 11:31 PM;621538d1347c690072f6b56f;Prezado Elias Mussi, boa noite!



Ao que parece, o problema está relacionado a um “cache” que está armazenando esse input ID de um start anterior do filebeat. Dito isso, sugiro limpar a pasta que armazena esses registros, onde o comando seria algo semelhante a:



{noformat}rm -fr /var/run/filebeat/*{noformat}



É nesse caminho que geralmente os registros ficam armazenados, então limpando eles o problema pode ser resolvido. Vale ressaltar que após essa ação o serviço do filebeat deve ser reiniciado.



Aguardo seu retorno.

Atenciosamente,

Vinícius Prysthon","17/mai/23 6:37 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Boas,



nao existe o /var/run/filebeat 😞 

eu tentei apagando o /var/lib/filebeat e necas

e mesmo mudando o ID ele da o mesmo erro","17/mai/23 6:39 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;[https://github.com/elastic/beats/pull/35134|https://github.com/elastic/beats/pull/35134|smart-link]

[https://github.com/elastic/beats/issues/31767|https://github.com/elastic/beats/issues/31767|smart-link]","17/mai/23 8:53 AM;62a7957ba80881006f6130ed;Prezado Mussi, bom dia!



Visando analisar melhor a situação informada, propomos uma call hoje as 09hrs, para tratar do assunto e poder melhor orienta-los quanto a solução.



Desta forma, encaminharei um convite através do e-mail com a sala virtual para nossa conversa.



Desde já, nos colocamos à disposição!","17/mai/23 11:01 AM;621538d1347c690072f6b56f;Prezado Elias Mussi, bom dia!

Conforme conversado em reunião realizada as 09hrs do dia 17/05/2023, a situação apresentada aparenta ser um bug da ferramenta que será submetido junto as evidências, para a engenharia da Elastic a fim de solicitar que em uma futura versão o bug venha a ser solucionado. Para fins de registro, o arquivo ‘inputs.d’ foi inserido no arquivo ‘filebeat.yml’, o que, por ora, solucionou o problema do envio de eventos. 

Sendo assim, solicitamos o fechamento do presente chamado, mas informamos que manteremos o time da Dataprev atualizado a qualquer comunicado da Elastic a respeito desse bug em um novo chamado que será aberto a fim de buscarmos a resolução do problema de forma definitiva.

 

Atenciosamente,

Vinícius Prysthon","17/mai/23 2:57 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;blz !

aperta os cara",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Itens concluídos
Perda de logs enviados pelo WAF Imperva para o ELK,DATAPREV-7,10907,Suporte técnico remoto: Incidente,Concluído(a),DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Alta,,Paulo Henrique Morato Góes,61e09d6968926d0068d83362,Paulo Jannuzzi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:351491f4-b45d-4144-955d-02d26a10137b,Paulo Jannuzzi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:351491f4-b45d-4144-955d-02d26a10137b,11/mai/23 11:40 AM,04/jul/23 9:11 AM,19/set/23 4:16 PM,,,0,"Bom dia.

Verificamos uma perda de logs no ELK recebidos pelo WAF.  Isso ocorreu de forma mais visível a partir do dia 9/05/2023. Fizemos captura de tráfego na interface do WAF e identificamos o envio correto pelo WAF. Porém este log não foi encontrado no ELK..

Solicitamos suporte para correção do problema. 







 

",,Paulo Henrique Morato Góes,,,,,61e09d6968926d0068d83362,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i005ir:,,português,ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb(ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb),ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77(ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77),fabio(fabio),ug:0e2daf0a-9fff-4018-89ac-200a262b74f9(ug:0e2daf0a-9fff-4018-89ac-200a262b74f9),,,,,,,,,,,3:48,,,,,,,,2023-05-11 14:49:46.841,,"11/mai/23 11:49 AM;61e09d6968926d0068d83362;Prezado Paulo Jannuzzi, bom dia.



Solicitamos uma call para entendermos com mais detalhes o que está ocorrendo. Qual o horário que vocês da Dataprev tem disponibilidade?

Aguardo retorno.","11/mai/23 12:17 PM;61e09d6968926d0068d83362;Prezado Paulo Jannuzzi,

Notamos que o chamado foi aberto como Severidade 1, porém de acordo com o que prevê o TR e o relato do ocorrido, entendemos que a severidade correta para o chamado seria a Severidade 2. Segue abaixo o que consta no TR:

_a) Severidade 1 – quando ocorre a perda ou paralisação de atividades exercidas ou de_
_serviços relevantes prestados pela DATAPREV, configurando-se como emergência._
_Uma solicitação de serviço de Severidade 1 pode possuir uma ou mais das seguintes_
_características_
_▪ Dados corrompidos;_
_▪ Uma função crítica não está disponível;_
_▪ O sistema se desliga repentinamente causando demoras excessivas e_
_intermitências para utilização de recursos;_
_▪ O sistema falha repetidamente após tentativas de reinicialização;_
_▪ O sistema continua em execução permanente (congelado) necessitando ser_
_reiniciado._
_b) Severidade 2 – quando se verifica uma grave perda de funcionalidade em_
_programas ou sistemas da DATAPREV, sem, no entanto, interromper em sua_
_totalidade a prestação do serviço._
_c) Severidade 3 – quando se verifica uma perda de menor relevância de_
_funcionalidades em programas ou sistemas, causando apenas inconveniências para_
_a realização de atividades exercidas ou pela devida prestação dos serviços pela_
_DATAPREV._
_d) Severidade 4 – quando se verifica como necessária a prestação de informações,_
_aperfeiçoamentos ou esclarecimentos sobre documentação ou funcionalidades,_
_porém sem prejudicar diretamente a operação dos programas ou sistemas da_
_DATAPREV._



Sendo assim, solicitamos permissão para alterar para a severidade correta dada a natureza do chamado, já que não há *corrompimento dos dados*, apenas *perda de alguns logs* que não interrompem com totalidade a prestação do serviço.



Atenciosamente,

Paulo Henrique",04/jul/23 9:09 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:351491f4-b45d-4144-955d-02d26a10137b;O chamado pode ser fechado.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Itens concluídos
"Atlassian test, please ignore!",DATAPREV-6,10580,Suporte técnico remoto: Incidente,cancelado,DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Baixa,,Bruno Altenhofen,60917e34eebe78006a4d9f3a,Bruno Altenhofen,60917e34eebe78006a4d9f3a,Bruno Altenhofen,60917e34eebe78006a4d9f3a,30/mar/23 10:28 AM,19/jun/23 4:04 PM,19/set/23 4:16 PM,,,0,test,,Bruno Altenhofen,,,,,60917e34eebe78006a4d9f3a,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i003i3:,,português,,,,,,,,,,,,,,,47:55,,,,,,,,,,30/mar/23 10:30 AM;60917e34eebe78006a4d9f3a;test,30/mar/23 10:32 AM;60917e34eebe78006a4d9f3a;test,30/mar/23 10:33 AM;60917e34eebe78006a4d9f3a;test,30/mar/23 10:34 AM;60917e34eebe78006a4d9f3a;test,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Itens concluídos
erro em criar vizualizacoes no kibana,DATAPREV-5,10579,Suporte técnico remoto: esclarecimento de dúvidas,Concluído(a),DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Baixa,,Paulo Henrique Morato Góes,61e09d6968926d0068d83362,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,28/mar/23 12:09 PM,16/jun/23 2:26 PM,19/set/23 4:16 PM,,,0,"turma .. começou a aaparecer esse erro aqui em um space do kibana, qdo se tenta criar uma viz (qdo se tenta criar um tsvb nao aparece nada) : 



Error

Error: Cannot read properties of undefined (reading 'aggs')
    at [https://hlogs.elk.prevnet/58934/bundles/plugin/visDefaultEditor/kibana/visDefaultEditor.chunk.1.js:1:134564|https://hlogs.elk.prevnet/58934/bundles/plugin/visDefaultEditor/kibana/visDefaultEditor.chunk.1.js:1:134564]
    at Object.useMemo ([https://hlogs.elk.prevnet/58934/bundles/kbn-ui-shared-deps-npm/kbn-ui-shared-deps-npm.dll.js:398:65133)|https://hlogs.elk.prevnet/58934/bundles/kbn-ui-shared-deps-npm/kbn-ui-shared-deps-npm.dll.js:398:65133)]
    at t.useMemo ([https://hlogs.elk.prevnet/58934/bundles/kbn-ui-shared-deps-npm/kbn-ui-shared-deps-npm.dll.js:374:5744)|https://hlogs.elk.prevnet/58934/bundles/kbn-ui-shared-deps-npm/kbn-ui-shared-deps-npm.dll.js:374:5744)]
    at _t ([https://hlogs.elk.prevnet/58934/bundles/plugin/visDefaultEditor/kibana/visDefaultEditor.chunk.1.js:1:134546)|https://hlogs.elk.prevnet/58934/bundles/plugin/visDefaultEditor/kibana/visDefaultEditor.chunk.1.js:1:134546)]
    at ca ([https://hlogs.elk.prevnet/58934/bundles/kbn-ui-shared-deps-npm/kbn-ui-shared-deps-npm.dll.js:398:59210)|https://hlogs.elk.prevnet/58934/bundles/kbn-ui-shared-deps-npm/kbn-ui-shared-deps-npm.dll.js:398:59210)]
    at Js ([https://hlogs.elk.prevnet/58934/bundles/kbn-ui-shared-deps-npm/kbn-ui-shared-deps-npm.dll.js:398:115077)|https://hlogs.elk.prevnet/58934/bundles/kbn-ui-shared-deps-npm/kbn-ui-shared-deps-npm.dll.js:398:115077)]
    at Fu ([https://hlogs.elk.prevnet/58934/bundles/kbn-ui-shared-deps-npm/kbn-ui-shared-deps-npm.dll.js:398:99860)|https://hlogs.elk.prevnet/58934/bundles/kbn-ui-shared-deps-npm/kbn-ui-shared-deps-npm.dll.js:398:99860)]
    at Pu ([https://hlogs.elk.prevnet/58934/bundles/kbn-ui-shared-deps-npm/kbn-ui-shared-deps-npm.dll.js:398:99682)|https://hlogs.elk.prevnet/58934/bundles/kbn-ui-shared-deps-npm/kbn-ui-shared-deps-npm.dll.js:398:99682)]
    at Ru ([https://hlogs.elk.prevnet/58934/bundles/kbn-ui-shared-deps-npm/kbn-ui-shared-deps-npm.dll.js:398:99515)|https://hlogs.elk.prevnet/58934/bundles/kbn-ui-shared-deps-npm/kbn-ui-shared-deps-npm.dll.js:398:99515)]
    at xu ([https://hlogs.elk.prevnet/58934/bundles/kbn-ui-shared-deps-npm/kbn-ui-shared-deps-npm.dll.js:398:96348)|https://hlogs.elk.prevnet/58934/bundles/kbn-ui-shared-deps-npm/kbn-ui-shared-deps-npm.dll.js:398:96348)]

ja passaram por isso ?",,Paulo Henrique Morato Góes,,,,,61e09d6968926d0068d83362,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i003hv:,,português,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:cece5bfc-fb79-4ccb-84f9-7ce16f24643b(qm:c4560720-f28e-48ad-8ca6-02074b751b6e:cece5bfc-fb79-4ccb-84f9-7ce16f24643b),,,,,,,,,,,,,,45:38,,,,,,,,2023-03-28 15:44:38.875,,"28/mar/23 12:44 PM;61e09d6968926d0068d83362;Prezado Elias Mussi, boa tarde!

Por estar acontecendo em um Space específico, isso pode estar acontecendo por estar faltado alguma permissão correta para isso. Você pode enivar as roles definidas para o usuário nesse space que está tendo este problema?



Aguardo retorno!","28/mar/23 1:05 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;ae q ta a treta …   ele criou … e agora nao rola mais .. e nada foi mudado :/



priviledio de indices : read e view-index-metadata

no kibana o discovery ta com tudo all","28/mar/23 1:23 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Entendi, isso acontece na criação de qualquer visualização, independente do índice ou somente em algum específico?

Grato!","28/mar/23 1:27 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;acontece om o tsvb e os “agragation based” … lens parece q vai.  o resto n testei.

e acontece mesmo qdo se cria um dash novo do zero tb 

dentro do mesmo space","28/mar/23 1:50 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Estamos investigando aqui o que pode estar ocorrendo para tal erro estar acontecendo. 

No entanto reparei que o chamado foi aberto como: 

“Severidade 2 - quando se verifica uma grave perda de funcionalidade em programas ou sistemas da DATAPREV, sem, no entanto, interromper em sua totalidade a prestação do serviço.”

Acredito no entanto, que pelo relatado, acontecer em um único Space e índices específicos com a ferramenta de visuaização, a severidade correta seria:

Severidade 3 – quando se verifica uma perda de menor relevância de funcionalidades em programas ou sistemas, causando apenas inconveniências para a realização de atividades exercidas ou pela devida prestação dos serviços pela DATAPREV.

Caso esteja de acordo, me confirme para que eu possa alterar a severidade aqui.","28/mar/23 1:57 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;blz muda ae … tinha aberto no normal

esse nomes ali me bagunçam hehehe","28/mar/23 3:12 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;achei a treta aqui … mas foi no arco da velha .. debug dentro do navegador para ver q uma referencia a um dataview estava quebrada.

uma brincada com objetos salvos e pronto .. parece q tudo voltou a funcionar



o foda é q o comportamento nao é mais como no 6. q te levava para poder escolher um dataview diferente :/

fecha ae :)","28/mar/23 3:14 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Entendi! É realmente algumas questões comportamentais do Elastic mudaram desde a versão 6, que bom que conseguiu encontrar o problema que estava causando o erro.

Sendo assim fecharei o chamado, mas seguimos a disposição para qualquer questão que possa aparecer.

Atenciosamente,

Paulo",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Itens concluídos
Can't get text on a START_OBJECT,DATAPREV-4,10578,Suporte técnico remoto: esclarecimento de dúvidas,Concluído(a),DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Muito Baixa,,Paulo Henrique Morato Góes,61e09d6968926d0068d83362,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,27/mar/23 8:28 AM,16/jun/23 2:26 PM,19/set/23 4:16 PM,,,0,"Turminha .. alguem sabe a causa desse motivo de erro no processamento de pipeline no logstash ?



{{Could not index event to Elasticsearch. status: 400, action: [""create"", {:_id=>nil, :_index=>""logs-app-pmf-log4j-8.6.1"", :routing=>nil}, {""ecs""=>{""version""=>""8.0.0""}, ""agent""=>{""version""=>""8.6.1"", ""ephemeral_id""=>""8727cfab-77c5-4a27-a7b2-91728dbdb508"", ""name""=>""v151h193"", ""id""=>""d6aa5d07-890c-410f-b178-5544e3d3e40c"", ""type""=>""filebeat""}, ""message""=>""Access is denied (user is not anonymous); delegating to AccessDeniedHandler"", ""event""=>{}, ""@timestamp""=>2023-03-27T10:43:12.954Z, ""host""=>{""name""=>""v151h193""}, ""dtp.redis_key""=>""pmf-log4j"", ""dtp_app""=>{""thrown""=>{""localizedMessage""=>""Access is denied"", ""extendedStackTrace""=>[{""file""=>""AffirmativeBased.java"", ""exact""=>false, ""location""=>""spring-security-core-5.3.4.RELEASE.jar!/"", ""method""=>""decide"", ""class""=>""org.springframework.security.access.vote.AffirmativeBased"", ""line""=>84, ""version""=>""5.3.4.RELEASE""}, {""file""=>""AbstractSecurityInterceptor.java"", ""exact""=>false, ""location""=>""spring-security-core-5.3.4.RELEASE.jar!/"", ""method""=>""beforeInvocation"", ""line""=>233, ""class""=>""org.springframework.security.access.intercept.AbstractSecurityInterceptor"", ""version""=>""5.3.4.RELEASE""}, {""file""=>""FilterSecurityInterceptor.java"", ""exact""=>false, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""invoke"", ""line""=>123, ""class""=>""org.springframework.security.web.access.intercept.FilterSecurityInterceptor"", ""version""=>""5.3.4.RELEASE""}, {""file""=>""FilterSecurityInterceptor.java"", ""exact""=>false, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilter"", ""line""=>90, ""class""=>""org.springframework.security.web.access.intercept.FilterSecurityInterceptor"", ""version""=>""5.3.4.RELEASE""}, {""file""=>""FilterChainProxy.java"", ""exact""=>false, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilter"", ""class""=>""org.springframework.security.web.FilterChainProxy$VirtualFilterChain"", ""line""=>334, ""version""=>""5.3.4.RELEASE""}, {""file""=>""ExceptionTranslationFilter.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilter"", ""line""=>118, ""class""=>""org.springframework.security.web.access.ExceptionTranslationFilter"", ""version""=>""5.3.4.RELEASE""}, {""file""=>""FilterChainProxy.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilter"", ""class""=>""org.springframework.security.web.FilterChainProxy$VirtualFilterChain"", ""line""=>334, ""version""=>""5.3.4.RELEASE""}, {""file""=>""SessionManagementFilter.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilter"", ""class""=>""org.springframework.security.web.session.SessionManagementFilter"", ""line""=>137, ""version""=>""5.3.4.RELEASE""}, {""file""=>""FilterChainProxy.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilter"", ""line""=>334, ""class""=>""org.springframework.security.web.FilterChainProxy$VirtualFilterChain"", ""version""=>""5.3.4.RELEASE""}, {""file""=>""AnonymousAuthenticationFilter.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilter"", ""class""=>""org.springframework.security.web.authentication.AnonymousAuthenticationFilter"", ""line""=>111, ""version""=>""5.3.4.RELEASE""}, {""file""=>""FilterChainProxy.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilter"", ""line""=>334, ""class""=>""org.springframework.security.web.FilterChainProxy$VirtualFilterChain"", ""version""=>""5.3.4.RELEASE""}, {""file""=>""SecurityContextHolderAwareRequestFilter.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilter"", ""class""=>""org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter"", ""line""=>158, ""version""=>""5.3.4.RELEASE""}, {""file""=>""FilterChainProxy.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilter"", ""class""=>""org.springframework.security.web.FilterChainProxy$VirtualFilterChain"", ""line""=>334, ""version""=>""5.3.4.RELEASE""}, {""file""=>""RequestCacheAwareFilter.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilter"", ""class""=>""org.springframework.security.web.savedrequest.RequestCacheAwareFilter"", ""line""=>63, ""version""=>""5.3.4.RELEASE""}, {""file""=>""FilterChainProxy.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilter"", ""line""=>334, ""class""=>""org.springframework.security.web.FilterChainProxy$VirtualFilterChain"", ""version""=>""5.3.4.RELEASE""}, {""file""=>""BasicAuthenticationFilter.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilterInternal"", ""class""=>""org.springframework.security.web.authentication.www.BasicAuthenticationFilter"", ""line""=>204, ""version""=>""5.3.4.RELEASE""}, {""file""=>""OncePerRequestFilter.java"", ""exact""=>true, ""location""=>""spring-web-5.2.8.RELEASE.jar!/"", ""method""=>""doFilter"", ""class""=>""org.springframework.web.filter.OncePerRequestFilter"", ""line""=>119, ""version""=>""5.2.8.RELEASE""}, {""file""=>""FilterChainProxy.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilter"", ""line""=>334, ""class""=>""org.springframework.security.web.FilterChainProxy$VirtualFilterChain"", ""version""=>""5.3.4.RELEASE""}, {""file""=>""LogoutFilter.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilter"", ""line""=>116, ""class""=>""org.springframework.security.web.authentication.logout.LogoutFilter"", ""version""=>""5.3.4.RELEASE""}, {""file""=>""FilterChainProxy.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilter"", ""class""=>""org.springframework.security.web.FilterChainProxy$VirtualFilterChain"", ""line""=>334, ""version""=>""5.3.4.RELEASE""}, {""file""=>""HeaderWriterFilter.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doHeadersAfter"", ""line""=>92, ""class""=>""org.springframework.security.web.header.HeaderWriterFilter"", ""version""=>""5.3.4.RELEASE""}, {""file""=>""HeaderWriterFilter.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilterInternal"", ""line""=>77, ""class""=>""org.springframework.security.web.header.HeaderWriterFilter"", ""version""=>""5.3.4.RELEASE""}, {""file""=>""OncePerRequestFilter.java"", ""exact""=>true, ""location""=>""spring-web-5.2.8.RELEASE.jar!/"", ""method""=>""doFilter"", ""class""=>""org.springframework.web.filter.OncePerRequestFilter"", ""line""=>119, ""version""=>""5.2.8.RELEASE""}, {""file""=>""FilterChainProxy.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilter"", ""class""=>""org.springframework.security.web.FilterChainProxy$VirtualFilterChain"", ""line""=>334, ""version""=>""5.3.4.RELEASE""}, {""file""=>""SecurityContextPersistenceFilter.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilter"", ""class""=>""org.springframework.security.web.context.SecurityContextPersistenceFilter"", ""line""=>105, ""version""=>""5.3.4.RELEASE""}, {""file""=>""FilterChainProxy.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilter"", ""class""=>""org.springframework.security.web.FilterChainProxy$VirtualFilterChain"", ""line""=>334, ""version""=>""5.3.4.RELEASE""}, {""file""=>""WebAsyncManagerIntegrationFilter.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilterInternal"", ""class""=>""org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter"", ""line""=>56, ""version""=>""5.3.4.RELEASE""}, {""file""=>""OncePerRequestFilter.java"", ""exact""=>true, ""location""=>""spring-web-5.2.8.RELEASE.jar!/"", ""method""=>""doFilter"", ""line""=>119, ""class""=>""org.springframework.web.filter.OncePerRequestFilter"", ""version""=>""5.2.8.RELEASE""}, {""file""=>""FilterChainProxy.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilter"", ""line""=>334, ""class""=>""org.springframework.security.web.FilterChainProxy$VirtualFilterChain"", ""version""=>""5.3.4.RELEASE""}, {""file""=>""FilterChainProxy.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilterInternal"", ""class""=>""org.springframework.security.web.FilterChainProxy"", ""line""=>215, ""version""=>""5.3.4.RELEASE""}, {""file""=>""FilterChainProxy.java"", ""exact""=>true, ""location""=>""spring-security-web-5.3.4.RELEASE.jar!/"", ""method""=>""doFilter"", ""class""=>""org.springframework.security.web.FilterChainProxy"", ""line""=>178, ""version""=>""5.3.4.RELEASE""}, {""file""=>""DelegatingFilterProxy.java"", ""exact""=>true, ""location""=>""spring-web-5.2.8.RELEASE.jar!/"", ""method""=>""invokeDelegate"", ""class""=>""org.springframework.web.filter.DelegatingFilterProxy"", ""line""=>358, ""version""=>""5.2.8.RELEASE""}, {""file""=>""DelegatingFilterProxy.java"", ""exact""=>true, ""location""=>""spring-web-5.2.8.RELEASE.jar!/"", ""method""=>""doFilter"", ""class""=>""org.springframework.web.filter.DelegatingFilterProxy"", ""line""=>271, ""version""=>""5.2.8.RELEASE""}, {""file""=>""ApplicationFilterChain.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""internalDoFilter"", ""class""=>""org.apache.catalina.core.ApplicationFilterChain"", ""line""=>193, ""version""=>""9.0.37""}, {""file""=>""ApplicationFilterChain.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""doFilter"", ""class""=>""org.apache.catalina.core.ApplicationFilterChain"", ""line""=>166, ""version""=>""9.0.37""}, {""file""=>""RequestContextFilter.java"", ""exact""=>true, ""location""=>""spring-web-5.2.8.RELEASE.jar!/"", ""method""=>""doFilterInternal"", ""class""=>""org.springframework.web.filter.RequestContextFilter"", ""line""=>100, ""version""=>""5.2.8.RELEASE""}, {""file""=>""OncePerRequestFilter.java"", ""exact""=>true, ""location""=>""spring-web-5.2.8.RELEASE.jar!/"", ""method""=>""doFilter"", ""class""=>""org.springframework.web.filter.OncePerRequestFilter"", ""line""=>119, ""version""=>""5.2.8.RELEASE""}, {""file""=>""ApplicationFilterChain.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""internalDoFilter"", ""line""=>193, ""class""=>""org.apache.catalina.core.ApplicationFilterChain"", ""version""=>""9.0.37""}, {""file""=>""ApplicationFilterChain.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""doFilter"", ""line""=>166, ""class""=>""org.apache.catalina.core.ApplicationFilterChain"", ""version""=>""9.0.37""}, {""file""=>""FormContentFilter.java"", ""exact""=>true, ""location""=>""spring-web-5.2.8.RELEASE.jar!/"", ""method""=>""doFilterInternal"", ""class""=>""org.springframework.web.filter.FormContentFilter"", ""line""=>93, ""version""=>""5.2.8.RELEASE""}, {""file""=>""OncePerRequestFilter.java"", ""exact""=>true, ""location""=>""spring-web-5.2.8.RELEASE.jar!/"", ""method""=>""doFilter"", ""class""=>""org.springframework.web.filter.OncePerRequestFilter"", ""line""=>119, ""version""=>""5.2.8.RELEASE""}, {""file""=>""ApplicationFilterChain.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""internalDoFilter"", ""class""=>""org.apache.catalina.core.ApplicationFilterChain"", ""line""=>193, ""version""=>""9.0.37""}, {""file""=>""ApplicationFilterChain.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""doFilter"", ""class""=>""org.apache.catalina.core.ApplicationFilterChain"", ""line""=>166, ""version""=>""9.0.37""}, {""file""=>""CharacterEncodingFilter.java"", ""exact""=>true, ""location""=>""spring-web-5.2.8.RELEASE.jar!/"", ""method""=>""doFilterInternal"", ""class""=>""org.springframework.web.filter.CharacterEncodingFilter"", ""line""=>201, ""version""=>""5.2.8.RELEASE""}, {""file""=>""OncePerRequestFilter.java"", ""exact""=>true, ""location""=>""spring-web-5.2.8.RELEASE.jar!/"", ""method""=>""doFilter"", ""line""=>119, ""class""=>""org.springframework.web.filter.OncePerRequestFilter"", ""version""=>""5.2.8.RELEASE""}, {""file""=>""ApplicationFilterChain.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""internalDoFilter"", ""line""=>193, ""class""=>""org.apache.catalina.core.ApplicationFilterChain"", ""version""=>""9.0.37""}, {""file""=>""ApplicationFilterChain.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""doFilter"", ""class""=>""org.apache.catalina.core.ApplicationFilterChain"", ""line""=>166, ""version""=>""9.0.37""}, {""file""=>""StandardWrapperValve.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""invoke"", ""line""=>202, ""class""=>""org.apache.catalina.core.StandardWrapperValve"", ""version""=>""9.0.37""}, {""file""=>""StandardContextValve.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""invoke"", ""class""=>""org.apache.catalina.core.StandardContextValve"", ""line""=>96, ""version""=>""9.0.37""}, {""file""=>""AuthenticatorBase.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""invoke"", ""class""=>""org.apache.catalina.authenticator.AuthenticatorBase"", ""line""=>541, ""version""=>""9.0.37""}, {""file""=>""StandardHostValve.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""invoke"", ""line""=>139, ""class""=>""org.apache.catalina.core.StandardHostValve"", ""version""=>""9.0.37""}, {""file""=>""ErrorReportValve.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""invoke"", ""class""=>""org.apache.catalina.valves.ErrorReportValve"", ""line""=>92, ""version""=>""9.0.37""}, {""file""=>""StandardEngineValve.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""invoke"", ""line""=>74, ""class""=>""org.apache.catalina.core.StandardEngineValve"", ""version""=>""9.0.37""}, {""file""=>""CoyoteAdapter.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""service"", ""line""=>343, ""class""=>""org.apache.catalina.connector.CoyoteAdapter"", ""version""=>""9.0.37""}, {""file""=>""Http11Processor.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""service"", ""class""=>""org.apache.coyote.http11.Http11Processor"", ""line""=>373, ""version""=>""9.0.37""}, {""file""=>""AbstractProcessorLight.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""process"", ""line""=>65, ""class""=>""org.apache.coyote.AbstractProcessorLight"", ""version""=>""9.0.37""}, {""file""=>""AbstractProtocol.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""process"", ""class""=>""org.apache.coyote.AbstractProtocol$ConnectionHandler"", ""line""=>868, ""version""=>""9.0.37""}, {""file""=>""NioEndpoint.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""doRun"", ""class""=>""org.apache.tomcat.util.net.NioEndpoint$SocketProcessor"", ""line""=>1589, ""version""=>""9.0.37""}, {""file""=>""SocketProcessorBase.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""run"", ""line""=>49, ""class""=>""org.apache.tomcat.util.net.SocketProcessorBase"", ""version""=>""9.0.37""}, {""file""=>""ThreadPoolExecutor.java"", ""exact""=>true, ""location""=>""?"", ""method""=>""runWorker"", ""line""=>1149, ""class""=>""java.util.concurrent.ThreadPoolExecutor"", ""version""=>""1.8.0_331""}, {""file""=>""ThreadPoolExecutor.java"", ""exact""=>true, ""location""=>""?"", ""method""=>""run"", ""class""=>""java.util.concurrent.ThreadPoolExecutor$Worker"", ""line""=>624, ""version""=>""1.8.0_331""}, {""file""=>""TaskThread.java"", ""exact""=>true, ""location""=>""tomcat-embed-core-9.0.37.jar!/"", ""method""=>""run"", ""line""=>61, ""class""=>""org.apache.tomcat.util.threads.TaskThread$WrappingRunnable"", ""version""=>""9.0.37""}, {""file""=>""Thread.java"", ""exact""=>true, ""location""=>""?"", ""method""=>""run"", ""line""=>750, ""class""=>""java.lang.Thread"", ""version""=>""1.8.0_331""}], ""message""=>""Access is denied"", ""commonElementCount""=>0, ""name""=>""org.springframework.security.access.AccessDeniedException""}, ""threadId""=>72, ""loggerName""=>""org.springframework.security.web.access.ExceptionTranslationFilter"", ""level""=>""DEBUG"", ""message""=>""Access is denied (user is not anonymous); delegating to AccessDeniedHandler"", ""threadPriority""=>5, ""instant""=>{""epochSecond""=>1679913785, ""nanoOfSecond""=>277000000}, ""endOfBatch""=>false, ""thread""=>""https-jsse-nio-8443-exec-3"", ""loggerFqcn""=>""org.apache.commons.logging.LogAdapter$Log4jLog""}, ""tags""=>[""_grokparsefailure""], ""type""=>""log4j"", ""dtp.logstash_indexer""=>""n321p004904.fast.prevnet"", ""log""=>{""file""=>{""path""=>""/u01/app/pmf/pmf-realizar-pericia-service/log/pmf-realizar-pericias-service.log""}, ""offset""=>1463}, ""data_stream""=>{""dataset""=>""app-pmf"", ""namespace""=>""log4j-8.6.1"", ""type""=>""logs""}, ""dtp.redis_server""=>""n141p000129.fast.prevnet"", ""dtp""=>{""cp""=>""cprj"", ""categoria""=>""RealizarPericiasService"", ""type""=>""log4j"", ""ambiente""=>""homolog"", ""sistema""=>""pmfpericias"", ""cliente""=>""pmf""}, ""input""=>{""type""=>""filestream""}, ""@version""=>""1""}], response: {""create""=>{""_index""=>"".ds-logs-app-pmf-log4j-8.6.1-2023.03.27-000042"", ""_id""=>""vm-qIocBkM4aX-yfNk75"", ""status""=>400, ""error""=>{""type""=>""mapper_parsing_exception"", ""reason""=>""failed to parse field [dtp_app.thrown] of type [keyword] in document with id 'vm-qIocBkM4aX-yfNk75'. Preview of field's value: '{extendedStackTrace=[{file=AffirmativeBased.java, method=decide, line=84, exact=false, location=spring-security-core-5.3.4.RELEASE.jar!/, class=org.springframework.security.access.vote.AffirmativeBased, version=5.3.4.RELEASE}, {file=AbstractSecurityInterceptor.java, method=beforeInvocation, line=233, exact=false, location=spring-security-core-5.3.4.RELEASE.jar!/, class=org.springframework.security.access.intercept.AbstractSecurityInterceptor, version=5.3.4.RELEASE}, {file=FilterSecurityInterceptor.java, method=invoke, line=123, exact=false, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.access.intercept.FilterSecurityInterceptor, version=5.3.4.RELEASE}, {file=FilterSecurityInterceptor.java, method=doFilter, line=90, exact=false, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.access.intercept.FilterSecurityInterceptor, version=5.3.4.RELEASE}, {file=FilterChainProxy.java, method=doFilter, line=334, exact=false, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.FilterChainProxy$VirtualFilterChain, version=5.3.4.RELEASE}, {file=ExceptionTranslationFilter.java, method=doFilter, line=118, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.access.ExceptionTranslationFilter, version=5.3.4.RELEASE}, {file=FilterChainProxy.java, method=doFilter, line=334, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.FilterChainProxy$VirtualFilterChain, version=5.3.4.RELEASE}, {file=SessionManagementFilter.java, method=doFilter, line=137, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.session.SessionManagementFilter, version=5.3.4.RELEASE}, {file=FilterChainProxy.java, method=doFilter, line=334, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.FilterChainProxy$VirtualFilterChain, version=5.3.4.RELEASE}, {file=AnonymousAuthenticationFilter.java, method=doFilter, line=111, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.authentication.AnonymousAuthenticationFilter, version=5.3.4.RELEASE}, {file=FilterChainProxy.java, method=doFilter, line=334, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.FilterChainProxy$VirtualFilterChain, version=5.3.4.RELEASE}, {file=SecurityContextHolderAwareRequestFilter.java, method=doFilter, line=158, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.servletapi.SecurityContextHolderAwareRequestFilter, version=5.3.4.RELEASE}, {file=FilterChainProxy.java, method=doFilter, line=334, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.FilterChainProxy$VirtualFilterChain, version=5.3.4.RELEASE}, {file=RequestCacheAwareFilter.java, method=doFilter, line=63, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.savedrequest.RequestCacheAwareFilter, version=5.3.4.RELEASE}, {file=FilterChainProxy.java, method=doFilter, line=334, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.FilterChainProxy$VirtualFilterChain, version=5.3.4.RELEASE}, {file=BasicAuthenticationFilter.java, method=doFilterInternal, line=204, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.authentication.www.BasicAuthenticationFilter, version=5.3.4.RELEASE}, {file=OncePerRequestFilter.java, method=doFilter, line=119, exact=true, location=spring-web-5.2.8.RELEASE.jar!/, class=org.springframework.web.filter.OncePerRequestFilter, version=5.2.8.RELEASE}, {file=FilterChainProxy.java, method=doFilter, line=334, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.FilterChainProxy$VirtualFilterChain, version=5.3.4.RELEASE}, {file=LogoutFilter.java, method=doFilter, line=116, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.authentication.logout.LogoutFilter, version=5.3.4.RELEASE}, {file=FilterChainProxy.java, method=doFilter, line=334, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.FilterChainProxy$VirtualFilterChain, version=5.3.4.RELEASE}, {file=HeaderWriterFilter.java, method=doHeadersAfter, line=92, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.header.HeaderWriterFilter, version=5.3.4.RELEASE}, {file=HeaderWriterFilter.java, method=doFilterInternal, line=77, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.header.HeaderWriterFilter, version=5.3.4.RELEASE}, {file=OncePerRequestFilter.java, method=doFilter, line=119, exact=true, location=spring-web-5.2.8.RELEASE.jar!/, class=org.springframework.web.filter.OncePerRequestFilter, version=5.2.8.RELEASE}, {file=FilterChainProxy.java, method=doFilter, line=334, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.FilterChainProxy$VirtualFilterChain, version=5.3.4.RELEASE}, {file=SecurityContextPersistenceFilter.java, method=doFilter, line=105, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.context.SecurityContextPersistenceFilter, version=5.3.4.RELEASE}, {file=FilterChainProxy.java, method=doFilter, line=334, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.FilterChainProxy$VirtualFilterChain, version=5.3.4.RELEASE}, {file=WebAsyncManagerIntegrationFilter.java, method=doFilterInternal, line=56, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.context.request.async.WebAsyncManagerIntegrationFilter, version=5.3.4.RELEASE}, {file=OncePerRequestFilter.java, method=doFilter, line=119, exact=true, location=spring-web-5.2.8.RELEASE.jar!/, class=org.springframework.web.filter.OncePerRequestFilter, version=5.2.8.RELEASE}, {file=FilterChainProxy.java, method=doFilter, line=334, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.FilterChainProxy$VirtualFilterChain, version=5.3.4.RELEASE}, {file=FilterChainProxy.java, method=doFilterInternal, line=215, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.FilterChainProxy, version=5.3.4.RELEASE}, {file=FilterChainProxy.java, method=doFilter, line=178, exact=true, location=spring-security-web-5.3.4.RELEASE.jar!/, class=org.springframework.security.web.FilterChainProxy, version=5.3.4.RELEASE}, {file=DelegatingFilterProxy.java, method=invokeDelegate, line=358, exact=true, location=spring-web-5.2.8.RELEASE.jar!/, class=org.springframework.web.filter.DelegatingFilterProxy, version=5.2.8.RELEASE}, {file=DelegatingFilterProxy.java, method=doFilter, line=271, exact=true, location=spring-web-5.2.8.RELEASE.jar!/, class=org.springframework.web.filter.DelegatingFilterProxy, version=5.2.8.RELEASE}, {file=ApplicationFilterChain.java, method=internalDoFilter, line=193, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.catalina.core.ApplicationFilterChain, version=9.0.37}, {file=ApplicationFilterChain.java, method=doFilter, line=166, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.catalina.core.ApplicationFilterChain, version=9.0.37}, {file=RequestContextFilter.java, method=doFilterInternal, line=100, exact=true, location=spring-web-5.2.8.RELEASE.jar!/, class=org.springframework.web.filter.RequestContextFilter, version=5.2.8.RELEASE}, {file=OncePerRequestFilter.java, method=doFilter, line=119, exact=true, location=spring-web-5.2.8.RELEASE.jar!/, class=org.springframework.web.filter.OncePerRequestFilter, version=5.2.8.RELEASE}, {file=ApplicationFilterChain.java, method=internalDoFilter, line=193, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.catalina.core.ApplicationFilterChain, version=9.0.37}, {file=ApplicationFilterChain.java, method=doFilter, line=166, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.catalina.core.ApplicationFilterChain, version=9.0.37}, {file=FormContentFilter.java, method=doFilterInternal, line=93, exact=true, location=spring-web-5.2.8.RELEASE.jar!/, class=org.springframework.web.filter.FormContentFilter, version=5.2.8.RELEASE}, {file=OncePerRequestFilter.java, method=doFilter, line=119, exact=true, location=spring-web-5.2.8.RELEASE.jar!/, class=org.springframework.web.filter.OncePerRequestFilter, version=5.2.8.RELEASE}, {file=ApplicationFilterChain.java, method=internalDoFilter, line=193, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.catalina.core.ApplicationFilterChain, version=9.0.37}, {file=ApplicationFilterChain.java, method=doFilter, line=166, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.catalina.core.ApplicationFilterChain, version=9.0.37}, {file=CharacterEncodingFilter.java, method=doFilterInternal, line=201, exact=true, location=spring-web-5.2.8.RELEASE.jar!/, class=org.springframework.web.filter.CharacterEncodingFilter, version=5.2.8.RELEASE}, {file=OncePerRequestFilter.java, method=doFilter, line=119, exact=true, location=spring-web-5.2.8.RELEASE.jar!/, class=org.springframework.web.filter.OncePerRequestFilter, version=5.2.8.RELEASE}, {file=ApplicationFilterChain.java, method=internalDoFilter, line=193, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.catalina.core.ApplicationFilterChain, version=9.0.37}, {file=ApplicationFilterChain.java, method=doFilter, line=166, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.catalina.core.ApplicationFilterChain, version=9.0.37}, {file=StandardWrapperValve.java, method=invoke, line=202, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.catalina.core.StandardWrapperValve, version=9.0.37}, {file=StandardContextValve.java, method=invoke, line=96, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.catalina.core.StandardContextValve, version=9.0.37}, {file=AuthenticatorBase.java, method=invoke, line=541, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.catalina.authenticator.AuthenticatorBase, version=9.0.37}, {file=StandardHostValve.java, method=invoke, line=139, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.catalina.core.StandardHostValve, version=9.0.37}, {file=ErrorReportValve.java, method=invoke, line=92, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.catalina.valves.ErrorReportValve, version=9.0.37}, {file=StandardEngineValve.java, method=invoke, line=74, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.catalina.core.StandardEngineValve, version=9.0.37}, {file=CoyoteAdapter.java, method=service, line=343, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.catalina.connector.CoyoteAdapter, version=9.0.37}, {file=Http11Processor.java, method=service, line=373, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.coyote.http11.Http11Processor, version=9.0.37}, {file=AbstractProcessorLight.java, method=process, line=65, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.coyote.AbstractProcessorLight, version=9.0.37}, {file=AbstractProtocol.java, method=process, line=868, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.coyote.AbstractProtocol$ConnectionHandler, version=9.0.37}, {file=NioEndpoint.java, method=doRun, line=1589, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.tomcat.util.net.NioEndpoint$SocketProcessor, version=9.0.37}, {file=SocketProcessorBase.java, method=run, line=49, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.tomcat.util.net.SocketProcessorBase, version=9.0.37}, {file=ThreadPoolExecutor.java, method=runWorker, line=1149, exact=true, location=?, class=java.util.concurrent.ThreadPoolExecutor, version=1.8.0_331}, {file=ThreadPoolExecutor.java, method=run, line=624, exact=true, location=?, class=java.util.concurrent.ThreadPoolExecutor$Worker, version=1.8.0_331}, {file=TaskThread.java, method=run, line=61, exact=true, location=tomcat-embed-core-9.0.37.jar!/, class=org.apache.tomcat.util.threads.TaskThread$WrappingRunnable, version=9.0.37}, {file=Thread.java, method=run, line=750, exact=true, location=?, class=java.lang.Thread, version=1.8.0_331}], localizedMessage=Access is denied, name=org.springframework.security.access.AccessDeniedException, commonElementCount=0, message=Access is denied}'"", ""caused_by""=>{""type""=>""illegal_state_exception"", ""reason""=>""Can't get text on a START_OBJECT at 1:404""}}}}}}



eu ainda nao entendi q padrão faz essa caca …  se seria apenas o tamanho do evento ou outra coisa.



Abraços

",,Paulo Henrique Morato Góes,,,,,61e09d6968926d0068d83362,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i003hn:,,português,,,,,,,,,,,,,,,31:11,,,,,,,,2023-03-27 15:25:51.005,,"27/mar/23 12:25 PM;61e09d6968926d0068d83362;Prezado Elias Mussi, boa tarde!

Esse erro normalmente acontece quando existe a tentativa de indexar um campo que é um objeto json, mas o mapeamento desse campo não permite, pois não foi mapeado como um objeto ou quando você está usando mapeamento dinâmico e já indexou um campo com o mesmo nome que não era um objeto.

Você precisará verificar o mapeamento do índice relacionado e compará-lo com o valor que está sendo enviado ao Elasticsearch para indexação.



Atenciosamente,

Paulo","27/mar/23 12:51 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;o mapeamento ali é dinamico … e nao vi onde ele gerou o conflito :(

ae complica pra dedeu



ja vi erros parecidos mas que falavam os campos","27/mar/23 1:17 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Observando o log que enviou, aparece - {{""reason""=>""failed to parse field [dtp_app.thrown] of type [keyword] in document with id 'vm-qIocBkM4aX-yfNk75'.}}

Não seria esse campo onde possa estar ocorrendo o problema?","27/mar/23 1:45 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;caracas … onde vc achou isso ?

qual vc acha q deveria ser o mapeamento disso ? 

parece q tem hora q vem so uma palavra ali … ou um monte de coisa, tipo … throw.abc.def

ae nao sei como poderia forçar esse campo

cada aplicacao manda de um jeito :/","27/mar/23 1:58 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

O mapeamento do campo depende da necessidade. Caso seja uma variável que contenha informações simples e pequenas, o mapeamento “object” é recomendado. Caso sejam complexos e que precisem ser pesquisados individualmente, o mapeamento “nested” é mais apropriado.","27/mar/23 2:12 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;um exemplo rapido para me ajudar ?

hj só tenho isso aqui : 

{noformat}{
  ""_routing"": {
    ""required"": false
  },
  ""numeric_detection"": false,
  ""dynamic_date_formats"": [
    ""strict_date_optional_time"",
    ""yyyy/MM/dd HH:mm:ss Z||yyyy/MM/dd Z""
  ],
  ""_source"": {
    ""excludes"": [],
    ""includes"": [],
    ""enabled"": true
  },
  ""dynamic"": true,
  ""dynamic_templates"": [],
  ""date_detection"": true
}{noformat}

","27/mar/23 2:41 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Não entendi, esse é o mapping que existe hoje em dia para o field “{{dtp_app.thrown}}""?","27/mar/23 2:43 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;esse é o mapping do template q o log pega.  nao tem mapeamento para o “dtp_app.”

ta tudo dinamico ali","27/mar/23 2:46 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Entendi, você pode me mandar o pipeline do logstash para eu dar uma olhada? acredito que essa alteração deverá ser realizada dentro do pipeline.



Grato!","27/mar/23 3:38 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;como assim ?

é um json q vem das apps, assim nao tem nada demais lá, em cima desse campo por ex. nenhum.

","28/mar/23 10:08 AM;61e09d6968926d0068d83362;Prezado Elias Mussi, bom dia!

Encaminho aqui um link para que possa entender melhor sobre o erro em questão. As alterações são feitas dentro do pipeline do logstash para contornar esse erro.
[https://discuss.elastic.co/t/problem-logstash-outputs-elasticsearch-could-not-index-event-to-elasticsearch-wazuh-alerts-3-x-2020-05-30/235038/6|https://discuss.elastic.co/t/problem-logstash-outputs-elasticsearch-could-not-index-event-to-elasticsearch-wazuh-alerts-3-x-2020-05-30/235038/6|smart-link] 



Atenciosamente,

Paulo",28/mar/23 12:13 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;certo … mas nada para fazer no lado do template para evitar q isso ocorra em outros pontos? isso pode pipocar por aqui :/,"28/mar/23 3:28 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Pelos meus estudos e pesquisas aqui, uma possiblidade é sim de tentar em vez de deixar o campo ser mapeado dinamicamente, setar o mapping correto pra ele.

Observe que, se você já indexou dados com o tipo de dados incorreto, precisará reindexar os dados após modificar o mapeamento para garantir que eles sejam indexados corretamente com o novo tipo de dados.



Aguardo retorno!","28/mar/23 8:02 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;vamos tentar … me manda um exemplo rapido do mapping ;)

","29/mar/23 10:24 AM;61e09d6968926d0068d83362;Prezado Elias Mussi, bom dia!

O mapeamento correto para “dtp_app.thrown” depende do tipo de dados que esse campo está armazenando. Se ""dtp_app.thrown"" contém um valor simples, como uma string, então o tipo ""text"" pode ser uma boa escolha. No entanto, se ""dtp_app.thrown"" contém um objeto JSON complexo, com várias propriedades, então o tipo ""object"" pode ser mais apropriado. Por outro lado, se ""dtp_app.thrown"" contém uma matriz de objetos JSON, então o tipo ""nested"" pode ser a melhor opção. O tipo ""nested"" permite que você indexe e pesquise matrizes de objetos JSON como uma única entidade, em vez de tratá-las como objetos independentes.

Dessa forma, é importante avaliar qual a estrutura de dados desse campo.



Atenciosamente, 

Paulo",17/mai/23 8:38 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;fecha esse treco sô :),,,,,,,,,,,,,,,,,,,,,,,Itens concluídos
Orientações na realização do parsing dos logs do WAF Imperva,DATAPREV-3,10575,Suporte técnico remoto: esclarecimento de dúvidas,Concluído(a),DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Muito Baixa,,Paulo Henrique Morato Góes,61e09d6968926d0068d83362,Paulo Jannuzzi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:351491f4-b45d-4144-955d-02d26a10137b,Paulo Jannuzzi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:351491f4-b45d-4144-955d-02d26a10137b,20/mar/23 4:32 PM,04/jul/23 9:11 AM,19/set/23 4:16 PM,,,0,"Boa tarde,

No WAF (Firewall de Aplicação) Imperva M160 foi configurado o envio dos logs utilizando o padrão RSA para a solução elastic, mas alguns campos estão sendo apresentados concatenados.

Ex.: rsa.misc.action ["""", GET]. Desta forma a busca e contabilização de itens fica prejudicada, pois não contabilizar os campos separadamente.



Solicitamos orientações para a correção.",,Ana Karolina Moreira Viana Catta Preta,josue,Paulo Henrique Morato Góes,,,62a7957ba80881006f6130ed,62a79211d0132d00687f77aa,61e09d6968926d0068d83362,,,,,,,,,,,12/abr/23 4:26 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:351491f4-b45d-4144-955d-02d26a10137b;alertaslog_mask_12042023.txt;https://bktech.atlassian.net/rest/api/3/attachment/content/10665,24/abr/23 11:35 AM;ug:0e9c7acd-4c78-4ef6-beb2-b00f34bddd59;image-20230420-175624 (056a83d4-08bd-45be-baca-adb626b33e81).png;https://bktech.atlassian.net/rest/api/3/attachment/content/10685,20/abr/23 3:01 PM;ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb;image-20230420-175624.png;https://bktech.atlassian.net/rest/api/3/attachment/content/10684,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i003gz:,,português,ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb(ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb),ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77(ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77),fabio(fabio),,,,,,,,,,,,21:55,,,,,,,,2023-03-20 20:07:10.161,,"20/mar/23 5:07 PM;61e09d6968926d0068d83362;Prezado Paulo Jannuzzi, boa tarde!



Em tempo, peço a gentileza para que *faça a alteração da severidade do chamado ou nos autorize a realizar a alteração*, uma vez que trata-se de esclarecimento de dúvidas, sendo, portanto, um chamado de severidade 4, conforme consta no item 14.1 do TR:

_d) Severidade 4 – quando se verifica como necessária a prestação de informações, aperfeiçoamentos ou_ *_esclarecimentos_* _sobre documentação ou_ *_funcionalidades, porém sem prejudicar diretamente a operação dos programas ou sistemas da DATAPREV._*

 

Desde já, agradecemos a cooperação e informamos que estamos trabalhando desde já para atende-lo!","20/mar/23 5:52 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:351491f4-b45d-4144-955d-02d26a10137b;Por favo,r informar o procedimento para alterar a prioridade do chamado.","20/mar/23 5:57 PM;61e09d6968926d0068d83362;Prezado Paulo Jannuzzi, 

Abaixo da mensagem de descrição, feita na abertura do chamado, possui o campo “Prioridade” que está como normal, que se refere a severidade 3.

Para a alteração, clicar em cima de “Normal” e colocar “Baixa” que se refere a severidade 4, a correta para este chamado. 

Caso não consiga, me informar que, com sua permissão, posso fazer essa alteração diretamente aqui.

Grato pela atenção!","21/mar/23 7:14 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:351491f4-b45d-4144-955d-02d26a10137b;Bom dia.

Pode trocar a prioridade, pois a opção de clicar em cima de “Normal“ não está habilitada.","21/mar/23 9:53 AM;61e09d6968926d0068d83362;Prezado Paulo Jannuzzi, bom dia!

A alteração na severidade foi realizada, sendo assim podemos prosseguir com o questionamento.

Gostaria de requisitar ao senhor, um arquivo com trechos de pelo menos 100 logs para que a gente possa comparar e visualizar o comportamento relatado.

Desde já, grato pela atenção e aguardo o retorno.","12/abr/23 4:26 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:351491f4-b45d-4144-955d-02d26a10137b;[^alertaslog_mask_12042023.txt]





Boa tarde.



Fornecido o arquivo pedido.



Além do pedido no chamado, acrescento os seguintes questionamentos / considerações:

1 - Quais chaves do log podemos usar sem quebrar o parser (Obs: temos apenas as n chaves conhecidas por padrão no WAF)?

2 – O WAF permite a criação de campos otimizados, como enviar esses dados não default de logs para o parsing?

3 - Exemplos de problemas:

* Campos user (dbusername, osusername) --> viram o mesmo vetor no filebeat (related field);
* HTTP method também

Isso impossibilita as visualizações, o que não atende nossas necessidades.

4 - Campos que deveriam seguir o padrão ECS, que são equivalentes ao do apache, estão no formato rsa.x.x, impossibilitando relacionamento/correlações.







","13/abr/23 11:00 AM;61e09d6968926d0068d83362;Bom dia Paulo Jannuzzi,

Avaliando os logs e o que foi relatado, existem algumas possíveis situações que possam estar gerando este comportamento.

Uma possível solução seria verificar as configurações do WAF para garantir que os logs estejam sendo formatados corretamente antes de serem enviados para o Elasticsearch. É importante visualizar se cada campo está sendo delimitado corretamente antes de ser enviado.

Caso os campos estejam sendo delimitados corretamente no WAF, verifique se a configuração do Filebeat está configurada para tratar esses campos como campos distintos, se a configuração do módulo do Filebeat para o WAF está correta e se os campos estão sendo definidos corretamente.



Aguardo retorno!","17/abr/23 10:52 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:351491f4-b45d-4144-955d-02d26a10137b;Bom dia.

Os Logs enviados pelo WAF estão usando o formato da RSA.

A delimitação dos campos, parecem estar corretas.

O Filebit está usando a configuração padrão dos campos RSA.

O que foi avaliado no log que pode estar gerando esse comportamento? Pode mostrar alguns exemplos?

","17/abr/23 5:05 PM;61e09d6968926d0068d83362;Prezado Paulo Jannuzzi, boa tarde!

Observando os logs, percebi algumas concatenações dos campos mesmo como por exemplo: 

* hdrName = Host, Connection, Accept, Access-Control-Request-Method, Access-Control-Request-Headers, Origin, User-Agent, Sec-Fetch-Mode, X-Requested-With, Sec-Fetch-Site, Sec-Fetch-Dest, Referer, Accept-Encoding, Accept-Language
* webQuery = """"
* url = ""/cc/ccc/ddd""
* application = """"



Por isso, aparenta ser algo relacionado a configuração do Filebeat ou do próprio WAF não estar dividindo corretamente os campos. Porém por essa ser uma função ainda experimental da Elastic, pode ser que seja realmente um bug do software. Estamos em contato com a Elastic para verificar este comportamento, contudo, se puder me enviar também o arquivo de configuração do módulo ativo no filebeat pode auxiliar também.","19/abr/23 10:27 AM;61e09d6968926d0068d83362;Prezado Paulo Jannuzzi, bom dia!

Em conversa diretamente com a Elastic, reiteraram a necessidade de observar os arquivos relevantes do filebeat para entender o comportamento que tem acontecido. Sendo assim, gostaria de pedir, além do arquivo de configuração do módulo ativo no filebeat requisitado anteriormente, também o arquivo filebeat.yml para avaliação.



Aguardo o retorno!","19/abr/23 4:34 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Boas,

{code:yaml}filebeat.config:
  modules:
    path: /etc/filebeat/modules.d/*.yml
    reload.enabled: false
  inputs:
    enabled: true
    path: /etc/filebeat/inputs.d/*.yml

output.redis:
  hosts: [""redisdf.elk.prevnet:6379""]
  key: ""%{[dtp.cliente]:cliente}-%{[dtp.type]:default_list}""
  password: MinhaSuperSenhaForte00@@
  timeout: 30
monitoring.enabled: false

path:
  config: /etc/filebeat
  data: /var/lib/filebeat
  home: /usr/share/filebeat
  logs: /var/log/filebeat{code}

esse e o filebeat.yaml

{code:yaml}n221p036887:/etc/filebeat:cat modules.d/dtp-waf-imperva-imperva.yml | grep -v '#'

- module: imperva
  securesphere:
    enabled: true
    var.syslog_host: 0.0.0.0
    var.syslog_port: 10312
  
    input.fields_under_root: true
    input.fields:
      dtp.cp: ""cpdf""
      dtp.ambiente: ""prod""
      dtp.cliente: ""dtp""
      dtp.sistema: ""waf""
      dtp.categoria: ""imperva""
      dtp.type: ""imperva""
{code}

e esse a config do modulo



são todos simples e sem invensão de moda 🙂 

Abraços","20/abr/23 3:01 PM;62a7957ba80881006f6130ed;Prezado Elias, boa tarde!



O Imperva por se tratar de um módulo experiemental da Elastic, que pode ser alterada ou remove em releases futuras, esta fora do escopo de suporte da Elastic.

!image-20230420-175624.png|width=724,height=131!

No entanto, a BKTech esta em consulta com a engenharia da Elastic buscando fazer o possivel para sanar tal situação. Assim, propomos uma reunião no dia 24/05/2023 às 11 hrs, para que possamos visualizar o ambiente e entender da melhor forma a situação exposta. Informo que o convite será enviado via e-mail.



LINK DOCUMENTAÇÃO ELASTIC: [https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-imperva.html|https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-imperva.html|smart-link] ","24/abr/23 11:35 AM;61e09d6968926d0068d83362;Prezado Paulo Jannuzzi, bom dia!

De acordo com o que acertamos na reunião de hoje, dia 24/04/2023 as 11:00, o módulo do Imperva no Elastic ainda se trata de uma funcionalidade prévia que ainda não está sujeita ao SLA de suporte dos recursos oficiais da Elastic. Dessa forma, após esse entendimento e conversa com o time da Dataprev, ficou determinado como solução o retorno ao parsing da forma que era feito anteriormente, já que o módulo não está funcionando da forma que a Dataprev necessita.

!image-20230420-175624 (056a83d4-08bd-45be-baca-adb626b33e81).png|width=391,height=71!



Agradecemos novamente o entendimento dessa questão e solicitamos a confirmação para fechamento do chamado.



Grato pela paciência e atenção!



LINK DOCUMENTAÇÃO ELASTIC: [https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-imperva.html|https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-module-imperva.html|smart-link]","09/jun/23 12:50 PM;62a79211d0132d00687f77aa;Prezado Paulo Jannuzzi,

Gostaria de solicitar, com base nos esclarecimentos fornecidos durante a última interação, a autorização para o fechamento do chamado em questão.",04/jul/23 9:08 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:351491f4-b45d-4144-955d-02d26a10137b;O chamado pode ser fechado.,,,,,,,,,,,,,,,,,,,,,,,,Itens concluídos
index X datastream,DATAPREV-2,10574,Suporte técnico remoto: esclarecimento de dúvidas,Concluído(a),DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Muito Baixa,,Vinícius Prysthon,621538d1347c690072f6b56f,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,15/mar/23 10:00 AM,16/jun/23 2:26 PM,19/set/23 4:16 PM,,,0,"Boas,



turminha … duvidas quanto ao comportamento e config do output do logstash. (vinculado ao problema do chamado anterior do backup X data)

hj usamos datastream, o que gera o problema da data, e minha config basica é essa :

{noformat}elasticsearch {
  hosts => [""n321p004880.fast.prevnet"", ""n321p004881.fast.prevnet"", ""n321p004882.fast.prevnet""]
  data_stream => true
  pipeline => ""%{[@metadata][pipeline]}""
  user => ""elastic""
  password => ""xxxxxxxx123""
  ssl => true
  ssl_certificate_verification => false
}{noformat}

penso em mudar para essa:

{noformat}   elasticsearch {
      hosts => [""n321p004880.fast.prevnet"", ""n321p004881.fast.prevnet"", ""n321p004882.fast.prevnet""]
      data_stream => false
      action => ""create""
#      index => ""%{[data_stream][type]}-%{[data_stream][dataset]}-%{[data_stream][namespace]}""
      index => ""logs-infra-%{[dtp][cliente]}-%{[dtp][type]}-%{[@metadata][version]}""
      pipeline => ""%{[@metadata][pipeline]}""
      user => ""elastic"" 
      password => ""xxxxxxxx123""
      #cacert => ""/etc/logstash/es.cer"" 
      ssl => true
      ssl_certificate_verification => false
{noformat}

nota:  com a config da linha 5 😉



pelo q vi em dev :  (duvidas ???  )

1- o que vejo no kibana e datastream sendo criado, é isso mesmo ?

2- sem o paramentro da linha 4 tomo erro de indexacao 😞 

3- esperava o comportamento como dos indices antigos com a linha 3.

4- o que acontece se a opcao data_stream => true + index =>  for setada ?

o que me dizem ?  

Abraços",,Ana Karolina Moreira Viana Catta Preta,Fabio Schmidt,Vinícius Prysthon,,,62a7957ba80881006f6130ed,5ea9bfda0590bb0b7bedbaa1,621538d1347c690072f6b56f,,,,,,,,,,,15/mar/23 5:10 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Screenshot from 2023-03-15 17-03-20.png;https://bktech.atlassian.net/rest/api/3/attachment/content/10661,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Dataprev,0|i003gr:,,português,ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb(ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb),fabio(fabio),ug:23192484-1347-4507-ac75-7d40e1cc1e5f(ug:23192484-1347-4507-ac75-7d40e1cc1e5f),ug:0e9c7acd-4c78-4ef6-beb2-b00f34bddd59(ug:0e9c7acd-4c78-4ef6-beb2-b00f34bddd59),ug:0e2daf0a-9fff-4018-89ac-200a262b74f9(ug:0e2daf0a-9fff-4018-89ac-200a262b74f9),,,,,,,,,,38:01,,,,,,,,2023-03-15 13:28:45.401,,"15/mar/23 10:28 AM;62a7957ba80881006f6130ed;Prezado Elias Mussi, bom dia!

Confirmo recebimento do chamado e informo que estamos trabalhando para atendê-lo.

Em tempo, peço a gentileza para que *faça a alteração da severidade do chamado ou nos autorize a realizar a alteração*,  uma vez que trata-se de esclarecimento de dúvidas, sendo, portanto, um chamado de severidade 4, conforme consta no item 14.1 do TR:

_d) Severidade 4 – quando se verifica como necessária a prestação de informações, aperfeiçoamentos ou_ *_esclarecimentos_* _sobre documentação ou_ *_funcionalidades, porém sem prejudicar diretamente a operação dos programas ou sistemas da DATAPREV._*



Desde já, agradecemos a cooperação e informamos que estamos trabalhando desde já para atende-lo!","15/mar/23 10:53 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;{color:#000000}Boas,{color} {color:#000000}{color} 
  
{color:#000000} uai ... nao foi assim que abri??{color} 
{color:#000000} foi bem essa opção que escolhi{color} {color:#000000}{color} 
  
{color:#000000} se estiver errado altera{color} {color:#000000}{color} 
  
{color:#000000} Abraços{color} 
----
 
{color:#000000}{color}","15/mar/23 11:16 AM;62a7957ba80881006f6130ed;Prezado Elias,



Foi alterado o nível de severidade, conforme acordado. Estamos trabalhando para sanar suas dúvidas o quanto antes.



Desde já grata! ","15/mar/23 11:23 AM;621538d1347c690072f6b56f;Prezado Elias Mussi,

Grato pela autorização, já fizemos a mudança aqui na severidade.

Voltando à sua dúvida, vi que na sua nova configuração de output o parâmetro {{data_stream}} está setado como {{false}} (não é necessário colocá-lo, uma vez que o default dele já é {{false}}). Aproveito também pra te fazer duas perguntas:

* Você tentou criar o índice com o parâmetro {{action}} setado como {{index}} ({{action => ""index""}})? Se sim, qual foi o comportamento? Pergunto porque na documentação da Elastic o {{action => ""create""}} é o padrão utilizado quando se deseja utilizar datastreams, o que não é o seu caso;
* Qual a razão para a criação do índice utilizando a config setada na linha 5 ao invés da linha 6?

Sobre sua pergunta a respeito de setar o {{data_stream => true}} e setar algum nome pro {{index}}, o que vai acontecer é a criação de um nome (o que você setar em {{index}}) que será atribuído ao datastream.



Desde já, grato pela atenção.","15/mar/23 11:49 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;na versao > 8 o default e auto … eu forcei um false para ver se nao usava … e ficasse como o “antigo” (old school heheheh)

(era false na versao 7, o que nao é nosso caso aqui)



action nao teste valores .. pq a priori nem deveria precisar dele. mas sem ele da erro 



o 5x6 era para ter controle inicialmente … depois eu deixaria o 6 q é um padrao amplo e construido com outras regras q tenho ali no output. (onde classifico meus logs para diferentes filas)



“Sobre sua pergunta a respeito de setar o data_stream => true e setar algum nome pro index, o que vai acontecer é a criação de um nome (o que você setar em index) que será atribuído ao datastream.”

repara … se vc ver minha linha 6 é exatamente a construção dessa nomeclatura … o dilema meu acaba sendo :

* tudo é datastream ? e os antigos index nao funcionao ?
* ou setando data_stream para false o comportamento volta a ser como dos velhos index ?

pq repara … pelas nosssas discuções eu tenho de sair do data_stream para voltar a ter o comportamento de virada de dia para os indices … mas qual a config para isso ?

esse comportamento de “data_stream”  com as conf q fiz me deixaram com duvida, e nao quero propagar essa mudança para prod sem sanar ela.

Abraços","15/mar/23 12:30 PM;621538d1347c690072f6b56f;Prezado Elias Mussi,

Poderia testar, por gentileza, utilizando o {{action => ""index""}} e retirando a linha 3 ({{data_stream => ""false""}})? Dessa forma poderemos ter uma noção do comportamento quando setamos o {{action => ""index""}}, já que sem esse parâmetro, está dando erro. O que imaginei foi algo parecido com isso (a utilização da linha 4/5 fica a seu critério):

{noformat}elasticsearch {
  hosts => [""n321p004880.fast.prevnet"", ""n321p004881.fast.prevnet"", ""n321p004882.fast.prevnet""]
  action => ""index""
#  index => ""%{[data_stream][type]}-%{[data_stream][dataset]}-%{[data_stream][namespace]}""
  index => ""logs-infra-%{[dtp][cliente]}-%{[dtp][type]}-%{[@metadata][version]}""
  pipeline => ""%{[@metadata][pipeline]}""
  user => ""elastic""
  password => ""xxxxxxxx123""
  #cacert => ""/etc/logstash/es.cer""
  ssl => true
  ssl_certificate_verification => false
}{noformat}

Aguardo seu retorno!","15/mar/23 1:46 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;para de funcionar e volta com o mesmo erro …. 

[2023-03-15T13:45:09,381][WARN ][logstash.outputs.elasticsearch][dataprev][493fee152c47c5b8dd579bfd30f817ac83af242124a3af4bc49b541ad84b23f7] Could not index event to Elasticsearch. status: 400, action: [""index"", {:_id=>nil, :_index=>""logs-infra-dataprev-apache-8.6.1"", :routing=>nil, :pipeline=>""filebeat-8.6.1-apache-access-pipeline""}, {""data_stream""=>{""dataset""=>""infra-dataprev"", ""namespace""=>""apache-8.6.1"", ""type""=>""logs""}, ""fileset""=>{""name""=>""access""}, ""dtp.redis_key""=>""dataprev-apache"", ""event""=>{""module""=>""apache"", ""dataset""=>""apache.access""}, ""@version""=>""1"", ""message""=>""10.190.1.55 - - [15/Mar/2023:13:45:06 -0300] ""GET / HTTP/1.0"" 302 193 ""-"" ""-"""", ""service""=>{""type""=>""apache""}, ""input""=>{""type""=>""log""}, ""host""=>{""name""=>""n231d037716""}, ""dtp.redis_server""=>""n241p036773.fast.prevnet"", ""agent""=>{""name""=>""n231d037716"", ""id""=>""cf68683a-8bf8-4288-bb74-968ff3183d35"", ""version""=>""8.6.1"", ""type""=>""filebeat"", ""ephemeral_id""=>""a75c46ee-e573-4920-b0f5-074ed56c0e2a""}, ""log""=>{""file""=>{""path""=>""/var/log/httpd/teste_access.log""}, ""offset""=>597792}, ""ecs""=>{""version""=>""1.12.0""}, ""dtp""=>{""ambiente""=>""dev"", ""categoria""=>""access"", ""cp""=>""cpdf"", ""cliente""=>""dataprev"", ""sistema""=>""teste"", ""type""=>""apache""}, ""@timestamp""=>2023-03-15T16:45:08.508Z, ""type""=>""apache"", ""dtp.logstash_indexer""=>""n321p004874.fast.prevnet""}], response: {""index""=>{""_index""=>""logs-infra-dataprev-apache-8.6.1"", ""_id""=>nil, ""status""=>400, ""error""=>{""type""=>""illegal_argument_exception"", ""reason""=>""only write ops with an op_type of create are allowed in data streams""}}}",15/mar/23 1:46 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;ahhhh   e nesse momento o index existe … pq nao apaguei ele,"15/mar/23 2:59 PM;621538d1347c690072f6b56f;Prezado Elias Mussi,

Observando a sua configuração, percebi que é possível que algum template criado anteriormente esteja configurado pra criar datastream e está sendo aplicado quando o índice criado recebe o prefixo {{logs-*}}. Poderia, por gentileza, verificar se há algum index template fazendo essa confusão? Se tiver algum template configurado com esse padrão e nele exista a criação de datastream, os índices que casarem com esse padrão de prefixo sempre serão convertidos para datastream. 



Grato pela atenção.","15/mar/23 5:10 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;repara ….



eu tenho um template geral, para o prefixo logs-*

mas no template que to usando e que os dados entram é esse ae:

{""index_patterns"":[""logs-infra-dataprev-apache-8*""],""template"":{""settings"":{""index"":{""lifecycle"":{""name"":""politica-longa""}}},""mappings"":{""_routing"":{""required"":false},""numeric_detection"":false,""dynamic_date_formats"":[""strict_date_optional_time"",""yyyy/MM/dd HH:mm:ss Z||yyyy/MM/dd Z""],""_source"":{""excludes"":[],""includes"":[],""enabled"":true},""dynamic"":true,""dynamic_templates"":[],""date_detection"":true}},""composed_of"":[""dtp_filebeat-8.6.1"",""dtp_parse_origination_data""],""priority"":103,""version"":1,""data_stream"":{""hidden"":false,""allow_custom_routing"":false}}

tem um data_stream no final :/  será q é isso ?

ouuu pode ser no template do filebeat q uso como componente.

a figura ae e do template q uso (nos testes).

!Screenshot from 2023-03-15 17-03-20.png|width=624,height=474!

","15/mar/23 5:36 PM;621538d1347c690072f6b56f;Prezado Elias Mussi,

Sim, de fato esse template está fazendo confusão… o parâmetro {{data_stream}} ali no final está transformando em data stream os índices que casam com o prefixo setado… sobre o component template do filebeat, precisaríamos dar uma olhada pra ver se compromete em algo, mas o que sugiro inicialmente é retirar o parâmetro {{data_stream}} do template e tentar converter novamente para índices.



Aguardo seu retorno.",15/mar/23 6:34 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;converter ?,"15/mar/23 6:55 PM;621538d1347c690072f6b56f;Perdão, ao dizer “converter”, me refiro a tentar novamente realizar a criação do índice (conversão de datastream para índices regulares)



Grato.","15/mar/23 7:04 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;oia a treta :(

composable template [dtp_apache-dataprev] with index patterns [logs-infra-dataprev-apache-8*], priority [103] and no data stream configuration would cause data streams [logs-infra-dataprev-apache-8.6.1, logs-infra-dataprev-apache-8.5.1] to no longer match a data stream template



me da esse erro e nao deixa salvar :/ 

pelo jeito se eu mudar para fora do DS eu teria de limpar todos os dados … o q me arrebenta pq ja tenho dados em prod","16/mar/23 10:42 AM;621538d1347c690072f6b56f;Prezado Elias Mussi,

Grato pelo retorno. O erro provavelmente tem a ver com a prioridade envolvida no index template. Ao criar um novo índice, o mesmo irá escolher (caso haja mais de um index template com as mesmas configurações) o template que possui prioridade maior ({{priority}}). 

Dessa forma, o que sugiro é o seguinte: para os novos índices criados, você deve criar um novo index template com as mesmas configurações do antigo mas sem a config do data stream e com uma prioridade maior do que a do index template anterior (que aparentemente é 103). 

Assim, os índices novos receberão as configurações do template novo, que, apesar de possuir as mesmas configurações do template antigo, possui prioridade maior (portanto será escolhido) e não terá a configuração de conversão para data streams. 

Vale destacar que: você não deve apagar o template antigo, apenas criar um novo com as mesmas configurações, porém sem o field de data stream e com prioridade maior que o antigo. Dessa forma, não será necessário limpar os dados, uma vez que os data streams já criados serão preservados pois estarão respeitando ainda o index template com a config de data stream, enquanto os novos índices já respeitarão as configurações do novo template com prioridade maior.

Caso tenha interesse, a documentação apresenta nessa página a descrição do field {{priority}}:

[https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-template.html|https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-template.html|smart-link] 

Aguardo seu retorno.","16/mar/23 3:32 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;mas foi exatamente isso que eu fiz.

o erro é na hora de salvar o novo index_template

ele nao deixa 

pelo jeito eu teria de apagar os indices antigos para poder criar o novo template q mude de data_stream para index

meio q:

 stop logstash indexacao

apago o DS

aplico e/ou atualizo o template do index

volto o logstash","16/mar/23 4:39 PM;621538d1347c690072f6b56f;Prezado Elias Mussi,

Poderia, por gentileza, enviar sua query de criação do novo template? Gostaria de entender quais mudanças estão sendo feitas no novo template com relação ao antigo. Se possível, enviar o template antigo também para avaliarmos.



Grato pela atenção.","16/mar/23 7:23 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Boas,

o novo template : 

{noformat}{""index_patterns"":[""logs-infra-dataprev-apache-8*""],""template"":{""settings"":{""index"":{""lifecycle"":{""name"":""politica-longa""}}},""mappings"":{""_routing"":{""required"":false},""numeric_detection"":false,""dynamic_date_formats"":[""strict_date_optional_time"",""yyyy/MM/dd HH:mm:ss Z||yyyy/MM/dd Z""],""_source"":{""excludes"":[],""includes"":[],""enabled"":true},""dynamic"":true,""dynamic_templates"":[],""date_detection"":true}},""composed_of"":[""dtp_filebeat-8.6.1"",""dtp_parse_origination_data""],""priority"":104,""version"":1}
{noformat}



que é baseado nesse daqui : 

{noformat}{   ""index_patterns"": [     ""logs-infra-dtp-apache-*""   ],   ""template"": {     ""settings"": {       ""index"": {         ""number_of_shards"": ""6""       }     },     ""mappings"": {       ""_routing"": {         ""required"": false       },       ""numeric_detection"": false,       ""dynamic_date_formats"": [         ""strict_date_optional_time"",         ""yyyy/MM/dd HH:mm:ss Z||yyyy/MM/dd Z""       ],       ""_source"": {         ""excludes"": [],         ""includes"": [],         ""enabled"": true       },       ""dynamic"": true,       ""dynamic_templates"": [],       ""date_detection"": true     }   },   ""composed_of"": [     ""dtp_filebeat-8.6.1"",     ""dtp_parse_origination_data""   ],   ""priority"": 103,   ""version"": 1,   ""data_stream"": {     ""hidden"": false,     ""allow_custom_routing"": false   } }  
{noformat}



mudei o index_pattern, priority e tirei as relações com data_stream","16/mar/23 11:01 PM;621538d1347c690072f6b56f;Prezado Elias Mussi,

De fato seu template novo é da forma que pensei, talvez o erro ao tentar salvar seja pelo motivo de o Logstash estar rodando e não permitir a criação desse template novo. O que sugiro seria tentarmos realizar a criação desse template após parar o Logstash (como você mesmo mencionou). Seria possível fazermos esse teste? Qual o impacto ao seu ambiente em parar o Logstash para fazer essa alteração?

Com relação aos datastreams que poderiam ser afetados com essa alteração, existe a possibilidade de reindexá-los manualmente para que os dados não sejam perdidos.



Aguardo seu retorno.","17/mar/23 9:23 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;pois é …

a treta é exatamente com os indices (data_stream) q existe  e nem tanto com o logstash

teria de apagar o DS existente para conseguir criar pelo jeito.

o parar o logstash é exatamente para nao criar um index antes (depois) de se conseguir apagar o DS e poder mexer no template 

o template nao se aplica com a existencia do DS antigo

um teste q to pensando em fazer é mudar a nomeclatura do envio para um template diferrente e ver no que que dá

*mas estava pensando em outra abordagem, para nao ter de mexer no DS/index.*

e se em vez de se fazer backup do dia anterior apenas, eu fizer de mais dias … ex d-1 e d-2

dessa forma (rotacionando la na politica em 1d) eu garanto q o d-2 esteje completo ( a priori) 

o problema é que gastaria o dobro de espaco no NFS  e teria de combinar com a turma q faz restore um novo procedimento diferente do atual

talvez assim eu consiga uma melhor eficiencia no cluster

o q acha ?

","17/mar/23 10:10 AM;5ea9bfda0590bb0b7bedbaa1;Prezado Elias Mussi,

Grato pelo retorno.

Conforme análise em conjunto realizada, para manter a configuração de Datastreams se faz necessário alterar o backup para ""D-2"" para se obter os dados de um dia anterior completo. Com essa implementação, não seria necessário retornar para índices diários e o ambiente como um todo é preservado. 

Essa configuração exige mais espaço para o armazenamento dos dados, porém, com uma visão de preservação ao ambiente Elastic em si, acreditamos que essa seja a melhor solução, uma vez que exploramos ao máximo, com o seu estimado apoio, essa implementação e as demais opções.

Agradecemos novamente o apoio no estudo dessa questão e gostaríamos de perguntar se já foram esclarecidas todas as dúvidas. Caso tenha esclarecido, solicitamos a confirmação para fechamento do chamado.

Grato,
Fabio S. Schmidt","17/mar/23 10:30 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;qse :)

uma preocupacao …

imaginando um caso de desastre, onde se perde todo o cluster e se tenha de recuperar os dados desse snapshot.

o que vcs acham que pode ocorrer ? qual a chance de recuperacao ? (tirando o buraco de dados q sei q terá)

quais pontos fracos ?

eu em particular nao se se mudou algo nos snaps da versao 6 para a 8 que possa fragilizar o meu processo de backup

Abraços","17/mar/23 6:15 PM;5ea9bfda0590bb0b7bedbaa1;Prezado Elias Mussi,

Grato pelo retorno.

Em caso de desastre, o snapshot da Elastic possui a garantia de recuperação dos dados, uma vez que o mesmo esteja devidamente armazenado pela ferramenta de backup e com a integridade preservada.

O snapshot é o método recomendado pela Elastic e a documentação pode ser encontrada no link abaixo, por exemplo:

[https://www.elastic.co/guide/en/elasticsearch/reference/current/snapshot-restore.html|https://www.elastic.co/guide/en/elasticsearch/reference/current/snapshot-restore.html|smart-link] 

Grato,
Fabio S. Schmidt","20/mar/23 8:27 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;mesmo o snapshot sendo só do dia d-1 ou d-2 ?  só dos indices que “.ds-logs* ?  (fora os dados do cluster, feature states)



Abraços","20/mar/23 10:44 AM;621538d1347c690072f6b56f;Prezado Elias Mussi,

A recuperação dos dados se fará de acordo com a configuração especificada. Com relação à configuração de dias, o snapshot irá recuperar dados de “d-1” ou “d-2” de acordo com a capacidade de retenção do backup utilizado. Caso a configuração realizada busque recuperar índices que casam com o padrão “.ds.logs*”, em caso de desastre, os dados recuperados seriam apenas os índices que correspondem a esse padrão. Dessa forma, entende-se que o snapshot deve ser configurado para se obter todos os dados desejados em caso de perda.



Grato pela atenção.","20/mar/23 1:40 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;oremos :)

fecha ae

abraços",,,,,,,,,,,,,Itens concluídos
rotina de backup e rotacionamento de indices por dia,DATAPREV-1,10570,Suporte técnico remoto: esclarecimento de dúvidas,Concluído(a),DATAPREV,Dataprev - Gestão Centralizada de Logs,service_desk,Fabio Schmidt,5ea9bfda0590bb0b7bedbaa1,,Normal,,Paulo Henrique Morato Góes,61e09d6968926d0068d83362,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,Elias Mussi,qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae,09/mar/23 10:28 AM,16/jun/23 2:26 PM,19/set/23 4:13 PM,,,0,"Boas,

Já conversamos sobre isso, e até tentamos algo. Mas o comportamento me preocupa

vale a pena investigarmos.

tenho visto que a config do template do indice não está se comportando como esperavamos.

* nao esta rotacionando (criando os indices diarios) na virada da noite,
* pior tem continuado a gravar em indice antigo (os dados são um streaming n faz sentido falar q eram dados com data antiga)
* esse comportamento ferra com o esquema de backup ( que faz snapshot apenas dos dados do dia “anterior”

precisamos entender e solucionar esse comportamento 

bora combinar um horario e fazer uma call para analizar melhor

Abraços

",,Ana Karolina Moreira Viana Catta Preta,Fabio Schmidt,Paulo Henrique Morato Góes,Rodrigo Tornis,Vinícius Prysthon,62a7957ba80881006f6130ed,5ea9bfda0590bb0b7bedbaa1,61e09d6968926d0068d83362,5f9829d0dbf337006c7dc32d,621538d1347c690072f6b56f,,,,,,,,,13/mar/23 7:48 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Screenshot from 2023-03-13 19-43-17.png;https://bktech.atlassian.net/rest/api/3/attachment/content/10659,13/mar/23 7:48 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Screenshot from 2023-03-13 19-44-58.png;https://bktech.atlassian.net/rest/api/3/attachment/content/10660,13/mar/23 7:48 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Screenshot from 2023-03-13 19-45-30.png;https://bktech.atlassian.net/rest/api/3/attachment/content/10658,09/mar/23 10:27 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;VirtualBox_vpn_09_03_2023_09_50_20.png;https://bktech.atlassian.net/rest/api/3/attachment/content/10638,09/mar/23 10:27 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;VirtualBox_vpn_09_03_2023_10_02_22.png;https://bktech.atlassian.net/rest/api/3/attachment/content/10637,09/mar/23 10:27 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;VirtualBox_vpn_09_03_2023_10_04_45.png;https://bktech.atlassian.net/rest/api/3/attachment/content/10636,09/mar/23 10:27 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;VirtualBox_vpn_09_03_2023_10_05_35.png;https://bktech.atlassian.net/rest/api/3/attachment/content/10634,09/mar/23 10:27 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;VirtualBox_vpn_09_03_2023_10_08_58.png;https://bktech.atlassian.net/rest/api/3/attachment/content/10635,13/mar/23 9:13 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;dtp_filebeat-8.6.1.json;https://bktech.atlassian.net/rest/api/3/attachment/content/10642,13/mar/23 9:13 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;dtp_geral.json;https://bktech.atlassian.net/rest/api/3/attachment/content/10641,13/mar/23 9:13 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;dtp_paloalto.json;https://bktech.atlassian.net/rest/api/3/attachment/content/10643,13/mar/23 9:13 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;dtp_parse_origination_data.json;https://bktech.atlassian.net/rest/api/3/attachment/content/10640,13/mar/23 9:13 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;politica_prd.json;https://bktech.atlassian.net/rest/api/3/attachment/content/10639,,,,,,,,,,,,,,,,,Dataprev,0|i003fv:,,português,ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb(ug:6f3fb4d5-f6a5-43d1-b51e-6230a520e2fb),ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77(ug:17f84c45-66e6-4c5f-9809-ac34ded9ea77),fabio(fabio),ug:0bf5b6db-b989-4171-bb50-b139a583a751(ug:0bf5b6db-b989-4171-bb50-b139a583a751),qm:c4560720-f28e-48ad-8ca6-02074b751b6e:cece5bfc-fb79-4ccb-84f9-7ce16f24643b(qm:c4560720-f28e-48ad-8ca6-02074b751b6e:cece5bfc-fb79-4ccb-84f9-7ce16f24643b),ug:0e2daf0a-9fff-4018-89ac-200a262b74f9(ug:0e2daf0a-9fff-4018-89ac-200a262b74f9),,,,,,,,,24:58,,,,,,,,2023-03-09 13:45:08.2,,"09/mar/23 10:28 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;!VirtualBox_vpn_09_03_2023_09_50_20.png|thumbnail!

!VirtualBox_vpn_09_03_2023_10_02_22.png|thumbnail!

!VirtualBox_vpn_09_03_2023_10_04_45.png|thumbnail!

!VirtualBox_vpn_09_03_2023_10_05_35.png|thumbnail!

!VirtualBox_vpn_09_03_2023_10_08_58.png|thumbnail!","09/mar/23 10:45 AM;61e09d6968926d0068d83362;Prezado Elias Mussi, bom dia.



Confirmo recebimento do chamado e informo que estamos trabalhando para atendê-lo.



Desde já, grato pela compreensão.","09/mar/23 4:38 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Poderia nos enviar as seguintes evidências, da mesma aplicação:

* Mapping do índice;
* Caso exista, o component template utilizado para a criação do data stream;
* O index template.



Grato!","13/mar/23 9:13 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;{color:#000000}Boas,{color} {color:#000000}{color} 
  
{color:#000000} desculpa a demora ... nao estava olhando o mail 🙂{color} 
{color:#000000}{color} 
  
{color:#000000} ve se falta algo{color} 
{color:#000000}{color} 
  
{color:#000000} Abraços{color} 
----
 
{color:#000000}{color}

[^politica_prd.json] _(0.0 kB)_

[^dtp_parse_origination_data.json] _(0.0 kB)_

[^dtp_geral.json] _(0.0 kB)_

[^dtp_filebeat-8.6.1.json] _(0.0 kB)_

[^dtp_paloalto.json] _(0.0 kB)_","13/mar/23 10:48 AM;61e09d6968926d0068d83362;Prezado Elias Mussi, bom dia.

Grato pela resposta com os arquivos requisitados anteriormente. Em paralelo, estivemos em contato com a Elastic para entendermos essa questão. A Elastic nos informou que o Elasticsearch utiliza como horário para as operações internas o UTC 0, sendo assim, é com base nessa timezone que ocorre a operação de rollover diário. Por conta de estarmos no UTC -3 (São Paulo, Brasil), esse rollover tem ocorrido as 21hrs. Por ser um padrão da Elastic, não é algo que possa ser modificado internamente.

Na hipótese de que o snapshot precise ser restaurado com os dados de um dia completo em nossa timezone, será necessário restaurar os índices de ambos os dias que contenham esses dados. Esse é o comportamento esperado, de acordo com a própria Elastic.

Caso sinta necessidade desses ou outros esclarecimentos a respeito deste assunto, estamos à disposição para uma call com o objetivo de sanar as dúvidas restantes.



Novamente, agradecemos a colaboração e compreensão.","13/mar/23 11:13 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;{color:#000000}Boas,{color} {color:#000000}{color} 
  
{color:#000000} para indices sim .. mas no ILM não ta rolando nem no UTC0 ... mas sim em 24hs da criação do indice.{color} {color:#000000}{color} 
  
{color:#000000} Ae eu tenho indices criados em vários horários diferentes do dia q passam por isso. (ex: 14:38, 19,15 e por ae vai ){color} {color:#000000}{color} 
  
{color:#000000} uma marretada que consegui fazer aqui e que melhorou um pouco as coisas foi forçar um rollover a 00hs, isso trouxe os indices existentes para um horário mais homogêneo, mas não resolve o problema em caso de novos indices/data stream serem criados ... e pelo q li seria arriscado forçar rodar um rollover todo dia a 00hs.{color} {color:#000000}{color} 
  
{color:#000000} será q usar a opção : index.lifecycle.origination_date{color} 
{color:#000000} não seria possível ?{color}  {color:#000000}{color} 
  
{color:#000000} o que vejo como necessário é que :{color}  
{color:#000000} 
{color}
* {color:#000000}qdo um data stream novo surgir ele seja criado com a data 00hs daquele dia, para que se possa virar a cada dia.
 
com indices (na forma antiga) ok .. o UTC mata ... mas com data stream o lance fica pior pq qq horário via a referencia de virada para um novo. isso pq o ilm leva em conta a ""idade"" do indice e não do dia em si de criação.{color} 

   
{color:#000000} será que melhorou a ideia do problema ?{color} {color:#000000}{color} 
  
{color:#000000} Abraços{color} {color:#000000}{color} 
----
 
{color:#000000}{color}","13/mar/23 11:40 AM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Para um melhor entendimento de ambas as partes, gostaríamos de agendar uma reunião. Propomos o horário de 14:30 de hoje (13/03/2023) para este fim. Caso não seja possível para o senhor este horário, por favor, propor um novo agendamento.



Desde já, grato!","13/mar/23 11:44 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;{color:#000000}Boas,{color} {color:#000000}{color} 
  
{color:#000000} combinado ... vcs mandam o link ?{color} {color:#000000}{color} 
  
{color:#000000} Abraços{color} 
----
 
{color:#000000}{color}","13/mar/23 12:01 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Segue abaixo o link para nossa reunião virtual às 14:30 de hoje (13/03/2023):

[https://us06web.zoom.us/j/86030396113?pwd=V1hJZkJWc0pXWmJUUEp3cXFMNmtiQT09|https://us06web.zoom.us/j/86030396113?pwd=V1hJZkJWc0pXWmJUUEp3cXFMNmtiQT09|smart-link] 



Até lá!","13/mar/23 7:48 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;fazendo um teste em paralelo aqui .. mas nao deu bom nao.

fiz como vcs ae, mudei o index_template para um aqui, e rodei um rollover …  a priori teria dado certo … mas com o correr do dia, o indice sumiu, sobrando apenas o ultimo. reparem nas figuras

!Screenshot from 2023-03-13 19-45-30.png|width=1641,height=655!

!Screenshot from 2023-03-13 19-43-17.png|width=1641,height=655!

!Screenshot from 2023-03-13 19-44-58.png|width=1641,height=655!

sai de hot … para warm .. cold e foi deletado :(","13/mar/23 10:30 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Qual foi o período para rollover setado para esse teste? 1 dia mesmo? Por que pelo que enxerguei o horário entre as mudanças de fase estão em aproximadamente em 10 minutos.

Além disso, qual alteração que foi realizada no index_template? Adicionou o parâmetro index.lifecycle.origination_date setado para algum dia específico?



Desde já, grato pela atenção.","14/mar/23 7:53 AM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;Boas,

sim … 

criei um template separado para um indice aqui, e apaguei o data stream antigo para criar um novo com o template novo 

depois rodei um rollover para virar o indice manualmente. ate mesmo para ver o comportamento dele na virada do dia.

esperava q na meia noite o indice virasse para o novo dia, mas nao ocorreu. parece q o ES so trata mesmo a idade do indice e nao a virada do dia :(

max_age =1d

5d > warm e 10 d > cold

----

{
  ""indices"": {
    "".ds-logs-infra-dataprev-apache-8.6.1-2023.03.13-{color:#ff991f}000002{color}"": {
      ""index"": "".ds-logs-infra-dataprev-apache-8.6.1-2023.03.13-000002"",
      ""managed"": true,
      ""policy"": ""politica"",
 {color:#bf2600}     ""index_creation_date"": ""2023-03-13T19:17:02.699Z"",{color}
{color:#bf2600}      ""index_creation_date_millis"": 1678735022699,{color}
{color:#bf2600}      ""time_since_index_creation"": ""15.32h"",{color}
{color:#bf2600}      ""lifecycle_date"": ""2023-01-01T03:00:00.000Z"",{color}
      ""lifecycle_date_millis"": 1672542000000,
      ""age"": ""72.31d"",
      ""phase"": ""hot"",
      ""phase_time"": ""2023-03-13T19:17:07.254Z"",
      ""phase_time_millis"": 1678735027254,
      ""action"": ""rollover"",
      ""action_time"": ""2023-03-13T19:17:07.856Z"",
      ""action_time_millis"": 1678735027856,
      ""step"": ""check-rollover-ready"",
      ""step_time"": ""2023-03-13T19:17:07.856Z"",
      ""step_time_millis"": 1678735027856,
      ""phase_execution"": {
        ""policy"": ""politica"",
        ""phase_definition"": {
          ""min_age"": ""0ms"",
          ""actions"": {
            ""rollover"": {
              ""max_age"": ""1d""
            },
            ""set_priority"": {
              ""priority"": 100
            }
          }
        },
        ""version"": 3,
        ""modified_date"": ""2023-02-14T20:09:16.541Z"",
        ""modified_date_in_millis"": 1676405356541
      }
    }
  }
}","14/mar/23 10:50 AM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Entendemos o comportamento que ocorreu. Quando setamos o parâmetro “{{index.lifecycle.origination_date}}“ o ILM é afetado pois todos os tempos de transição de fase são baseados na data de origem. Então como foi setado para o início do ano (""age"": ""72.31d""), o ILM entende que já se passaram o max_age = 1 para o rollover, os 5d para transicionar para a fase warm, e também os 10d para ir em cold. Sendo assim teria que existir uma “compensação” do período para que essa estratégia funcionasse.

[https://www.elastic.co/pt/blog/control-ilm-phase-transition-timings-using-origination-date|https://www.elastic.co/pt/blog/control-ilm-phase-transition-timings-using-origination-date|smart-link] 



Continuamos estudando uma possível solução para o problema, qualquer novidade enviamos aqui. Grato!","14/mar/23 2:48 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Aprofundando no tema, vimos que, pela fabricante, não é possível garantir que o rollover em um Data Stream vá ocorrer exatamente na virada do dia (até mesmo se o rollover for setado de forma manual), não é dessa forma que funciona a criação dos índices. A data da criação do índice é provavelmente quando você executou a configuração pela primeira vez ou quando foi ingerido o primeiro evento. 

Antes de o rollover e o data stream estarem disponíveis, a maneira padrão de criar índices baseados em tempo era criar índices com a data no nome. Todos os dados com um registro de data e hora pertencente a essa data iriam para esse índice. Isso potencialmente oferece a separação de dados que está sendo procurada, embora seja baseado no horário UTC e não no fuso horário local. O problema com essa abordagem é que ela resulta facilmente em shards de tamanho muito desigual, o que pode ser um problema de desempenho, sendo assim, não recomendada sua aplicação no cluster.

Pensando na questão de retenção dos dados, para que se tenha os dados completos do dia anterior, recomenda-se manter os dados no cluster um pouco mais de tempo se o índice a ser excluído cobrir vários dias. Dessa forma terá a garantia de que os dados que cobrem um determinado período de retenção estejam disponíveis, não sendo um problema.

Conforme o que foi relatado, sendo o comportamento da fabricante Elastic o relatado acima, de que o data stream não ocorre exatamente na virada do dia, essa questão fica esclarecida?



Grato pela atenção!","14/mar/23 2:57 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;bom … é exatamente contra esse problema que estamos trabalhando … se é uma issue:melhoria a ser aberta para eles, ok. 

mas o problema nao resolve.

da forma como vc fala, a unica forma de ter o comportamento que desejo seria abandonar os datastream e voltar para os indices de formato antigo.



alguma outra forma, sem ser forçar o rollover à 00hs, q permita forçar o comportamento desejado ?","14/mar/23 3:04 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Teremos uma reunião as 15:30 marcada juntamente com a Elastic para um melhor entendimento. Se puder participar, será ótimo.

[https://us06web.zoom.us/j/81557308938?pwd=WC96eGlKZjJCOHJxTEQwSjBieVAydz09|https://us06web.zoom.us/j/81557308938?pwd=WC96eGlKZjJCOHJxTEQwSjBieVAydz09|smart-link] 



Grato!",14/mar/23 3:18 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;opa … vou dar um jeito de entrar,"14/mar/23 4:19 PM;61e09d6968926d0068d83362;Prezado Elias Mussi,

Conforme conversado em reunião às 15:30 do dia 14/03/2023, juntamente com o Engenheiro do Suporte da Elastic, Luiz Santos, chegamos ao ponto de que realmente o Data Stream não tem a possibilidade de definir o rollover baseado em um dia certo. Sendo assim, a solução encontrada para tal situação, é de criar os índices com a data do índice no nome para que todos os dados com um registro de data e hora pertencente a essa data iriam para esse índice.

De tal forma, chegando a esse entendimento por ambas as partes, podemos finalizar o atendimento do chamado?



Grato pela atenção e parceria!","14/mar/23 9:03 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;turma …

as config do output elasticsearch para criar um indice como antigamente mudou ?

estou tentando assim : 

{noformat}   elasticsearch {
      hosts => [""n321p004880.fast.prevnet"", ""n321p004881.fast.prevnet"", ""n321p004882.fast.prevnet""]
      data_stream => false
#     manage_template => false
      action => ""create""
      index => ""%{[data_stream][type]}-%{[data_stream][dataset]}-%{[data_stream][namespace]}""
      #index => ""%{[data_stream][type]}-%{[data_stream][dataset]}-%{[data_stream][namespace]}-%{+YYYY.MM.dd}""
      pipeline => ""%{[@metadata][pipeline]}""
      user => ""elastic""
      password => ""xxxxxxxxx000""
      ssl => true
      ssl_certificate_verification => false
    }
{noformat}



mas cria datastream .. e nao indice … me senti incomodado.

","14/mar/23 10:47 PM;5ea9bfda0590bb0b7bedbaa1;Prezado Elias Mussi, boa noite!

Grato pelo retorno.

Gostaria de informar que devido ao esclarecimento em conjunto com o suporte da Elastic em nossa reunião no dia de hoje, o nosso entendimento é de que a orientação da requisição deste chamado foi atendida com as informações encaminhadas anteriormente.

Desta forma, entendemos que o esclarecimento da demanda deste chamado foi realizado.

Sugerimos que o novo ponto requisitado, seja tratado como uma nova necessidade de orientação para a criação dos índices.

Portanto, solicitamos a autorização para a conclusão do presente chamado, e que seja aberto uma nova demanda, com a severidade apropriada, de acordo com o TR.

Desde já, gratos pela compreensão e estamos à disposição para quaisquer dúvidas ou esclarecimentos.",14/mar/23 10:51 PM;qm:c4560720-f28e-48ad-8ca6-02074b751b6e:0c2caf32-1b34-41c0-8c43-c5596af3c5ae;blz … fecha ae,,,,,,,,,,,,,,,,,,Itens concluídos
